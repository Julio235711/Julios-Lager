# -*- coding: utf-8 -*-
"""SupervisedCO2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o0VUB3ZTzy5M9LdDGE3Pt6FuZz349Mh6

#**CO2 Dataset**
"""

pip install dask[dataframe]

"""#**A .SupervisedCO2: Multiclass Classification with ordinal labels. Binary Subproblem One vs. all or One vs.one no yet decided.**"""

#Drive Mount for Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Import libraries

import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt

# Libraries for formating and normalizing the Data.

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder


# Libraries for Calculating Accuracy and Precision

from sklearn.metrics import f1_score
from imblearn.over_sampling import RandomOverSampler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn import svm

# Libraries for modeling

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestClassifier
from lightgbm import LGBMClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_absolute_error
from sklearn.tree import DecisionTreeRegressor



# Setting Drive Paths for cvs archives and reading the archives.

path_co2_2010 = '/content/drive/MyDrive/DataCO2Total/CO2_2010.csv'
path_co2_2011 = '/content/drive/MyDrive/DataCO2Total/CO2_2011.csv'
path_co2_2012 = '/content/drive/MyDrive/DataCO2Total/CO2_2012.csv'
path_co2_2013_HR = '/content/drive/MyDrive/DataCO2Total/CO2_2013_HR.csv'
path_co2_2013_NO = '/content/drive/MyDrive/DataCO2Total/CO2_2013_NO.csv'
path_co2_2013_V7 = '/content/drive/MyDrive/DataCO2Total/CO2_2013_V7.csv'
path_co2_2014_NO = '/content/drive/MyDrive/DataCO2Total/CO2_2014_NO.csv'
path_co2_2014_V9 = '/content/drive/MyDrive/DataCO2Total/CO2_2014_V9.csv'
path_co2_2015 = '/content/drive/MyDrive/DataCO2Total/CO2_2015.csv'
path_co2_2016 = '/content/drive/MyDrive/DataCO2Total/CO2_2016.csv'
path_co2_2017 = '/content/drive/MyDrive/DataCO2Total/CO2_2017.csv'
path_co2_2018 = '/content/drive/MyDrive/DataCO2Total/CO2_2018.csv'
path_co2_2019 = '/content/drive/MyDrive/CO2_2019.csv'


df_CO2_2010 = pd.read_csv(path_co2_2010,sep='\t', encoding='utf-8', on_bad_lines='skip', low_memory = False)
df_CO2_2011 = pd.read_csv(path_co2_2011,sep='\t', encoding='utf-8', on_bad_lines='skip',low_memory = False)
df_CO2_2012 = pd.read_csv(path_co2_2012,sep='\t', encoding='utf-8', on_bad_lines='skip',low_memory = False)
df_CO2_2013_HR = pd.read_csv(path_co2_2013_HR,sep='\t', encoding='utf-8', on_bad_lines='skip',low_memory = False)
df_CO2_2013_NO = pd.read_csv(path_co2_2013_NO,sep='\t', encoding='utf-8', on_bad_lines='skip',low_memory = False)
df_CO2_2013_V7 = pd.read_csv(path_co2_2013_V7,sep='\t', encoding='utf-8', on_bad_lines='skip',low_memory = False)
df_CO2_2014_NO = pd.read_csv(path_co2_2014_NO,sep='\t', encoding='utf-8', on_bad_lines='skip',low_memory = False)
df_CO2_2014_V9 = pd.read_csv(path_co2_2014_V9,sep='\t', encoding='utf-8', on_bad_lines='skip',low_memory = False)
df_CO2_2015 = pd.read_csv(path_co2_2015,sep='\t', encoding='utf-8', on_bad_lines='skip',low_memory = False)
df_CO2_2016 = pd.read_csv(path_co2_2016,sep='\t', encoding='utf-8', on_bad_lines='skip',low_memory = False)
df_CO2_2017 = pd.read_csv(path_co2_2017, on_bad_lines='skip',encoding='utf-16le',delimiter=r"\s+", chunksize =1000, low_memory=False)
df_CO2_2018 = pd.read_csv(path_co2_2018,encoding='latin', on_bad_lines='skip',delimiter=r"\s+", chunksize =1000,low_memory = False)
df_CO2_2019 = pd.read_csv(path_co2_2019,sep=',',encoding='latin', on_bad_lines='skip', chunksize =100,low_memory = False)

# Creating emergency copies. We are going to use the archives from 2017 to 2019 to train
# and to forecast data. Theses achives hat more than 2 gigabytes.

CO2_1 = df_CO2_2010.copy(deep = True)
CO2_2 = df_CO2_2011.copy(deep = True)
CO2_3 = df_CO2_2012.copy(deep = True)
CO2_4 = df_CO2_2013_HR.copy(deep = True)
CO2_5 = df_CO2_2013_NO.copy(deep = True)
CO2_6 = df_CO2_2013_V7.copy(deep = True)
CO2_7 = df_CO2_2014_NO.copy(deep = True)
CO2_8 = df_CO2_2014_V9.copy(deep = True)
CO2_9 = df_CO2_2015.copy(deep = True)
CO2_10 = df_CO2_2016.copy(deep = True)
#CO2_11 = df_CO2_2017.copy(deep = True) # while this dataset is only a part of the complete one . Chunk function we are going to workit later.
#CO2_12 = df_CO2_2018.copy(deep = True) # while this dataset is only a part of the complete one . Chunk function we are going to workit later.
#CO2_13 = df_CO2_2019.copy(deep = True) # while this dataset is only a part of the complete one . Chunk function we are going to workit later.


CO2_1 = CO2_1.drop(['id'], axis=1)
CO2_2 = CO2_2.drop(['ID'], axis=1)
CO2_3 = CO2_3.drop(['id'], axis=1)
CO2_4 = CO2_4.drop(['id'], axis=1)
CO2_5 = CO2_5.drop(['id'], axis=1)
CO2_6 = CO2_6.drop(['id'], axis=1)
CO2_7 = CO2_7.drop(['id'], axis=1)
CO2_8 = CO2_8.drop(['id'], axis=1)
CO2_9 = CO2_9.drop(['id'], axis=1)
CO2_10 = CO2_10.drop(['id'], axis=1)
#CO2_11 = CO2_11.drop(['id'], axis=1)
#CO2_12 = CO2_12.drop(['id'], axis=1)
#CO2_13 = CO2_13.drop(['id'], axis=1)

CO2_2.head()

# Before concatenating the archives we are going to normalize
#the tables first. First to lower case to normalize the archives
# from the 2010 und 2014 datasets.

CO2_1.insert(1,'TAN',1)
CO2_1.insert(1,'ep (KW)',1)

CO2_7.insert(1,'mp',1)

CO2_1.columns = CO2_1.columns.str.lower()
CO2_2.columns = CO2_2.columns.str.lower()
CO2_3.columns = CO2_3.columns.str.lower()
CO2_4.columns = CO2_4.columns.str.lower()
CO2_5.columns = CO2_5.columns.str.lower()
CO2_6.columns = CO2_6.columns.str.lower()
CO2_7.columns = CO2_7.columns.str.lower()
CO2_8.columns = CO2_8.columns.str.lower()
CO2_9.columns = CO2_9.columns.str.lower()
CO2_10.columns = CO2_10.columns.str.lower()

#Create the year field. Every year field value will have the year that identifies every archive. ( Not active)
'''
CO2_1['year'] = 2010
CO2_2['year'] = 2011
CO2_3['year'] = 2012
CO2_4['year'] = 2013
CO2_5['year'] = 2013
CO2_6['year'] = 2013
CO2_7['year'] = 2014
CO2_8['year'] = 2014
CO2_9['year'] = 2015
CO2_10['year'] = 2016
CO2_11['year'] = 2017
CO2_12['year'] = 2018
CO2_13['year'] = 2019
'''

# Concatenate the archives from 2014 to 2016. And to check later the null values. # The computer can not modelate efficiently more than three years. The percentage of NaN is as follows:

#tables = [CO2_1,CO2_2,CO2_3,CO2_4,CO2_5,CO2_6,CO2_7,CO2_8,CO2_9,CO2_10]

tables = [CO2_8,CO2_9,CO2_10]

CO2_Dataset_O = pd.concat(tables,axis = 0)



#CO2_Dataset_O.dropna(subset=['e (g/km)'], inplace = True)

CO2_Dataset_O.isnull().sum().sort_values(ascending = False)*100/len(CO2_Dataset_O)
CO2_Dataset_O.shape

# Divide Dataframe CO2_Dataset_O into two Dataframes: CO2_Dataset and CO2_Dataset2. The division of the tables is not active.

CO2_Dataset = CO2_Dataset_O
'''
CO2_Dataset = CO2_Dataset_O.iloc[:1410885]
CO2_Dataset2 = CO2_Dataset_O.iloc[1410886:]
'''

"""Drop all the columns that we are not going to use for the training and modeling of the new concatenated Dataset."""

CO2_Dataset = CO2_Dataset.drop(columns=['man','mms','ep (kw)','mk','ve','t','va','ve','z (wh/km)','it','w (mm)','er (g/km)','at2 (mm)','at1 (mm)','cn','mh','fm','tan'])

# Change the original archives column names for a more meaningfull and describtiv names and display the info from this dataset.

Dictionary = {'ms':'Member_state', 'mp':'Manufacturer_pooling',
                   'mh':'Manufacturer_name_EU', 'man':'Manufacturer_name_om',
                   'mms':'Manufacturer_name_ms','tan':'Type_approval_number',
                   't':'Type','va':'Variant','ve':'Version','mk':'Make','cn':'Commercial_name',
                   'ct':'Category_vehicule','r':'Total_new_registration',
                   'e (g/km)':'CO2_emission','m (kg)':'Mass_in_running_kg',
                   'w (mm)':'Wheel_base_mm','at1 (mm)':'Axle_width_steering_axle_mm',
                   'at2 (mm)':'Axle_width_other_axle_mm','ft':'Fuel_type', 'fm':'Fuel_mode',
                   'ec (cm3)':'Engine_capacity_cm3','ep (kw)':'Engine_power_kw',
                   'z (wh/km)':'Electric_energy_consumption_wh_km','it':'Innovative_technology',
                   'er (g/km)':'Emision_reduction_inno_tech_g_km' }



CO2_Dataset = CO2_Dataset.rename(Dictionary, axis =1)


CO2_Dataset.reset_index(drop=True)
CO2_Dataset.head()

# Display the info for the categorical values  and the uniqueness from his categories (CO2_Database).
CO2_Dataset.describe(include = 'O')

# Display the info from the nummerical values from the Dataset CO2_Dataset.
CO2_Dataset.describe(include = np.number)

# Identify the duplicates values from CO2_Dataset
CO2_Dataset.duplicated().sum()

# Drop the duplicates from CO2_Dataset und verify the action.

 CO2_Dataset.drop_duplicates(subset=None, keep="first", inplace=True)
 CO2_Dataset.duplicated().sum()

# Verify null values in CO2_Dataset.

CO2_Dataset.isnull().sum().sort_values(ascending = False)*100/len(CO2_Dataset)

CO2_Dataset.isna().sum() #Missing Values

display(CO2_Dataset['CO2_emission'].value_counts()) #Display the categories

display(CO2_Dataset['Engine_capacity_cm3'].value_counts())

CO2_Dataset.dropna(subset=['CO2_emission'], inplace=True)

# Categorise the CO2_emission column in possible 4 categories other levels of emission from 1 to 4 other from low to high level emission:

CO2_Dataset.loc[(CO2_Dataset['CO2_emission']<=136),'CO2_emission']=1
CO2_Dataset.loc[(CO2_Dataset['CO2_emission']>136) & (CO2_Dataset['CO2_emission']<=160) ,'CO2_emission']=2
CO2_Dataset.loc[(CO2_Dataset['CO2_emission']>160) & (CO2_Dataset['CO2_emission']<=240) ,'CO2_emission']=3
CO2_Dataset.loc[(CO2_Dataset['CO2_emission']>240),'CO2_emission']=4

'''

# Filter CO2_Dataset in order to only have category M vehicules. Not active

CO2_Dataset = CO2_Dataset[CO2_Dataset['Category_vehicule'] != 'M1G'] # Only category M
CO2_Dataset = CO2_Dataset[CO2_Dataset['Category_vehicule'] != 'N1G']
CO2_Dataset = CO2_Dataset[CO2_Dataset['Category_vehicule'] != 'N1']
CO2_Dataset = CO2_Dataset[CO2_Dataset['Category_vehicule'] != 'nan']
CO2_Dataset = CO2_Dataset[CO2_Dataset['Category_vehicule'] != 'M1']

'''

# Filter CO2_Dataset in order to only non electric cars.

#CO2_Dataset['Fuel_type'].unique()



CO2_Dataset = CO2_Dataset.drop(CO2_Dataset[(CO2_Dataset['Fuel_type'] == 'Electric') | (CO2_Dataset['Fuel_type'] == 'electric') | (CO2_Dataset['Fuel_type'] == 'ELECTRIC')].index)

CO2_Dataset['Fuel_type'].unique()



#Separate into two CO2 datasets the Target Variable in one Dataset and the other Dataset with the explanatory variables.

df_expvar = CO2_Dataset.drop(['CO2_emission'], axis = 1)


df_targetvar = CO2_Dataset['CO2_emission']

df_targetvar.info()



# Separate df_expvar and df_targetvar in two Datasets: X_train , X_test , y_train,y_test datasets.

X_train, X_test, y_train, y_test = train_test_split(df_expvar,df_targetvar, test_size = 0.20, random_state = 42) # #We are going to split the Database into a training set and a Test se

# Fit and transform the new separated Datasets with StandardScaler

cols = ['Engine_capacity_cm3','Mass_in_running_kg']
sc = StandardScaler()
X_train[cols] = sc.fit_transform(X_train[cols])
X_test[cols] = sc.transform(X_test[cols])





'''
clf = svm.SVC(gamma=0.01,  kernel='poly')
clf.fit(X_train, y_train)  #Support Vector Machine (SVM) separators are a set of supervised learning techniques that solve classification and regression problems
'''
#Other standarization Model

'''
X_train_scaled = preprocessing.scale(X_train)

print(X_train_scaled.mean(axis=0))

print(X_train_scaled.std(axis=0))
...


'''
'''
scaler = preprocessing.StandardScaler().fit(X_train)

X_train_scaled = scaler.transform(X_train)

'''
'''
print(X_train_scaled.mean(axis=0))

print(X_train_scaled.std(axis=0))

'''

'''
clf = svm.SVC(gamma=0.01,  kernel='poly')
clf.fit(X_train_scaled, y_train)
'''
'''
 #Support Vector Machine (SVM) separators are a set of supervised learning techniques that solve classification and regression problems
'''

# Separated the dataframes X_train, y_train, X_test and y_test in two groups for every dataframe : Categorical and Numerical dataframes.
# Categorical data Dataframes

columns_to_drop = ['Engine_capacity_cm3','Mass_in_running_kg','Total_new_registration']


cat_train = X_train.drop(columns = columns_to_drop , axis =1)
cat_test  = X_test.drop(columns = columns_to_drop , axis =1)


# Numerical data Dataframes

columns_to_drop_cat = ['Fuel_type','Member_state','Manufacturer_pooling','Category_vehicule']

num_train= X_train.drop(columns =columns_to_drop_cat, axis = 1 )
num_test = X_test.drop(columns = columns_to_drop_cat, axis = 1)

num_train.head()



# Normalizing the rows value names from the column Fuel_type from cat train and test. The group 1 was assigned to Volkswagen and group na to Daimler.


cat_train['Fuel_type'].replace({'Petrol':'Petrol', 'Diesel':'Diesel', 'E85':'E85', 'LPG':'Lpg', 'Electric':'Electric', 'ELECTRIC':'Electric',
                                'PETROL-ELECTRIC':'Petrol_Electric', 'DIESEL':'Diesel', 'PETROL':'Petrol', 'NG-biomethane':'NG_Biomethane',
                                'Petrol-Electric':'Petrol_Electric', 'Diesel-Electric':'Diesel_Electric', 'nan':'Nan', 'NG-BIOMETHANE':'NG_Biomethane',
                                'Hydrogen':'Hydrogen', 'Biodiesel':'Bio_Diesel', 'Diesel-electric':'Diesel_Electric', 'petrol':'Petrol', 'diesel':'Diesel',
                                'electric':'Electric', 'petrol-electric':'Petrol_Electric', 'hydrogen':'Hydrogen', 'diesel-electric':'Diesel_Electric',
                                'DIESEL-ELECTRIC':'Diesel_Electric', 'Petrol-electric':'Petrol_Electric', 'Petrol-Gas':'Petrol_Gas', 'HYDROGEN':'Hydrogen',
                                'BIODIESEL':'Bio_Diesel', 'HYBRID/PETROL/E':'Hybrid_Petrol_E', 'PETROL PHEV':'Petrol_Phev', 'PETROL/ELECTRIC':'Petrol_Electric',
                                'petrol ':'Petrol','PETROL ':'Petrol', 'OTHER':'Other','Diesel/Electric':'Diesel_Electric',
                                'Petrol/Electric':'Petrol_Electric','NG-Biomethane':'NG_Biomethane','NG_biomethane':'NG_Biomethane' },inplace = True) #To normalize the names of the types of Fuel

cat_test['Fuel_type'].replace({'Petrol':'Petrol', 'Diesel':'Diesel', 'E85':'E85', 'LPG':'Lpg', 'Electric':'Electric', 'ELECTRIC':'Electric',
                               'PETROL-ELECTRIC':'Petrol_Electric', 'DIESEL':'Diesel', 'PETROL':'Petrol', 'NG-biomethane':'NG_Biomethane',
                               'Petrol-Electric':'Petrol_Electric', 'Diesel-Electric':'Diesel_Electric', 'nan':'Nan', 'NG-BIOMETHANE':'NG_Biomethane',
                               'Hydrogen':'Hydrogen', 'Biodiesel':'Bio_Diesel', 'Diesel-electric':'Diesel_Electric', 'petrol':'Petrol', 'diesel':'Diesel',
                               'electric':'Electric', 'petrol-electric':'Petrol_Electric', 'hydrogen':'Hydrogen', 'diesel-electric':'Diesel_Electric',
                               'DIESEL-ELECTRIC':'Diesel_Electric', 'Petrol-electric':'Petrol_Electric', 'Petrol-Gas':'Petrol_Gas', 'HYDROGEN':'Hydrogen',
                               'BIODIESEL':'Bio_Diesel', 'HYBRID/PETROL/E':'Hybrid_Petrol_E', 'PETROL PHEV':'Petrol_Phev', 'PETROL/ELECTRIC':'Petrol_Electric',
                               'petrol ':'Petrol','PETROL ':'Petrol', 'OTHER':'Other','Diesel/Electric':'Diesel_Electric',
                               'Petrol/Electric':'Petrol_Electric','NG-Biomethane':'NG_Biomethane','NG_biomethane':'NG_Biomethane' },inplace = True) #To normalize the names of the types of Fuel

# Normalizing the rows values from the column name Member_state.

Data1 = ['AT', 'BE', 'BG', 'CY', 'CZ', 'DE', 'DK', 'EE', 'ES', 'FI', 'FR','GB', 'GR', 'HU', 'IE', 'IT', 'LT', 'LU', 'LV', 'MT','M1', 'NL','NO','UK', 'PL','PT', 'RO', 'SE', 'SI', 'HR']


Data2 = ['Austria','Belgium','Bulgary', 'Cyprus', 'Czech Republic', 'Deutschland', 'Denmark', 'Estonia', 'Spain','Finland','France','Great Britain',
         'Greece','Hungary','Ireland','Italy','Lituane','Luxembourg','Latvia','Malta', 'M1','Netherland','Norway','United Kingdom', 'Poland','Portugal','Romania', 'Sweden','Slovenia','Croatia']

cat_train['Member_state']= cat_train['Member_state'].replace(Data1,Data2)
cat_test['Member_state']= cat_test['Member_state'].replace(Data1,Data2)

# Normalizing the rows values from the column name Manufacturing Pooling.

cat_test['Manufacturer_pooling'].replace({'FORD POOL':'Ford', 'JLT POOL':'Jlt', 'SUZUKI POOL':'Suzuki',1:'Volkswagen','MITSUBISHI POOL':'Mitsubishi',
                                           'DAIMLER AG':'Daimler', 'FORD-WERKE GMBH':'Ford', 'HONDA MOTOR EUROPE LTD':'Honda','SUZUKI':'Suzuki',
                                           'MITSUBISHI MOTORS':'Mitsubishi', 'VW GROUP PC':'Volkswagen', 'POOL RENAULT':'Renault','BMW GROUP':'Bmw',
                                           'FIAT GROUP AUTOMOBILES SPA':'Fiat','TOYOTA -DAIHATSU GROUP':'Toyota', 'GENERAL MOTORS':'Gm','TATA MOTORS LTD':'Tata',
                                           'JAGUAR CARS LTD':'Jaguar','na':'Daimler','LAND ROVER':'Land Rover','HYUNDAI':'Hyundai', 'KIA':'Kia',
                                           'FCA ITALY SPA':'Fca', 'RENAULT':'Renault','TATA MOTORS LTD':'Tata', 'JAGUAR CARS LTD':'Jaguar','LAND ROVER':'Land Rover',
                                           'TOYOTA-DAHAITSU GROUP':'Toyota','TATA MOTORS LTD, JAGUAR CARS LTD , LAND ROVER':'Several','TATA MOTORS LTD, JAGUAR CARS LTD, LAND ROVER':'Several'},inplace = True)


cat_train['Manufacturer_pooling'].replace({'FORD POOL':'Ford', 'JLT POOL':'Jlt', 'SUZUKI POOL':'Suzuki',1:'Volkswagen','MITSUBISHI POOL':'Mitsubishi',
                                           'DAIMLER AG':'Daimler', 'FORD-WERKE GMBH':'Ford', 'HONDA MOTOR EUROPE LTD':'Honda','SUZUKI':'Suzuki',
                                           'MITSUBISHI MOTORS':'Mitsubishi', 'VW GROUP PC':'Volkswagen', 'POOL RENAULT':'Renault','BMW GROUP':'Bmw',
                                           'FIAT GROUP AUTOMOBILES SPA':'Fiat','TOYOTA -DAIHATSU GROUP':'Toyota', 'GENERAL MOTORS':'Gm','TATA MOTORS LTD':'Tata',
                                           'JAGUAR CARS LTD':'Jaguar','na':'Daimler','LAND ROVER':'Land Rover','HYUNDAI':'Hyundai', 'KIA':'Kia',
                                           'FCA ITALY SPA':'Fca','RENAULT':'Renault','TATA MOTORS LTD':'Tata', 'JAGUAR CARS LTD':'Jaguar','LAND ROVER':'Land Rover',
                                           'TOYOTA-DAHAITSU GROUP':'Toyota','TATA MOTORS LTD, JAGUAR CARS LTD , LAND ROVER':'Several','TATA MOTORS LTD, JAGUAR CARS LTD, LAND ROVER':'Several'},inplace = True)



# Filter the data. We have to work only with non electric engine vehicules.

# Fill the NaN with the simple imputer For Categorical Variables.

imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')

cat_train[['Fuel_type','Member_state','Manufacturer_pooling','Category_vehicule']] = imputer.fit_transform(cat_train[['Fuel_type','Member_state','Manufacturer_pooling','Category_vehicule']])

cat_test[['Fuel_type','Member_state','Manufacturer_pooling','Category_vehicule']] = imputer.transform(cat_test[['Fuel_type','Member_state','Manufacturer_pooling','Category_vehicule']])

# Fill the NaN with the simple imputer For numerical variables

imputer2 = SimpleImputer(missing_values=np.nan, strategy='median')

num_train[['Engine_capacity_cm3','Mass_in_running_kg','Total_new_registration']] = imputer2.fit_transform(num_train[['Engine_capacity_cm3','Mass_in_running_kg','Total_new_registration']])

num_test[['Engine_capacity_cm3','Mass_in_running_kg','Total_new_registration']] = imputer2.transform(num_test[['Engine_capacity_cm3','Mass_in_running_kg','Total_new_registration']])



# To confirm that there is no NaN values.

cat_test.isnull().sum().sort_values(ascending = False)*100/len(cat_test)

# To confirm that the values for the row Manufacturer Pooling ist normalized

cat_train['Manufacturer_pooling'].unique()
#cat_test['Manufacturer_pooling'].unique()


#cat_train['Manufacturer_pooling'].value_counts()

"""Our algorithms work on computations and only take numerical values as input, strings cannot be processed. It is therefore important to encode the categorical variables we want to use.**

Encode the modalities of the explanatory variables fueltype, aspiration using a OneHotEncoder. Be careful to apply the transformations rigorously over the training and test datasets.
"""

ohe = OneHotEncoder(handle_unknown = 'ignore', sparse_output = False).set_output(transform = 'pandas')

ohetransform1 = ohe.fit_transform(cat_train[['Manufacturer_pooling']])
ohetransform2 = ohe.transform(cat_test[['Manufacturer_pooling']])
ohetransform3 = ohe.fit_transform(cat_train[['Member_state']])
ohetransform4 = ohe.transform(cat_test[['Member_state']])
ohetransform5 = ohe.fit_transform(cat_train[['Fuel_type']])
ohetransform6 = ohe.transform(cat_test[['Fuel_type']])

# Concatenate the encoded columns and drop the original columns from Manufacturer pooling and Member state.

cat_train = pd.concat([cat_train,ohetransform1,ohetransform3,ohetransform5],axis = 1).drop(columns = ['Manufacturer_pooling','Member_state','Fuel_type'])

cat_test = pd.concat([cat_test,ohetransform2,ohetransform4,ohetransform6],axis = 1).drop(columns = ['Manufacturer_pooling','Member_state','Fuel_type'])

# Delete the column 'Manufacturer_pooling_na'
'''
del cat_train['Manufacturer_pooling_na']
del cat_test['Manufacturer_pooling_na']

'''

# Drop the NaN from the cat train and test.

cat_train.dropna()
cat_test.dropna()

#Delete the column Category Vehicule

del cat_train['Category_vehicule']
del cat_test['Category_vehicule']

num_train.info()

#Reindex the datasets cat train and test. Reindex num train and test.
'''
cat_train = cat_train.reset_index()
num_train = num_train.reset_index()
cat_test = cat_test.reset_index()
num_test = num_test.reset_index()
'''
#Concat the datasets cat_train and num_train. Concat cat_test and num_test

Recon_train = pd.concat([cat_train,num_train], axis = 1) # Reconstitute the training and test sets by concatenating train_num2 with cat_train2 and test_num2 with cat_test2
Recon_test = pd.concat([cat_test,num_test], axis = 1)

'''
Recon_train = Recon_train.reset_index()
Recon_test = Recon_test.reset_index()
'''

Recon_test.head(5)

# Drop NaN from Recon train and test. Also y test and train.

Recon_train.dropna()
Recon_test.dropna()
y_test.dropna()
y_train.dropna()

# Fill empty spaces for Recon train and test and y train and test.

Recon_train = Recon_train.fillna(0)
Recon_test = Recon_test.fillna(0)
y_train = y_train.fillna(0)
y_test = y_test.fillna(0)

"""#**(1) Multiclass Classification: Our data is ready, we can now proceed to the modeling part. For each of the models we are going to implement, we are going to evaluate them thanks to the confusion matrix and the usual metrics: the score, the recall, the precision and the f1-score.**"""

model_1 = LogisticRegression(solver='lbfgs', max_iter=100000000)
model_2 = DecisionTreeClassifier()
model_3 = RandomForestClassifier()
model_4 = XGBClassifier()
model_5 = LGBMClassifier()
model_6 = LinearRegression()
model_7 = DecisionTreeRegressor(random_state=42)

Recon_train

from sklearn.linear_model import LinearRegression

model_6.fit(Recon_train, y_train)

from sklearn.linear_model import LinearRegression

regressor = LinearRegression()
regressor.fit(Recon_train, y_train)


coeffs = list(regressor.coef_)
coeffs.insert(0, regressor.intercept_)

feats2 = list(df_expvar.columns)
feats2.insert(0, 'Intercept')

coefficients = pd.DataFrame({'Estimated value': coeffs})
#coefficients = pd.DataFrame({'Estimated value': coeffs}, index=feats2)
display(coefficients)

#Display the coefficient of determination (R²) on the training and test data set.

print('Coefficient of determination of the model on the train set :', regressor.score(Recon_train, y_train))
print('Coefficient of determination of the model on the test set', regressor.score(Recon_test, y_test))

'''
CO2_Dataset
df_expvar
df_targetvar
df_clf
'''
'''
print('Coefficient of determination of the model on the train set :', regressor.score(Recon_train, y_train))
print('Coefficient of determination of the model on the test set', regressor.score(Recon_test, y_test))
'''

import matplotlib.pyplot as plt

fig = plt.figure(figsize = (10,10))
pred_test = regressor.predict(Recon_test)
plt.scatter(pred_test, y_test, c='green')

plt.plot((y_test.min(), y_test.max()), (y_test.min(), y_test.max()), color = 'red')

plt.xlabel("Predicted values")
plt.ylabel("True values")
plt.title('Linear regression for car price prediction')

plt.show()

from sklearn.tree import DecisionTreeRegressor

model_7.fit(Recon_train, y_train)

print(regressor.score(Recon_train, y_train))

print(regressor.score(Recon_test, y_test))

'''
Classification
Our data are ready, we can now proceed to the modeling part. For each of the models we are going
to implement, we are going to evaluate them thanks to the confusion matrix and the usual metrics:
the score, the recall, the precision and the f1-score.

   These metrics and their interpretations are detailed in the notebook 3 "Linear et Tree-based models".
3.1. Logistic Regression
(a)
(1) Instantiate a LogisticRegression model and train it on your training data.
(2) Evaluate the model using the score method on the training and test data.

20 minutes long

'''

'''
model_1.fit(Recon_train,y_train)
pred = model_1.predict(Recon_test)
accuracy_score(y_test,pred)



reglog = LogisticRegression()
reglog.fit(Recon_train, y_train)

print('Score on the train set', reglog.score(Recon_train, y_train))
print('Score on the test set', reglog.score(Recon_test, y_test))
'''

#Evaluate your model by displaying the classification_report and displaying the confusion matrix of it. What can you conclude?


'''
y_pred = reglog.predict(Recon_test)

display(pd.crosstab(y_test,y_pred, rownames=['True'], colnames=['Prediction']))

print(classification_report(y_test, y_pred, ))
'''



"""#**B.DecisionTreeClassifier**

#**(1) Build a decision tree model and train it on your training data.**
#**(2) Evaluate the model using the score method on your training and test data.**
"""

model_2.fit(Recon_train,y_train)
pred = model_2.predict(Recon_test)
accuracy_score(y_test,pred)

from sklearn import tree

clf = tree.DecisionTreeClassifier()
clf.fit(Recon_train, y_train)

print('Score on the train set ', clf.score(Recon_train, y_train))
print('Score on the test set', clf.score(Recon_test, y_test))

from sklearn.metrics import classification_report

y_pred = clf.predict(Recon_test)

display(pd.crosstab(y_test,y_pred, rownames=['True'], colnames=['Prediction']))
print(classification_report(y_test, y_pred))

"""# Feature Importance and Depth"""

print("Feature importances:", clf.feature_importances_)
print("Tree depth:", clf.get_depth())

# Import necessary libraries
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report
import pandas as pd

# Step 1: Instantiate the Decision Tree model
clf = DecisionTreeClassifier()

# Step 2: Train the model on the training data
clf.fit(Recon_train, y_train)

# Step 3: Evaluate the model
# Get accuracy on training and test data
train_score = clf.score(Recon_train, y_train)
test_score = clf.score(Recon_test, y_test)

print('Score on the train set:', train_score)
print('Score on the test set:', test_score)

# Step 4: Make predictions on the test set
y_pred = clf.predict(Recon_test)

# Step 5: Display the results
# Confusion matrix
conf_matrix = pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Prediction'])
display(conf_matrix)

# Classification report
print(classification_report(y_test, y_pred))

# Step 6: Display feature importance
feature_importances = clf.feature_importances_

# Assuming Recon_train is a DataFrame, use column names for better readability
feature_importance_df = pd.DataFrame({
    'Feature': Recon_train.columns,
    'Importance': feature_importances
})

# Sort by importance
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Display feature importance
display(feature_importance_df)

"""Classification Report and Confusion Matrix: These provide insights into model performance on each class, which is especially useful for imbalanced datasets."""

# Step 3: Evaluate the model
# Get accuracy on training and test data
train_score = clf.score(Recon_train, y_train)
test_score = clf.score(Recon_test, y_test)

print('Score on the train set:', train_score)
print('Score on the test set:', test_score)

""" Tuning with RandomizedSearchCV"""

from sklearn.model_selection import RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier
import numpy as np

# Define the parameter distribution
param_dist = {
    'max_depth': [None, 10, 20, 30, 40],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'criterion': ['gini', 'entropy'],
    'max_features': ['auto', 'sqrt', 'log2']
}

# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=clf, param_distributions=param_dist,
                                   n_iter=50, cv=5, n_jobs=-1, scoring='accuracy', random_state=42)

# Fit RandomizedSearchCV to the training data
random_search.fit(Recon_train, y_train)

# Get the best parameters and best score
print("Best Parameters:", random_search.best_params_)
print("Best Score:", random_search.best_score_)

# Evaluate on the test set with the best model
best_model_random = random_search.best_estimator_
test_accuracy_random = best_model_random.score(Recon_test, y_test)
print("Test Set Accuracy with Tuned Parameters (RandomizedSearch):", test_accuracy_random)

"""#**C.Random Forest Classifier**"""

model_3.fit(Recon_train,y_train)
pred = model_3.predict(Recon_test)
accuracy_score(y_test,pred)

from sklearn.ensemble import RandomForestClassifier #8 minutes process.

rf = RandomForestClassifier()
rf.fit(Recon_train, y_train)

print('Score on the train set', rf.score(Recon_train, y_train))
print('Score on the test set', rf.score(Recon_test, y_test))

y_pred = rf.predict(Recon_test)

print(pd.crosstab(y_test,y_pred))


print(classification_report(y_test, y_pred))

#4. A word about class imbalance
#Display the percentage distribution of the target variable.

df_targetvar.value_counts(normalize=True)

#Run the next cell to perform this resampling and display the new distribution


rOs = RandomOverSampler()
X_ro, y_ro = rOs.fit_resample(Recon_train, y_train)
print('Oversampled sample classes :', dict(pd.Series(y_ro).value_counts(normalize = True)))

#) Train your best model with the resampled data and comment on the results.



rf = RandomForestClassifier()
rf.fit(X_ro, y_ro)

print('Score on train set', rf.score(X_ro, y_ro))
print('Score on test set', rf.score(Recon_test, y_test))

from sklearn.metrics import classification_report

y_pred = rf.predict(Recon_test)


print(pd.crosstab(y_test,y_pred))
print(classification_report(y_test, y_pred))

"""# Feature Importance"""

import matplotlib.pyplot as plt
import numpy as np

# Step 1: Retrieve feature importance from the trained model
feature_importances = rf.feature_importances_

# Step 2: Create a DataFrame to organize feature importances with their corresponding feature names
feature_names = Recon_train.columns
importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})

# Step 3: Sort features by importance
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Display the sorted DataFrame to observe top features
print(importance_df)

# Step 4: Plot feature importance
plt.figure(figsize=(10, 8))
plt.barh(importance_df['Feature'], importance_df['Importance'], align='center')
plt.gca().invert_yaxis()  # Most important feature at the top
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance in Random Forest')
plt.show()

"""#**D.Other Models**"""

'''
model_4.fit(Recon_train,y_train)
pred = model_4.predict(Recon_test)
accuracy_score(y_test,pred)
'''

model_5.fit(Recon_train,y_train)
pred = model_5.predict(Recon_test)
accuracy_score(y_test,pred)

from sklearn.tree import DecisionTreeClassifier

df_clf =DecisionTreeClassifier(max_depth = 3, min_samples_leaf = 25, random_state=42)
df_clf.fit(Recon_train,y_train)

y_pred_test = df_clf.predict(Recon_test)

print(y_pred_test[0:5])


from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score
confusion_matrix(y_test, y_pred_test)

print('Score on train set', df_clf.score(Recon_train, y_train))

print('Score on test set', df_clf.score(Recon_test, y_test))

model = LinearRegression()

model.fit(Recon_train,y_train)

print(model.intercept_)
print(model.coef_[0])

from sklearn.metrics import classification_report

print(classification_report(y_test,y_pred_test ))

"""#**Run the following cell to display the tree. Decision Tree Classifier**"""

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

fields3 =['Manufacturer_pooling_Bmw','Manufacturer_pooling_Daimler','Manufacturer_pooling_Fca','Manufacturer_pooling_Fiat','Manufacturer_pooling_Ford','Manufacturer_pooling_Gm','Manufacturer_pooling_Honda','Manufacturer_pooling_Hyundai','Manufacturer_pooling_Kia','Manufacturer_pooling_Mitsubishi','Manufacturer_pooling_Renault','Manufacturer_pooling_Several','Manufacturer_pooling_Suzuki','Manufacturer_pooling_Toyota','Manufacturer_pooling_Volkswagen','Member_state_Austria','Member_state_Belgium','Member_state_Bulgary','Member_state_Croatia','Member_state_Cyprus','Member_state_Czech Republic','Member_state_Denmark','Member_state_Deutschland','Member_state_Estonia','Member_state_Finland','Member_state_France','Member_state_Great Britain','Member_state_Greece','Member_state_Hungary','Member_state_Ireland','Member_state_Italy','Member_state_Latvia','Member_state_Lituane','Member_state_Luxembourg','Member_state_Malta','Member_state_Netherland','Member_state_Poland','Member_state_Portugal','Member_state_Romania','Member_state_SK','Member_state_Slovenia','Member_state_Spain','Member_state_Sweden','Fuel_type_Bio_Diesel','Fuel_type_Diesel','Fuel_type_Diesel_Electric','Fuel_type_E85','Fuel_type_Hydrogen','Fuel_type_Lpg','Fuel_type_NG_Biomethane','Fuel_type_Petrol','Fuel_type_Petrol_Electric','Fuel_type_Petrol_Gas','Total_new_registration','Mass_in_running_kg','Engine_capacity_cm3']

plt.figure(figsize=(20, 20))
plot_tree(df_clf,feature_names= fields3,class_names = ['1.','2.','3.','4.'],rounded=True, # Rounded node edges
          filled=True, # Adds color according to class
          proportion=True); # Displays the proportions of class samples instead of the whole number of samples

model = DecisionTreeClassifier(random_state=12345, max_depth=8,class_weight='balanced')
model.fit(Recon_train, y_train)
model.feature_importances_


feat_importances = pd.DataFrame(model.feature_importances_, index=Recon_train.columns, columns=["Importance"])
feat_importances.sort_values(by='Importance', ascending=False, inplace=True)
feat_importances.plot(kind='bar', figsize=(8,6))



from sklearn.tree import DecisionTreeRegressor #13 minutes

from sklearn.ensemble import RandomForestRegressor
import sklearn.metrics
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error

### DecisionTree

regressor_decision_tree = DecisionTreeRegressor(random_state=42)

regressor_decision_tree.fit(Recon_train, y_train)

y_pred_decision_tree = regressor_decision_tree.predict(Recon_test)
y_pred_train_decision_tree = regressor_decision_tree.predict(Recon_train)

# Metrics

# Training set
mae_decision_tree_train = mean_absolute_error(y_train,
                                              y_pred_train_decision_tree)
mse_decision_tree_train = mean_squared_error(y_train,
                                             y_pred_train_decision_tree,
                                             squared=True)
rmse_decision_tree_train = mean_squared_error(y_train,
                                              y_pred_train_decision_tree,
                                              squared=False)

# Test set
mae_decision_tree_test = mean_absolute_error(y_test, y_pred_decision_tree)
mse_decision_tree_test = mean_squared_error(y_test,
                                            y_pred_decision_tree,
                                            squared=True)
rmse_decision_tree_test = mean_squared_error(y_test,
                                             y_pred_decision_tree,
                                             squared=False)

### RandomForest

regressor_random_forest = RandomForestRegressor(random_state=42)

regressor_random_forest.fit(Recon_train, y_train)

# Metrics

y_pred_random_forest = regressor_random_forest.predict(Recon_test)
y_pred_random_forest_train = regressor_random_forest.predict(Recon_train)

# Training set

mae_random_forest_train = mean_absolute_error(y_train,
                                              y_pred_random_forest_train)
mse_random_forest_train = mean_squared_error(y_train,
                                             y_pred_random_forest_train,
                                             squared=True)
rmse_random_forest_train = mean_squared_error(y_train,
                                              y_pred_random_forest_train,
                                              squared=False)

# Test set

mae_random_forest_test = mean_absolute_error(y_test, y_pred_random_forest)
mse_random_forest_test = mean_squared_error(y_test,
                                            y_pred_random_forest,
                                            squared=True)
rmse_random_forest_test = mean_squared_error(y_test,
                                             y_pred_random_forest,
                                             squared=False)

# Creation of a dataframe to compare the metrics of the two algorithms

data = {
    'MAE train': [mae_decision_tree_train, mae_random_forest_train],
    'MAE test': [mae_decision_tree_test, mae_random_forest_test],
    'MSE train': [mse_decision_tree_train, mse_random_forest_train],
    'MSE test': [mse_decision_tree_test, mse_random_forest_test],
    'RMSE train': [rmse_decision_tree_train, rmse_random_forest_train],
    'RMSE test': [rmse_decision_tree_test, rmse_random_forest_test]
}

# DataFrame

df = pd.DataFrame(data, index=['Decision Tree', 'Random Forest '])

df.head()



"""# ** Hiperparameter tunnig using Grid Search **"""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier()
rf.fit(Recon_train, y_train)

print('Score on the train set', rf.score(Recon_train, y_train))
print('Score on the test set', rf.score(Recon_test, y_test))

y_pred = rf.predict(Recon_test)

print(pd.crosstab(y_test,y_pred))


print(classification_report(y_test, y_pred))



from sklearn.linear_model import Ridge

model = Ridge()

param_grid = {'alpha':[0.001,0.01,0.1,1.0,10.0,100.0],'solver':['auto','svd','lsqr'],}

from sklearn.model_selection import GridSearchCV

grid_search = GridSearchCV(estimator = model, param_grid=param_grid, cv = 5, scoring = 'neg_mean_squared_error')

grid_search.fit(Recon_train,y_train)

best_params = grid_search.best_params_

best_params

best_model = grid_search.best_estimator_

from sklearn.metrics import mean_squared_error

y_pred = best_model.predict(Recon_test)

y_pred

mse = mean_squared_error(y_test, y_pred)

mse

from sklearn.model_selection import RandomizedSearchCV

random_param_grid = [{'n_estimators' :[500,1000,1500],'min_samples_split':[5,10,15],'criterion' :['entropy','gini'],'min_samples_leaf' : [1,2,4],'max_depth':[10,20,30]}]

random_grid_search = RandomizedSearchCV(rf,random_param_grid, cv = 5,scoring = 'accuracy',n_jobs = -1, random_state =26 )

random_grid_search.fit(Recon_train,y_train)

random_grid_search.best_score_

random_grid_search.best_params_

Recon_train.info()