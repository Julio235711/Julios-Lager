# -*- coding: utf-8 -*-
"""DATA CLEANSING_CL_CLEANED

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ClexDCv9r3Kdk-aLPhSqKp3tfYcEwmLw

# Path for CL datasets

# Step 1: Uploading Datasets to Google Drive and import the libraries
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Step 2: Import Libraries"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

"""# Step 3 : Read the files making connection between google drive and google colab"""

# Path for CL Datasets

path_cl_2012 = '/content/drive/MyDrive/DataCO2Project/CLFiles/cl_2012.xlsx' # This is the path where the files are located
path_cl_2013 = '/content/drive/MyDrive/DataCO2Project/CLFiles/cl_2013.xlsx' # This is the path where the files are located
path_cl_2014 = '/content/drive/MyDrive/DataCO2Project/CLFiles/cl_2014.xlsx' # This is the path where the files are located

df_cl_2012 = pd.read_excel(path_cl_2012)
df_cl_2013 = pd.read_excel(path_cl_2013)
df_cl_2014 = pd.read_excel(path_cl_2014)

"""# Step 4: Making security copy files"""

df_cl_1 = df_cl_2012.copy(deep = True)
df_cl_2 = df_cl_2013.copy(deep = True)
df_cl_3 = df_cl_2014.copy(deep = True)

"""# Step 5: Data Inspection

"""

df_cl_1.info()

df_cl_2.info()

df_cl_3.info()

"""# Step 6 Normalizing names for every column name for every CL Dataset. First create the dictionaries for every table. There are three dictionaries because the 2013/14 Dataset has different column name for the field fuel type and the column names in the 2013 dataset are entirely different.

"""

Dictionary_2012 = {'lib_mrq':'brand', 'lib_mod_doss':'Model_file', 'lib_mod':'Model_UTAC',
                   'dscom':'Commercial_name', 'cnit':'Code_National_Identification_Type',
                   'tvv':'Type_Variante_Version(TVV)','typ_cbr':'fuel_type',
                   'hybride':'Hybride','puiss_admin_98':'Administrative_power','puiss_max':'power_maximal (kW)','typ_boite_nb_rapp':'Gearbox',
                   'conso_urb':'Urban_consumption (l/100km)',
                   'conso_exurb':'Extra_urban_consumption(l/100km)','conso_mixte':'Consumption_mix(l/100km)',
                   'co2':'CO2','co_typ_1':'CO_type_I (g/km)',
                   'hc':'hc','nox':'nox',
                   'hcnox':'hcnox','ptcl':'Particles', 'masse_ordma_min':'Empty_mass_min(kg)',
                   'masse_ordma_max':'Empty_mass_max(kg)',
                   'champ_v9':'Champ_V9','date_maj':'Last_update','Carosserie':'Carosserie',
                   'gamme':'range'}


Dictionary_2013 = {'Marque':'brand','Modèle dossier':'Model_file', 'Modèle UTAC':'Model_UTAC',
                   'Désignation commerciale':'Commercial_name', 'CNIT':'Code_National_Identification_Type',
                   'Type Variante Version (TVV)':'Type_Variante_Version(TVV)','Carburant':'fuel_type',
                   'Hybride':'Hybride','Puissance administrative':'Administrative_power','Puissance maximale (kW)':'power_maximal (kW)','Boîte de vitesse':'Gearbox',
                   'Consommation urbaine (l/100km)':'Urban_consumption (l/100km)',
                   'Consommation extra-urbaine (l/100km)':'Extra_urban_consumption(l/100km)','Consommation mixte (l/100km)':'Consumption_mix(l/100km)',
                   'CO2 (g/km)':'CO2','CO type I (g/km)':'CO_type_I (g/km)',
                   'hc':'hc','nox':'nox',
                   'hcnox':'hcnox','Particules (g/km)':'Particles', 'masse vide euro min (kg)':'Empty_mass_min(kg)',
                   'masse vide euro max (kg)':'Empty_mass_max(kg)',
                   'Champ V9':'Champ_V9','Date de mise à jour':'Last_update','Carosserie':'Carosserie',
                   'gamme':'range'}





Dictionary_2014 = {'lib_mrq':'brand','lib_mod_doss':'Model_file', 'lib_mod':'Model_UTAC',
                   'dscom':'Commercial_name', 'cnit':'Code_National_Identification_Type',
                   'tvv':'Type_Variante_Version(TVV)','cod_cbr':'fuel_type',
                   'hybride':'Hybride','puiss_admin_98':'Administrative_power','puiss_max':'power_maximal (kW)','typ_boite_nb_rapp':'Gearbox',
                   'conso_urb':'Urban_consumption (l/100km)',
                   'conso_exurb':'Extra_urban_consumption(l/100km)','conso_mixte':'Consumption_mix(l/100km)',
                   'co2':'CO2','co_typ_1':'CO_type_I (g/km)',
                   'hc':'hc','nox':'nox',
                   'hcnox':'hcnox','ptcl':'Particles', 'masse_ordma_min':'Empty_mass_min(kg)',
                   'masse_ordma_max':'Empty_mass_max(kg)',
                   'champ_v9':'Champ_V9','date_maj':'Last_update','Carosserie':'Carosserie',
                   'gamme':'range'}


df_cl_1 = df_cl_1.rename(Dictionary_2012, axis = 1)
df_cl_2 = df_cl_2.rename(Dictionary_2013, axis = 1)
df_cl_3 = df_cl_3.rename(Dictionary_2014, axis = 1)

"""# Step 6.1 Changing the datatype fitting the variable"""

df_cl_1['Urban_consumption (l/100km)'] = pd.to_numeric(df_cl_1['Urban_consumption (l/100km)'], errors='coerce')
df_cl_2['Urban_consumption (l/100km)'] = pd.to_numeric(df_cl_2['Urban_consumption (l/100km)'], errors='coerce')
df_cl_3['Urban_consumption (l/100km)'] = pd.to_numeric(df_cl_3['Urban_consumption (l/100km)'], errors='coerce')

df_cl_1['Extra_urban_consumption(l/100km)'] = pd.to_numeric(df_cl_1['Extra_urban_consumption(l/100km)'], errors='coerce')
df_cl_2['Extra_urban_consumption(l/100km)'] = pd.to_numeric(df_cl_2['Extra_urban_consumption(l/100km)'], errors='coerce')
df_cl_3['Extra_urban_consumption(l/100km)'] = pd.to_numeric(df_cl_3['Extra_urban_consumption(l/100km)'], errors='coerce')


df_cl_1['Consumption_mix(l/100km)'] = pd.to_numeric(df_cl_1['Consumption_mix(l/100km)'], errors='coerce')
df_cl_2['Consumption_mix(l/100km)'] = pd.to_numeric(df_cl_2['Consumption_mix(l/100km)'], errors='coerce')
df_cl_3['Consumption_mix(l/100km)'] = pd.to_numeric(df_cl_3['Consumption_mix(l/100km)'], errors='coerce')


df_cl_1['Last_update'] = pd.to_datetime(df_cl_1['Last_update'], errors='coerce')
df_cl_2['Last_update'] = pd.to_datetime(df_cl_2['Last_update'], errors='coerce')
df_cl_3['Last_update'] = pd.to_datetime(df_cl_3['Last_update'], errors='coerce')

"""# Step 7: Concatenate all the tables from CL and called it CL Union"""

cl_union_cleaned = pd.concat([df_cl_1,df_cl_2,df_cl_3], axis = 0) #Table that represents the total data from 2012 to 2014


cl_union_cleaned.info()

"""# Step 8: Check for missing values in the united dataset"""

cl_union_cleaned.isnull().sum().sort_values(ascending = False)*100/len(cl_union_cleaned)

cl_union_cleaned.head()

"""### **CHECKING MISSING VALUES**"""

display(cl_union_cleaned.isna().sum())

"""# Step 9: Check for duplicates"""

# Check all column names to identify any with '_x' or '_y'
print(cl_union_cleaned.columns)

# If 'fuel_type_x' is the correct one, rename it to 'fuel_type'
cl_union_cleaned.rename(columns={'fuel_type_x': 'fuel_type'}, inplace=True)

# Optionally, if 'fuel_type_y' exists and is redundant, you can drop it
cl_union_cleaned.drop(columns=['fuel_type_y'], inplace=True, errors='ignore')

# Verify the changes
print(cl_union_cleaned.columns)

cl_union_cleaned.isna().any(axis=1).count()

"""# Step 10: Summary Statistics: Use .describe() to get a summary of the data, including mean, median, min, max, and quartiles, which can help identify outliers and data inconsistencies.

"""

cl_union_cleaned.describe()

print("Column names and data types in the DataFrame:")
print(cl_union_cleaned.dtypes)

"""### **Standardizing THE DATA CL_UNION FROM COLUMN FUEL TYPEvand grouping after countries**

# Step 11: Substitute all blank values for the mode in fuel type and standardize in column fuel types
"""

import pandas as pd

def categorize_fuel_type(fuel_type):
    if pd.isna(fuel_type):
        return 'NaN'
    elif fuel_type == 'EL':
        return 'Electric'
    elif fuel_type == 'GP':
        return 'Liquified_Petroleum_Gas'
    elif fuel_type == 'FE':
        return 'Superethanol_E85'
    elif fuel_type == 'ES':
        return 'Petrol_95'
    elif fuel_type == 'GO':
        return 'Diesel'
    elif fuel_type == 'GP/ES':
        return 'Dual_fuel_petrol/Liquified_Petroleum_Gas'
    elif fuel_type == 'EE':
        return 'Hybrid_Petrol/Electric_plug_in'
    elif fuel_type == 'ES/GP':
        return 'Dual_fuel_petrol/Liquified_Petroleum_Gas'
    elif fuel_type == 'GN':
        return 'Single_fuel_Natural_Gas'
    elif fuel_type == 'GN/ES':
        return 'Dual_fuel_petrol/Natural_Gas'
    elif fuel_type == 'ES/GN':
        return 'Dual_fuel_petrol/Natural_Gas'
    elif fuel_type == 'EH':
        return 'Hybrid_Petrol/Electric_non_plug_in'
    elif fuel_type == 'GH':
        return 'Hybrid_Diesel/Electric_non_plug_in'
    elif fuel_type == 'GL':
        return 'Hybrid_Diesel/Electric_plug_in'
    else:
        return 'Diesel'

# Apply the categorization function to the 'fuel_type' column
cl_union_cleaned['fuel_type'] = cl_union_cleaned['fuel_type'].apply(categorize_fuel_type)

# Handle missing values by filling them with the mode of the 'fuel_mode' column
cl_union_cleaned['fuel_type'] = cl_union_cleaned['fuel_type'].fillna(cl_union_cleaned['fuel_type'].mode()[0])

# Check unique values in the 'fuel_mode' column
print(cl_union_cleaned['fuel_type'].unique())

"""## **Standardize the names in column fuel type **

```
# This is formatted as code
```


"""

cl_union_cleaned['fuel_type'].replace({'EL':'Electric', 'GP':'Liquified_Petroleum_Gas', 'FE':'Superethanol_E85', 'ES':'Petrol_95',
       'GO':'Diesel', 'GH':'Hybrid_Diesel/Electric_non_plug_in','EH':'Hybrid_Petrol/Electric_non_plug_in', 'GP/ES':'Dual_fuel_petrol/Liquified_Petroleum_Gas', 'EE':'Hybrid_Petrol/Electric_plug_in', 'ES/GP':'Dual_fuel_petrol/Liquified_Petroleum_Gas',
       'GN':'Single_fuel_Natural_Gas', 'GL': 'Hybrid_Diesel/Electric_plug_in','OTHER':'Other','GN/ES':'Dual_fuel_petrol/Natural_Gas','ES/GN':'Dual_fuel_petrol/Natural_Gas' },inplace = True) #To normalize the names of the types of Fuel


sns.heatmap(pd.crosstab(cl_union_cleaned['fuel_type'],cl_union_cleaned['fuel_type'],margins=True, margins_name= 'Totals',normalize = True, dropna = False),cmap="YlGnBu", annot=True, cbar=True);
plt.title('Fuel Type versus Fuel Mode for Autos -Normalize Data-');
plt.xlabel('fuel Mode');
plt.ylabel('Fuel Type');

"""Diesel Dominance:

Diesel fuel dominates the dataset, making up 87% of the total, as indicated by the dark blue color corresponding to Diesel in both the Fuel Type and Fuel Mode axes.
Petrol 95:

Petrol 95 is the second most common fuel type, representing about 11% of the data. This is also reflected in the corresponding Fuel Mode.
Minor Fuel Types:

Predominance of Traditional Fuels: The overwhelming presence of Diesel and Petrol 95 suggests that traditional fuel types still dominate the automotive sector within this dataset, with alternative fuels playing a minor role.

# STEP 13: CREATE A DICTIONARY FOR THE CAR BRANDS AND ITS COUNTRIES AND GROUPS
"""

# Mapping of car brands to their Group and Country
brand_group_country = {
    # UK
    'LOTUS': ('PROTON', 'UK'),
    'ASTON MARTIN': ('ASTON MARTIN', 'UK'),
    'JAGUAR': ('JLR', 'UK'),
    'JAGUAR LAND ROVER LIMITED': ('JLR', 'UK'),
    'LAND ROVER': ('JLR', 'UK'),
    # Sweden
    'VOLVO': ('GEELY', 'Sweden'),
    # France
    'DANGEL': ('DANGEL', 'France'),
    'CITROEN': ('PSA', 'France'),
    'PEUGEOT': ('PSA', 'France'),
    'DACIA': ('RENAULT', 'France'),
    'RENAULT': ('RENAULT', 'France'),
    'RENAULT-TECH': ('RENAULT', 'France'),
    'RENAULT TECH': ('RENAULT', 'France'),
    'MIA': ('MIA ELECTRIC', 'France'),
    # Russia
    'LADA': ('AVTOVAZ', 'Russia'),
    # Japan
    'SUBARU': ('SUBARU', 'Japan'),
    'HONDA': ('HONDA', 'Japan'),
    'MITSUBISHI': ('MITSUBISHI', 'Japan'),
    'SUZUKI': ('SUZUKI', 'Japan'),
    'MAZDA': ('MAZDA', 'Japan'),
    'INFINITI': ('NISSAN', 'Japan'),
    'NISSAN': ('NISSAN', 'Japan'),
    'DAIHATSU': ('TOYOTA', 'Japan'),
    'LEXUS': ('TOYOTA', 'Japan'),
    'TOYOTA': ('TOYOTA', 'Japan'),
    # USA
    'TESLA': ('TESLA', 'USA'),
    'FORD': ('FORD', 'USA'),
    'CADILLAC': ('GENERAL MOTORS', 'USA'),
    'CHEVROLET': ('GENERAL MOTORS', 'USA'),
    'OPEL': ('GENERAL MOTORS', 'USA'),
    'VAUXHALL': ('GENERAL MOTORS', 'USA'),
    # Germany
    'BMW': ('BMW', 'Germany'),
    'MINI': ('BMW', 'Germany'),
    'ROLLS ROYCE': ('BMW', 'Germany'),
    'ROLLS-ROYCE': ('BMW', 'Germany'),
    'MERCEDES': ('MERCEDES', 'Germany'),
    'SMART': ('MERCEDES', 'Germany'),
    'MERCEDES-BENZ': ('MERCEDES', 'Germany'),
    'MERCEDES-AMG': ('MERCEDES', 'Germany'),
    'MERCEDES AMG': ('MERCEDES', 'Germany'),
    'MAYBACH': ('MERCEDES', 'Germany'),
    'AUDI': ('VOLKSWAGEN', 'Germany'),
    'BENTLEY': ('VOLKSWAGEN', 'Germany'),
    'LAMBORGHINI': ('VOLKSWAGEN', 'Germany'),
    'PORSCHE': ('VOLKSWAGEN', 'Germany'),
    'QUATRO': ('VOLKSWAGEN', 'Germany'),
    'QUATTRO': ('VOLKSWAGEN', 'Germany'),
    'SEAT': ('VOLKSWAGEN', 'Germany'),
    'SKODA': ('VOLKSWAGEN', 'Germany'),
    'VOLKSWAGEN': ('VOLKSWAGEN', 'Germany'),
    # Korea
    'HYUNDAI': ('KOREA', 'Korea'),
    'KIA': ('KOREA', 'Korea'),
    'SSANGYONG': ('KOREA', 'Korea'),
    # Italy
    'ALFA ROMEO': ('FIAT', 'Italy'),
    'ALFA-ROMEO': ('FIAT', 'Italy'),
    'FERRARI': ('FIAT', 'Italy'),
    'FIAT': ('FIAT', 'Italy'),
    'JEEP': ('FIAT', 'Italy'),
    'LANCIA': ('FIAT', 'Italy'),
    'MASERATI': ('FIAT', 'Italy')
}

# Function to map 'brand' to 'Group' and 'Country' using the dictionary
def map_brand_to_group_country(brand):
    # Look up the brand in the dictionary, return ('Unknown', 'Unknown') if not found
    return brand_group_country.get(brand, ('Unknown', 'Unknown'))

# Apply the mapping to the 'brand' column and create two new columns: 'Group' and 'Country'
cl_union_cleaned[['Group', 'Country']] = cl_union_cleaned['brand'].apply(map_brand_to_group_country).apply(pd.Series)

# Verify the result
print(cl_union_cleaned[['brand', 'Group', 'Country']].head())

print(cl_union_cleaned.columns)

# Step 1: Extract unique brands from the 'brand' column in the dataset
unique_brands_in_dataset = cl_union_cleaned['brand'].unique()

# Step 2: Get the keys (brands) from the brand_group_country dictionary
brands_in_dict = set(brand_group_country.keys())

# Step 3: Find brands in the dataset that are missing in the dictionary
missing_brands = [brand for brand in unique_brands_in_dataset if brand not in brands_in_dict]

# Step 4: Display the missing brands
if missing_brands:
    print("Missing brands in the dictionary:", missing_brands)
else:
    print("No missing brands. All brands are accounted for in the dictionary.")

# Group by 'Group' and 'Country' and count the number of cars for each group
grouped_data = cl_union_cleaned.groupby(['brand','Group', 'Country']).size().reset_index(name='car_count')

# Sort the grouped data by 'car_count' in ascending order
grouped_data = grouped_data.sort_values(by='car_count', ascending=False)

# Display the sorted grouped data
print(grouped_data)

# Do NOT overwrite the original DataFrame `cl_union_cleaned`

# Group by 'Group' and 'Country' and count the number of cars for each group
grouped_data = cl_union_cleaned.groupby(['Group', 'Country']).size().reset_index(name='car_count')

# Sort the grouped data by 'car_count' in ascending order
grouped_data = grouped_data.sort_values(by='car_count', ascending=False)

# Display the sorted grouped data
print(grouped_data)

print(cl_union_cleaned.columns)

import matplotlib.pyplot as plt
import seaborn as sns

# Step 5: Visualize the grouped data
plt.figure(figsize=(12, 8))
sns.barplot(x='Group', y='car_count', hue='Country', data=grouped_data, palette='viridis')
plt.title('Number of Cars by Group and Country')
plt.xlabel('Group')
plt.ylabel('Number of Cars')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Group by 'Country' and sum the 'car_count'
country_grouped = cl_union_cleaned.groupby('Country').size().reset_index(name='car_count')

# Sort by 'car_count' in descending order
country_grouped = country_grouped.sort_values(by='car_count', ascending=False)

# Select the top 10 countries
top_10_countries = country_grouped.head(10)

# Display the top 10 countries
print(top_10_countries)

"""#Step 13: Matrix Correlation and Heatmap to determine important variables

### **Analysing target variable**
"""

# Correct column names based on the provided variables
columns = [
    'brand', 'Model_file', 'Model_UTAC', 'Commercial_name', 'Code_National_Identification_Type',
    'Type_Variante_Version(TVV)', 'fuel_type', 'Hybride', 'Administrative_power', 'power_maximal (kW)',
    'Gearbox', 'Urban_consumption (l/100km)', 'Extra_urban_consumption(l/100km)', 'Consumption_mix(l/100km)',
    'CO2', 'CO_type_I (g/km)', 'hc', 'nox', 'hcnox',
    'Particles', 'Empty_mass_min(kg)', 'Empty_mass_max(kg)', 'Champ_V9', 'Missing_data',
    'Carrosserie', 'range'
]

# Ensure the columns exist in the DataFrame
columns = [col for col in columns if col in cl_union_cleaned.columns]

# Select relevant variables and drop rows with missing CO2 values
filtered_df = cl_union_cleaned[columns].dropna(subset=['CO2'])

# Ensure no duplicate columns exist in filtered_df
assert filtered_df.columns.duplicated().sum() == 0, "There are still duplicate columns in filtered_df"

# Step 1: Identify the car that emits the most CO2 emissions
max_co2 = filtered_df['CO2'].max()
max_co2_car = filtered_df[filtered_df['CO2'] == max_co2]

print("Car with the highest CO2 emissions:")
print(max_co2_car[['brand', 'Model_file', 'Commercial_name', 'CO2']])

# Step 2: Analyze correlations between CO2 emissions and other numerical variables
numerical_columns = [
    'CO2', 'Administrative_power', 'power_maximal (kW)', 'Urban_consumption (l/100km)',
    'Extra_urban_consumption(l/100km)', 'Consumption_mix(l/100km)', 'CO_type_I (g/km)',
    'Hydrocarbon', 'nitrogen oxides', 'hydrocarbon and nitrogen oxides', 'Particles (g/km)',
    'Empty_mass_min(kg)', 'Empty_mass_max(kg)', 'range'
]

# Ensure the numerical columns exist in the DataFrame
numerical_columns = [col for col in numerical_columns if col in filtered_df.columns]
# Reset the index of the DataFrame to ensure unique labels
filtered_df = filtered_df.reset_index(drop=True)
# Step 3: Create a scatter plot matrix to visualize relationships between CO2 and other variables
sns.pairplot(filtered_df[numerical_columns])
plt.suptitle('Scatter Plot Matrix of CO2 and Related Variables', y=1.02)
plt.show()


# Print the correlation matrix to identify the variables that correlate the most with CO2 emissions
print("Correlation matrix with CO2 emissions:")

"""The scatter plot matrix shows:

Strong Positive Correlation between CO2 emissions and both Power Maximal (kW) and Administrative Power—higher power vehicles emit more CO2.
Moderate Positive Correlation between CO2 emissions and Empty Mass—heavier vehicles tend to have higher emissions.
CO2 Type aligns closely with CO2 emissions, indicating consistency or redundancy between these variables.
This matrix highlights power and weight as key drivers of CO2 emissions in vehicles.
"""

numerical_columns = [
    'CO2', 'Administrative_power', 'power_maximal (kW)', 'Urban_consumption (l/100km)',
    'Extra_urban_consumption(l/100km)', 'Consumption_mix(l/100km)', 'CO_type_I (g/km)',
    'Hydrocarbon', 'nitrogen oxides', 'hydrocarbon and nitrogen oxides', 'Particles (g/km)',
    'Empty_mass_min(kg)', 'Empty_mass_max(kg)', 'range'
    ]

# Ensure the numerical columns exist in the DataFrame
numerical_columns = [col for col in numerical_columns if col in filtered_df.columns]
# Create a heatmap to visualize the correlation matrix
# Select only numeric columns for correlation calculation
numerical_df = filtered_df[numerical_columns].select_dtypes(include=['number'])
correlation_matrix = numerical_df.corr()

plt.figure(figsize=(14, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix with CO2 Emissions')
plt.show()

print(correlation_matrix['CO2'].sort_values(ascending=False))

"""The correlation matrix reveals:

Strong Positive Correlation: CO2 emissions are strongly linked to vehicle weight—Empty Mass Min (0.66) and Empty Mass Max (0.60).
Moderate Positive Correlation: Administrative Power (0.50) is moderately correlated with CO2 emissions, indicating that higher power vehicles tend to emit more CO2.
Weaker Correlation: Power Maximal (0.38) has a weaker correlation with CO2 emissions.
Negative Correlation: CO2 Type (-0.12) shows a slight negative correlation with CO2 emissions, suggesting some data inconsistencies or nuances.
Overall, vehicle weight is the most significant factor influencing CO2 emissions, followed by power-related variables.

# Step 14: Creation of CO2 categories


> CO2 class
It is based on the level of CO2 emissions approved according to European regulations. 7 classes specifying the CO2
emission levels of a new vehicle appear on the "energy / CO2" label , which must be displayed on each vehicle presented at the point of sale. Each class corresponds to a letter (from A to G) and a color (green / yellow / orange / red).


*   Class A: CO2 emissions less than or equal to 100 g/km
*   Class B: from 101 to 120 g / km
*   Class C: from 121 to 140 g/km
*   Class D: from 141 to 160 g / km
*   Class E: from 161 to 200 g / km
*   Class F: from 201 to 250 g / km
*   Class G: greater than 250 g/km




The increasing diversity of propulsion technologies, and the evolution of homologation protocols, have gradually led to a decorrelation between homologated CO2 emissions and the energy/fuel consumption of vehicles in real use: some vehicles, with low or zero CO2 emissions , can nevertheless be very energy-intensive.
"""

# Define a function to classify the CO2 emissions
def classify_co2_emissions(co2_value):
    if co2_value <= 100:
        return 'A'
    elif 101 <= co2_value <= 120:
        return 'B'
    elif 121 <= co2_value <= 140:
        return 'C'
    elif 141 <= co2_value <= 160:
        return 'D'
    elif 161 <= co2_value <= 200:
        return 'E'
    elif 201 <= co2_value <= 250:
        return 'F'
    else:  # Greater than 250 g/km
        return 'G'

# Apply the function to the 'CO2' column to create the 'CO2_class' column
cl_union_cleaned['CO2_class'] = cl_union_cleaned['CO2'].apply(classify_co2_emissions)

# Check the first few rows to verify
print(cl_union_cleaned[['CO2', 'CO2_class']].head())

# Verify if 'CO2_class' is in the DataFrame
print(cl_union_cleaned.columns)

# Check the first few rows to ensure the column is present and correctly assigned
print(cl_union_cleaned[['CO2', 'CO2_class']].head())

"""# Step 15: Research the relationship between CO2 target variable and other variables

## **Research relationship between CO2 emissions and Consumption mix**
"""

co2_col = 'CO2'  # Replace with the correct column name for CO2
# Let's assume the correct column name for consumption mix is something like 'Consumption_mix_l_per_100km'
consumption_col ='Consumption_mix(l/100km)'  # Replace with the correct column name for consumption mix

# Convert relevant columns to numerical types if they are not already
cl_union_cleaned[co2_col] = pd.to_numeric(cl_union_cleaned[co2_col], errors='coerce')
cl_union_cleaned[consumption_col] = pd.to_numeric(cl_union_cleaned[consumption_col], errors='coerce')

# Select relevant variables and drop rows with missing values in any of the important columns
filtered_df = cl_union_cleaned[['brand', 'Model_file', 'Commercial_name', co2_col, consumption_col]].dropna(subset=[co2_col, consumption_col])

# Combine brand and commercial name for better labels
filtered_df['Car'] = filtered_df['brand'] + " " + filtered_df['Commercial_name']

# Create a scatter plot
plt.figure(figsize=(12, 8))

# Scatter plot for CO2 vs. Consumption_mix
scatter = plt.scatter(
    filtered_df[co2_col],
    filtered_df[consumption_col],
    alpha=0.6,
    c=filtered_df[consumption_col],  # Color by Consumption_mix for additional dimension
    cmap='viridis'
)

plt.colorbar(scatter, label='Consumption Mix (l/100km)' )
plt.xlabel('CO2 Emissions (g/km)')
plt.ylabel('Consumption_mix (l/100km)' )
plt.title('Scatter Plot of CO2 Emissions and Consumption Mix')

# Highlight the car with the highest CO2 emissions
max_co2 = filtered_df[co2_col].max()
max_co2_car = filtered_df[filtered_df[co2_col] == max_co2]
plt.scatter(max_co2_car[co2_col], max_co2_car[consumption_col], color='red', edgecolor='black', label='Highest CO2 Emitter')

# Add legend
plt.legend(loc='upper right')

# Show the plot
plt.tight_layout()
plt.show()

"""The scatter plot shows a strong positive correlation between CO2 emissions and fuel consumption. As vehicles consume more fuel (l/100km), their CO2 emissions increase. The highest CO2 emitter, highlighted in red, also has the highest fuel consumption, confirming that less efficient vehicles are major contributors to higher emissions. Outliers with very high emissions and consumption indicate potential areas for efficiency improvements."""

import pandas as pd


print("Column names in the DataFrame:")
print(cl_union_cleaned.columns.tolist())

# Correct column names based on the previous print output
consumption_col = 'Consumption_mix(l/100km)'  # Replace with the correct column name for consumption mix

# Convert the consumption_mix (l/100km) column to numerical type if it is not already
cl_union_cleaned[consumption_col] = pd.to_numeric(cl_union_cleaned[consumption_col], errors='coerce')

# Filter numeric columns
numeric_columns = cl_union_cleaned.select_dtypes(include=['float64', 'int64']).columns

# Calculate the correlation matrix using only numeric columns
correlation_matrix = cl_union_cleaned[numeric_columns].corr()

# Display the correlation of all variables with consumption_mix (l/100km)
consumption_correlations = correlation_matrix[consumption_col].sort_values(ascending=False)
print("Correlation of variables with consumption_mix (l/100km):")
print(consumption_correlations)

# Sort the DataFrame by consumption_mix (l/100km) in descending order and select the top 20 rows
top_20_consumption_cars = cl_union_cleaned.sort_values(by=consumption_col, ascending=False).head(20)

# Display the top 20 cars with the highest consumption_mix (l/100km) and relevant features
relevant_features = ['brand', 'Model_file', 'Administrative_power', 'power_maximal (kW)', 'Empty_mass_min(kg)', 'Empty_mass_max(kg)', consumption_col]
print("Top 20 cars with the highest consumption_mix (l/100km) and relevant features:")
print(top_20_consumption_cars[relevant_features])

# Step 1: Define a function to classify the CO2 emissions
def classify_co2_emissions(co2_value):
    if co2_value <= 100:
        return 'A'
    elif 101 <= co2_value <= 120:
        return 'B'
    elif 121 <= co2_value <= 140:
        return 'C'
    elif 141 <= co2_value <= 160:
        return 'D'
    elif 161 <= co2_value <= 200:
        return 'E'
    elif 201 <= co2_value <= 250:
        return 'F'
    else:  # Greater than 250 g/km
        return 'G'

# Step 2: Apply the function to classify CO2 emissions and create the 'CO2_class' column
cl_union_cleaned['CO2_class'] = cl_union_cleaned['CO2'].apply(classify_co2_emissions)

# Step 3: Group data by CO2 emission classes
grouped_by_co2_class = cl_union_cleaned.groupby('CO2_class')

# Step 4: Loop through each CO2 class and display the most common brands, models, and fuel types
for co2_class, group in grouped_by_co2_class:
    print(f"\nCO2 Emission Class: {co2_class}")

    # Most common brands in this CO2 class
    common_brands = group['brand'].value_counts().head(5)  # Display top 5 brands
    print("\nMost common brands:")
    print(common_brands)

    # Most common models in this CO2 class
    common_models = group['Model_file'].value_counts().head(5)  # Display top 5 models
    print("\nMost common models:")
    print(common_models)

    # Most common fuel types in this CO2 class
    common_fuel_types = group['fuel_type'].value_counts().head(5)  # Display top 5 fuel types
    print("\nMost common fuel types:")
    print(common_fuel_types)

print(cl_union_cleaned.columns)

import matplotlib.pyplot as plt
import seaborn as sns

# Define the CO2 classes for consistency
co2_classes = ['A', 'B', 'C', 'D', 'E', 'F', 'G']

# Step 1: Initialize a figure with subplots (adjust the grid to accommodate all classes)
fig, axs = plt.subplots(4, 2, figsize=(15, 24))  # Increased rows to 4
axs = axs.flatten()  # Flatten the axes array for easy iteration

# Step 2: Loop through each CO2 class and create a bar plot for most common brands
for i, co2_class in enumerate(co2_classes):
    # Filter the data for the current CO2 class
    group = cl_union_cleaned[cl_union_cleaned['CO2_class'] == co2_class]

    if group.empty:
        continue  # Skip if there's no data for this class

    # Get the top 5 most common brands
    common_brands = group['brand'].value_counts().head(5)

    # Plot the data
    sns.barplot(x=common_brands.values,  y=common_brands.index, ax=axs[i], palette='viridis')
    axs[i].set_title(f'Most Common Brands (CO2 Class {co2_class})')
    axs[i].set_xlabel('Count')
    axs[i].set_ylabel('Brand')

# Step 3: Adjust layout and show the figure
plt.tight_layout()
plt.show()

"""## common characteristics CO2 emissions below 95"""

import pandas as pd

# Assuming cl_union is the DataFrame that already contains your combined data
# Print column names to verify
print("Column names in the DataFrame:")
print(cl_union_cleaned.columns.tolist())

# Correct column names based on the previous print output
co2_col = 'CO2'  # Replace with the correct column name for CO2
fuel_type_col = 'fuel_type'  # Replace with the correct column name for fuel_type

# Convert the CO2 column to numerical type if it is not already
cl_union_cleaned[co2_col] = pd.to_numeric(cl_union_cleaned[co2_col], errors='coerce')

# Filter the DataFrame to include only cars with CO2 emissions of 95 or lower
filtered_df = cl_union_cleaned[cl_union_cleaned[co2_col] <= 95]

# Display the filtered DataFrame
print("Cars with CO2 emissions of 95 or lower:")
print(filtered_df)

# Describe the common characteristics CO2 emissions below 95
common_characteristics = filtered_df.describe(include='all')

# Additionally, you can analyze specific categorical columns like brand, model, and fuel type
common_brands = filtered_df['brand'].value_counts()
common_models = filtered_df['Model_file'].value_counts()
common_fuel_types = filtered_df['fuel_type'].value_counts()

print("\nMost common brands:")
print(common_brands)

print("\nMost common models:")
print(common_models)

print("\nMost common fuel types:")
print(common_fuel_types)

"""#Step 16: Emission Trend Analysis"""

df_cl_1['Year'] = 2012
df_cl_2['Year'] = 2013
df_cl_3['Year'] = 2014

# Standardize the column names before merging
df_cl_1.rename(columns={'Modèle dossier': 'Model_file'}, inplace=True)
df_cl_2.rename(columns={'Modèle dossier': 'Model_file'}, inplace=True)
df_cl_3.rename(columns={'Modèle dossier': 'Model_file'}, inplace=True)

cl_union_cleaned.dtypes

print(cl_union_cleaned['fuel_type'])

# Concatenate the DataFrames
cl_union_cleaned = pd.concat([df_cl_1, df_cl_2, df_cl_3], axis=0, ignore_index=True)

# Reapply the fuel type categorization function
cl_union_cleaned['fuel_type'] = cl_union_cleaned['fuel_type'].apply(categorize_fuel_type)

# Handle missing values by filling them with the mode of the 'fuel_type' column
cl_union_cleaned['fuel_type'] = cl_union_cleaned['fuel_type'].fillna(cl_union_cleaned['fuel_type'].mode()[0])

# Check unique values in the 'fuel_type' column
print(cl_union_cleaned['fuel_type'].unique())

# Also reapply any other transformations, such as 'CO2_class'
cl_union_cleaned['CO2_class'] = cl_union_cleaned['CO2'].apply(classify_co2_emissions)

# Reapply the Group and Country mappings (if needed)
cl_union_cleaned[['Group', 'Country']] = cl_union_cleaned['brand'].apply(map_brand_to_group_country).apply(pd.Series)

# Check the final state of the DataFrame
print(cl_union_cleaned.info())

print(cl_union_cleaned.columns)

# Group by Year and Model to get average CO2 emissions per model each year
yearly_emissions_model = cl_union_cleaned.groupby(['Year', 'Model_file'])['CO2'].mean().reset_index()

# Group by Year and Carrosserie (vehicle type) to get average CO2 emissions per type each year
yearly_emissions_type = cl_union_cleaned.groupby(['Year', 'Carrosserie'])['CO2'].mean().reset_index()

print(yearly_emissions_model.head())

# Select top 5 vehicle types for simplicity
top_vehicle_types = yearly_emissions_type['Carrosserie'].value_counts().head(5).index.tolist()

plt.figure(figsize=(14, 8))

for vehicle_type in top_vehicle_types:
    type_data = yearly_emissions_type[yearly_emissions_type['Carrosserie'] == vehicle_type]
    plt.plot(type_data['Year'], type_data['CO2'], marker='o', label=vehicle_type)

plt.xlabel('Year')
plt.ylabel('Average CO2 Emissions (g/km)')
plt.title('CO2 Emission Trends Over Time by Vehicle Type')
plt.legend(title='Vehicle Type')
plt.grid(True)
plt.show()

"""Coupe and Cabriolet:

Coupe vehicles have the highest CO2 emissions, starting above 210 g/km in 2012, slightly decreasing in 2013, and then increasing again by 2014.
Cabriolet vehicles start just below 200 g/km and show a consistent decline throughout the period.

Break vehicles have relatively stable CO2 emissions around 160 g/km, with a slight decline over the period.
Berline vehicles show a steady decrease from around 150 g/km in 2012 to about 140 g/km in 2014, indicating a trend toward improved efficiency.

Combispace:
Combispace vehicles exhibit the lowest CO2 emissions, starting slightly above 140 g/km and declining steadily to below 140 g/km by 2014.


High Emission Vehicle Types: Coupes and Cabriolets have the highest CO2 emissions, likely due to their design focus on performance and power, which typically results in higher fuel consumption.

Efficiency Improvements: Berline and Combispace vehicles show a clear trend of decreasing CO2 emissions, suggesting ongoing improvements in fuel efficiency or a shift towards greener technologies within these vehicle types.

Stable Emissions: Break vehicles show a relatively stable trend, with only slight improvements in emissions over the years.
"""

import matplotlib.pyplot as plt

# Group by Year and Model to get average CO2 emissions per model each year
yearly_emissions_brand = cl_union_cleaned.groupby(['Year', 'brand'])['CO2'].mean().reset_index()


# Get the top 5 models based on the combined data
top_brands = cl_union_cleaned['brand'].value_counts().head(10).index.tolist()
print("Top brands selected:", top_brands)

# Filter the DataFrame for these top models
filtered_yearly_emissions = yearly_emissions_brand [yearly_emissions_brand ['brand'].isin(top_brands)]

# Plot the data
plt.figure(figsize=(14, 8))

for model in top_brands:
    model_data = filtered_yearly_emissions[filtered_yearly_emissions['brand'] == model]
    plt.plot(model_data['Year'], model_data['CO2'], marker='o', label=model)

plt.xlabel('Year')
plt.ylabel('Average CO2 Emissions (g/km)')
plt.title('CO2 Emission Trends Over Time by Brand')
plt.legend(title='Brand')
plt.grid(True)
plt.show()

"""BMW consistently has the highest CO2 emissions among the brands, starting above 170 g/km in 2012 and steadily decreasing over the years, ending just below 160 g/km in 2014.

Mercedes-Benz and Volkswagen also start with relatively high CO2 emissions, around 160 g/km, and show a gradual decrease over time.

Renault, Citroen, and Opel have the lowest CO2 emissions, with a significant decline observed from 2012 to 2014. By 2014, these brands achieve CO2 emissions well below 130 g/km.

Skoda and Fiat show slight decreases in CO2 emissions, maintaining a mid-range position relative to other brands.

Audi exhibits a slight increase in CO2 emissions, contrasting with the general downward trend seen in other brands.

Interpretation:
High Emission Brands: BMW, Mercedes-Benz, and Volkswagen remain at the higher end of CO2 emissions, although all show improvements over time.

Efficient Brands: Renault, Citroen, and Opel demonstrate significant improvements, reaching the lowest CO2 emissions by 2014, indicating a strong focus on efficiency.

Mixed Trends: Audi's slight increase in emissions may suggest a shift in model offerings or other factors affecting fuel efficiency during this period.
"""

# Filter the data for the winning model across all years
winning_model = 'SKODA'  # Replace with the actual model name

# Filter the data for the specific model
model_data = cl_union_cleaned [cl_union_cleaned['brand'] == winning_model]

# Ensure that only numeric columns are aggregated
numeric_columns = model_data.select_dtypes(include=['number']).columns

# Group by 'Year' and calculate the mean for numeric columns only
spec_changes = model_data.groupby('Year')[numeric_columns].mean()

print("Technical specifications over time for the winning model:")
print(spec_changes)

# Plot the changes in key specifications
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 8))
plt.plot(spec_changes.index, spec_changes['power_maximal (kW)'], marker='o', label='Maximal Power (kW)')
plt.plot(spec_changes.index, spec_changes['Empty_mass_max(kg)'], marker='o', label='Empty Mass Max (kg)')
plt.plot(spec_changes.index, spec_changes['Administrative_power'], marker='o', label='Administrative Power')

plt.xlabel('Year')
plt.ylabel('Values')
plt.title(f'Changes in Technical Specifications for {winning_model}')
plt.legend()
plt.grid(True)
plt.show()

"""Maximal Power (kW): The maximal power remains stable around 100-200 kW throughout the period, indicating no significant changes in engine performance for SKODA vehicles during these years.

Empty Mass Max (kg): This metric starts at around 1425 kg in 2012, shows a slight decrease in 2013, and continues to decline slightly into 2014, ending just below 1400 kg. This suggests a gradual reduction in vehicle weight.

Administrative Power: This remains close to zero throughout the period, indicating that there were no significant changes or fluctuations in this aspect of the vehicles.

Interpretation:
Stable Power Performance: The consistent maximal power suggests SKODA maintained similar engine power output across these years, likely reflecting stability in the types of engines used.

Weight Reduction: The slight but consistent decrease in Empty Mass Max could indicate efforts by SKODA to make their vehicles lighter, potentially to enhance fuel efficiency or meet evolving regulatory standards.

Consistency in Administrative Power: The minimal fluctuation in administrative power reflects that this technical specification was likely standardized across models during these years.
"""

print(cl_union_cleaned['fuel_type'])

print(cl_union_cleaned['fuel_type'].unique())
print(cl_union_cleaned['fuel_type'].isnull().sum())  # Check for missing values

# Filter the data to include only SKODA vehicles
skoda_data = cl_union_cleaned[cl_union_cleaned['brand'] == 'SKODA']

# Group by Year and calculate the mean for numeric variables
numeric_features = ['power_maximal (kW)', 'Administrative_power', 'Empty_mass_max(kg)', 'Empty_mass_min(kg)', 'CO2']
skoda_numeric_changes = skoda_data.groupby('Year')[numeric_features].mean()

print("Numeric variables over time for SKODA:")
print(skoda_numeric_changes)

# Plot the changes in numeric variables
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 8))

for feature in numeric_features:
    if feature != 'CO2':  # Exclude CO2 from this plot, we'll analyze it separately
        plt.plot(skoda_numeric_changes.index, skoda_numeric_changes[feature], marker='o', label=feature)

plt.xlabel('Year')
plt.ylabel('Average Values')
plt.title('Changes in Numeric Variables for SKODA (2012-2014)')
plt.legend(title='Numeric Variables')
plt.grid(True)
plt.show()

"""Power Maximal (kW) and Administrative Power: These values remain relatively stable over the three years. Power Maximal is consistent around 100-200 kW, while Administrative Power stays close to zero, indicating little change in engine power output over time.

Empty Mass Max (kg) and Empty Mass Min (kg): Both metrics show high values, with Empty Mass Max around 1400 kg and Empty Mass Min showing a slight decline from about 1425 kg in 2012 to just below 1400 kg in 2014. This indicates a small reduction in vehicle weight over time.

Stable Power Output: The consistent Power Maximal and Administrative Power suggest that SKODA maintained a steady level of engine performance across these years.

Slight Decrease in Vehicle Weight: The small decline in both Empty Mass Max and Min may indicate efforts to reduce vehicle weight, potentially to improve fuel efficiency or meet regulatory standards.
"""

# Identify the relevant categorical columns
categorical_features = ['fuel_type', 'Carrosserie', 'range', 'Gearbox']  # Replace with your actual categorical columns

# Group by Year and calculate the mode (most common value) for each categorical feature
# Handle cases where mode returns only one value
skoda_categorical_changes = skoda_data.groupby('Year')[categorical_features].agg(lambda x: x.mode()[0] if not x.mode().empty else 'Unknown') # Replace None with 'Unknown'

print("Categorical variables over time for SKODA:")
print(skoda_categorical_changes)

# Plot the changes for each categorical variable
plt.figure(figsize=(14, 6))

for feature in categorical_features:
    plt.plot(skoda_categorical_changes.index, skoda_categorical_changes[feature], marker='o', label=feature)

plt.xlabel('Year')
plt.ylabel('Category')
plt.title('Changes in Categorical Variables for SKODA (2012-2014)')
plt.legend(title='Categorical Variables')
plt.grid(True)
plt.show()

"""Gearbox (M 5 vs. M 6): The chart shows a transition from M 5 (5-speed manual gearbox) to M 6 (6-speed manual gearbox). Over time, M 6 remains constant, while M 5 shows some fluctuations, indicating a shift in preference or standardization towards the 6-speed gearbox.

Range (MOY-INFER vs. MOY-SUPER): The MOY-INFER category shows an increase over time, while MOY-SUPER remains constant. This suggests a shift in the range classification for the SKODA vehicles, possibly towards more mid-range offerings.

Carrosserie (BERLINE): The BERLINE category remains constant throughout the period, indicating that this body type is a stable offering for SKODA without significant changes.

### ***moy-infer is a low-mid range vehicle***



# Moyenne Inférieure (moy-infer)
Segment: This typically refers to lower-middle or compact cars. These vehicles are often part of the "C-segment" in European car classification, which includes small family cars or compact cars.
Characteristics:
Size: Generally smaller in size compared to "moy-super gamme" cars, with more compact dimensions.
Price: Usually more affordable, making them accessible to a broader range of consumers.
Features: Basic to mid-range features, focusing on practicality and efficiency rather than luxury.

# Moyenne Supérieure (moy-super gamme): Larger, more expensive, and feature-rich cars, often mid-size sedans or premium vehicles.

Hence, Skoda changed their size and features to make the care more practical and efficient resulting in lower CO2 emissions

#Step 17: Data Cleansing Preparation
"""

print(cl_union_cleaned['fuel_type'])

# Step 2: Initial inspection

print(cl_union_cleaned.info())  # Check data types and non-null counts
print(cl_union_cleaned.describe()) # Summary statistics
print(cl_union_cleaned.isnull().sum())  # Check for missing values

# Step 3: Backup the original dataset
cl_union_backup = cl_union_cleaned.copy()

# Step 4: Check for duplicates
print(cl_union_cleaned.duplicated().sum())

"""

>




---



 Step 5: Plan and apply data cleaning strategies
handle missing values, convert data types, drop duplicates
cl_union.drop_duplicates(inplace=True)
cl_union['date_column'] = pd.to_datetime(cl_union['date_column'])

Step 6: Segment or filter the data if needed
cl_union_filtered = cl_union[cl_union['year'] >= 2014]

Step 7: Data exploration and further cleaning
sns.histplot(cl_union['CO2'])
sns.boxplot(x=cl_union['brand'], y=cl_union['CO2'])



---


>

"""

cl_union_cleaned.info()

"""# Step 18: Deletion of Column Hybride"""

cl_union_cleaned = cl_union_cleaned.drop(columns=['Hybride'])

# Verify that the column has been removed
print("Columns after dropping 'Hybride':")
print(cl_union_cleaned.columns.tolist())

"""# Step 19: Deletion of fuel types with electric or hybrid engine"""

print(cl_union_cleaned['fuel_type'])

# Correct column names based on the previous print output
fuel_type_col = 'fuel_type'  # Replace with the correct column name for fuel_type

# Verify unique values in the fuel_type column before removal
print("Unique values in the fuel_type column before removal:")
print(cl_union_cleaned[fuel_type_col].unique())

# Remove rows where fuel_type is 'EL','EE', 'EH', 'GL', 'GH'
cl_union_cleaned =cl_union_cleaned [~cl_union_cleaned[fuel_type_col].isin(['Electric', 'Hybrid_Petrol/Electric_plug_in',  'Hybrid_Petrol/Electric_non_plug_in','Hybrid_Diesel/Electric_non_plug_in', 'Hybrid_Diesel/Electric_plug_in'])]

# Verify that the rows have been removed
print("Unique values in the fuel_type column after removal:")
print(cl_union_cleaned[fuel_type_col].unique())

import matplotlib.pyplot as plt

# Get the top 5 models based on the combined data
top_models = cl_union_cleaned['Model_file'].value_counts().head(5).index.tolist()
print("Top models selected:", top_models)

# Filter the DataFrame for these top models
filtered_yearly_emissions = yearly_emissions_model[yearly_emissions_model['Model_file'].isin(top_models)]

# Plot the data
plt.figure(figsize=(14, 8))

for model in top_models:
    model_data = filtered_yearly_emissions[filtered_yearly_emissions['Model_file'] == model]
    plt.plot(model_data['Year'], model_data['CO2'], marker='o', label=model)

plt.xlabel('Year')
plt.ylabel('Average CO2 Emissions (g/km)')
plt.title('CO2 Emission Trends Over Time by Vehicle Model')
plt.legend(title='Vehicle Model')
plt.grid(True)
plt.show()

"""Sprinter consistently has the highest CO2 emissions, remaining stable at over 220 g/km throughout the period from 2012 to 2014.

Viano and Vito also maintain stable CO2 emissions around 210 g/km, showing no significant changes during these years.

Crafter shows a decreasing trend from 210 g/km in 2012 to below 200 g/km in 2013, but then increases again slightly in 2014, indicating some fluctuations in emissions.

Classe E has the lowest CO2 emissions among the models, starting at around 160 g/km in 2012 and steadily decreasing to below 150 g/km by 2014.

#Step 20: Data visualization and deletion of the minibus category
## we drop the minibus as it does not fit our analysis of conventional cars
"""

# Update these variables according to your actual column names
carrosserie_col = 'Carrosserie'  # Replace with the correct column name for Carrosserie

# Remove rows where Carrosserie is 'MINIBUS'
cl_union_cleaned = cl_union_cleaned[cl_union_cleaned[carrosserie_col] != 'MINIBUS']

_
# Verify that the rows have been removed by printing unique values in the Carrosserie column
print("Unique values in the Carrosserie column after removing 'MINIBUS':")
print(cl_union_cleaned[carrosserie_col].unique())

print(cl_union_cleaned.columns)

"""##Numbers of cars over time"""

# Count the number of cars produced each year
production_counts = cl_union_cleaned.groupby('Year').size()

print("Number of cars produced each year:")
print(production_counts)

# Plot the production counts
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(production_counts.index, production_counts.values, marker='o', linestyle='-', color='blue')
plt.xlabel('Year')
plt.ylabel('Number of Cars Produced')
plt.title('Number of Cars Produced Each Year (2012-2014)')
plt.grid(True)
plt.show()

"""Car production peaked in 2013 at around 11,000 units, followed by a sharp decline in 2014 back to 2012 levels of approximately 7,500 units. This suggests a temporary surge in production in 2013 that was not maintained.
Production Spike in 2013: The peak in 2013 suggests a potential increase in demand, expansion of manufacturing capacity, or the introduction of new models that year.
Decline in 2014: The return to 2012 production levels in 2014 could indicate market saturation, economic factors, or strategic shifts in production focus.
"""

import pandas as pd
import matplotlib.pyplot as plt

# Group the data by 'Year' and calculate the average CO2 emissions for each year
average_co2_per_year = cl_union_cleaned.groupby('Year')['CO2'].mean()

# Display the results
print("Average CO2 emissions for each year:")
print(average_co2_per_year)

# Visualize the average CO2 emissions over the years
plt.figure(figsize=(10, 6))
plt.plot(average_co2_per_year.index, average_co2_per_year.values, marker='o', linestyle='-', color='blue')
plt.xlabel('Year')
plt.ylabel('Average CO2 Emissions (g/km)')
plt.title('Average CO2 Emissions Per Year (2012-2014)')
plt.grid(True)
plt.show()

"""
The graph shows a steady decline in average CO2 emissions from vehicles between 2012 and 2014, dropping from 152 g/km to 145 g/km. This indicates a continuous improvement in vehicle efficiency or environmental regulations during this period, reflecting positive progress towards reducing emissions."""

# Count the number of each car model produced across all years
model_production_counts = cl_union_cleaned['Model_file'].value_counts()

print("Top 10 most produced car models:")
print(model_production_counts.head(10))

# Get the top 10 most produced models
top_10_models = model_production_counts.head(10).index

# Filter the dataset to include only the top 10 most produced models
top_models_data = cl_union_cleaned[cl_union_cleaned['Model_file'].isin(top_10_models)]

# Calculate the average CO2 emissions for these top models
top_models_co2 = top_models_data.groupby('Model_file')['CO2'].mean().sort_values()

print("Average CO2 emissions of the top 10 most produced car models:")
print(top_models_co2)

# Plot the CO2 emissions of the top 10 most produced models
plt.figure(figsize=(10, 6))
top_models_co2.plot(kind='bar', color='green')
plt.xlabel('Car Model')
plt.ylabel('Average CO2 Emissions (g/km)')
plt.title('Average CO2 Emissions of the Top 10 Most Produced Car Models (2012-2014)')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

"""Classe M has the highest average CO2 emissions, exceeding 175 g/km.
Classe B and Classe A have the lowest average CO2 emissions, around 125 g/km.
Other models like Classe C, Classe E, Serie 3, Caddy, Classe GLK, Serie 5, and Classe S have average CO2 emissions ranging between 130 g/km and 160 g/km.

Classe M stands out as the model with the highest CO2 emissions, suggesting it may be a larger, less efficient vehicle compared to others.
Classe B and Classe A are the most efficient among the top 10, likely indicating smaller or more fuel-efficient designs.

# Step 21: Data visualization and analysis – drop classe M  AS IT EMITS TOO MUCH CO2
"""

# Drop rows where the car model is 'Classe S'
cl_union_cleaned = cl_union_cleaned[cl_union_cleaned['Model_file'] != 'Classe S']

# Verify that 'Classe S' has been dropped
print(cl_union_cleaned['Model_file'].unique())

print(cl_union_cleaned.columns)

"""# Step 22: Determining outliers and deletion

# **which cars are outliers based on carosserie and maximal engine power**
"""

# Import necessary libraries
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Print column names to verify
print("Column names in the DataFrame:")
print(cl_union_cleaned.columns.tolist())

# Correct column names based on the previous print output
carrosserie_col = 'Carrosserie'  # Replace with the correct column name for Carrosserie
power_col = 'power_maximal (kW)'  # Replace with the correct column name for power_maximal

# Convert relevant columns to numerical types if they are not already
cl_union_cleaned[power_col] = pd.to_numeric(cl_union_cleaned[power_col], errors='coerce')

# Select relevant variables and drop rows with missing values in any of the important columns
filtered_df = cl_union_cleaned[[carrosserie_col, power_col]].dropna(subset=[power_col])

# Create a boxplot for power_maximal (kW) grouped by Carrosserie
plt.figure(figsize=(12, 8))
sns.boxplot(x=power_col, y=carrosserie_col, data=filtered_df, palette='viridis')

# Set labels and title
plt.xlabel('Power Maximal (kW)')
plt.ylabel('Carrosserie')
plt.title('Boxplot of Power Maximal (kW) by Carrosserie')

# Show the plot
plt.tight_layout()
plt.show()

"""Berline, Coupe, and TS Terrains/Chemins categories still have the highest power outputs with many outliers, particularly in the higher power ranges (above 200 kW).
Other categories like Break, Cabriolet, Monospace, and Combispace have relatively lower power outputs, with fewer outliers.
The removal of "Classe M" appears to have reduced the overall range of power outputs, particularly in categories where "Classe M" might have contributed significantly to higher power outliers.

Impact of Removing Classe M: The removal of "Classe M" models has likely reduced some of the extreme outliers in terms of power output, indicating that "Classe M" was a significant contributor to high-power vehicles.
Remaining Trends: The general trend of high power concentration in Berline, Coupe, and TS Terrains/Chemins remains consistent, suggesting these categories are still associated with higher-performance vehicles.

Removing outliers before splitting ensures that both the training and test sets are drawn from the same underlying data distribution without extreme values. This approach can lead to a more homogeneous and stable model training process and are more representative of the general data population.
"""

# Define the thresholds for outlier removal
high_power_threshold = 300  # Outliers for high-powered categories
low_power_threshold = 150   # Outliers for lower-powered categories

# Define the categories considered high-powered and low-powered
high_power_categories = ['BERLINE', 'COUPE', 'CABRIOLET', 'BREAK', 'TS TERRAINS/CHEMINS']
low_power_categories = ['MONOSPACE', 'COMBISPACE', 'MINIBUS']

# Create a filter to remove outliers for high-powered categories
high_power_filter = (cl_union_cleaned['Carrosserie'].isin(high_power_categories)) & (cl_union_cleaned['power_maximal (kW)'] <= high_power_threshold)

# Create a filter to remove outliers for low-powered categories
low_power_filter = (cl_union_cleaned['Carrosserie'].isin(low_power_categories)) & (cl_union_cleaned['power_maximal (kW)'] <= low_power_threshold)

# Keep the non-outliers for both categories
non_outliers = cl_union_cleaned[high_power_filter | low_power_filter]

# Verify the number of rows before and after removing outliers
print(f"Original number of rows: {cl_union_cleaned.shape[0]}")
print(f"Number of rows after removing outliers: {non_outliers.shape[0]}")

# Optionally, save the cleaned DataFrame
# non_outliers.to_csv('cleaned_data.csv', index=False)

print(cl_union_cleaned.columns)

"""# Step 23: Deletion of variable Last_update as it is missing too much data

Removing the column last update as it is irrelevant and misses a big chunk of data
"""

cl_union_cleaned = cl_union_cleaned.drop(columns=['Last_update'])

print(cl_union_cleaned.columns)

# Identify unique categories for each categorical column before splitting
Carrosserie_categories = cl_union_cleaned['Carrosserie'].unique().tolist()
fuel_type_categories = cl_union_cleaned['fuel_type'].unique().tolist()
Gearbox_categories = cl_union_cleaned['Gearbox'].unique().tolist()

print("Carrosserie Categories:", Carrosserie_categories)
print("Fuel Type Categories:", fuel_type_categories)
print("Gearbox Categories:", Gearbox_categories)

print(cl_union_cleaned.columns)

cl_union_cleaned.to_csv('/content/drive/My Drive/cl_union_cleaned.csv', index=False)
cl_union_cleaned_verify = pd.read_csv('/content/drive/My Drive/cl_union_cleaned.csv')
print(cl_union_cleaned_verify)

"""# Step 24: Data Separation – import sklearn library"""

from sklearn.model_selection import train_test_split

"""# Step 25: Splitting Cl Dataset into train (80) and test (20) data"""

train_set, test_set = train_test_split(cl_union_cleaned, test_size=0.2, random_state=42)

# Verify the split
print(f"Training set size: {train_set.shape[0]} rows")
print(f"Test set size: {test_set.shape[0]} rows")

from sklearn.preprocessing import OneHotEncoder

# Use the identified unique categories for OneHotEncoder
encoder = OneHotEncoder(categories=[Carrosserie_categories, fuel_type_categories, Gearbox_categories], drop=None) # Remove sparse argument

# Fit the encoder on the training set and transform both train and test sets
encoded_train = encoder.fit_transform(train_set[['Carrosserie', 'fuel_type', 'Gearbox']])
encoded_test = encoder.transform(test_set[['Carrosserie', 'fuel_type', 'Gearbox']])

# Convert the encoded data back into DataFrames
encoded_train_df = pd.DataFrame(encoded_train.toarray(), columns=encoder.get_feature_names_out(['Carrosserie', 'fuel_type', 'Gearbox'])) # Call toarray() to convert to dense array
encoded_test_df = pd.DataFrame(encoded_test.toarray(), columns=encoder.get_feature_names_out(['Carrosserie', 'fuel_type', 'Gearbox'])) # Call toarray() to convert to dense array

# Reset the indices and concatenate the encoded columns with the original DataFrame
train_set = pd.concat([train_set.reset_index(drop=False), encoded_train_df], axis=1)
test_set = pd.concat([test_set.reset_index(drop=False), encoded_test_df], axis=1)

# Drop the original categorical columns if no longer needed
train_set = train_set.drop(columns=['Carrosserie', 'fuel_type', 'Gearbox'])
test_set = test_set.drop(columns=['Carrosserie', 'fuel_type', 'Gearbox'])

# Verify the resulting columns
print(train_set.columns)
print(test_set.columns)

from sklearn.preprocessing import OneHotEncoder

# List of all categorical columns to encode
categorical_columns = ['range', 'brand', 'Group', 'Country', 'CO2_class']

# Identify unique categories for each column (if needed) or let OneHotEncoder handle it automatically
encoder = OneHotEncoder(drop=None)  # Automatically detects categories from the data

# Fit the encoder on the training set and transform both train and test sets
encoded_train = encoder.fit_transform(train_set[categorical_columns])
encoded_test = encoder.transform(test_set[categorical_columns])

# Convert the encoded data back into DataFrames
encoded_train_df = pd.DataFrame(encoded_train.toarray(), columns=encoder.get_feature_names_out(categorical_columns))
encoded_test_df = pd.DataFrame(encoded_test.toarray(), columns=encoder.get_feature_names_out(categorical_columns))

# Reset the indices and concatenate the encoded columns with the original DataFrame
train_set = pd.concat([train_set.reset_index(drop=True), encoded_train_df], axis=1)
test_set = pd.concat([test_set.reset_index(drop=True), encoded_test_df], axis=1)

# Drop the original categorical columns
train_set = train_set.drop(columns=categorical_columns)
test_set = test_set.drop(columns=categorical_columns)

# Verify the resulting columns
print(train_set.columns)
print(test_set.columns)

"""# Step 26: Cleaning the Data From Column hc, nox and hcnox"""

df = cl_union_cleaned

# Define the function to fill missing values

def fill_missing_values(df, col_hc, col_nox, col_hcnox):
    # Calculation of missing values in 'hcnox'
    mask_hcnox = pd.isna(df[col_hcnox]) & pd.notna(df[col_hc]) & pd.notna(df[col_nox])
    df.loc[mask_hcnox, col_hcnox] = df.loc[mask_hcnox, col_hc] + df.loc[mask_hcnox, col_nox]

    # Calculation of missing values in 'hc'
    mask_hc = pd.isna(df[col_hc]) & pd.notna(df[col_nox]) & pd.notna(df[col_hcnox])
    df.loc[mask_hc, col_hc] = df.loc[mask_hc, col_hcnox] - df.loc[mask_hc, col_nox]

    # Calculation of missing values in 'nox'
    mask_nox = pd.isna(df[col_nox]) & pd.notna(df[col_hc]) & pd.notna(df[col_hcnox])
    df.loc[mask_nox, col_nox] = df.loc[mask_nox, col_hcnox] - df.loc[mask_nox, col_hc]

# Split the original cl_union dataset into train and test sets
train_set, test_set = train_test_split(cl_union_cleaned, test_size=0.2, random_state=42)

# Apply the function to the training set
fill_missing_values(train_set, 'hc', 'nox', 'hcnox')

# Apply the function to the test set
fill_missing_values(test_set, 'hc', 'nox', 'hcnox')

# Verify the results
print(train_set[['hc', 'nox', 'hcnox']].head())
print(test_set[['hc', 'nox', 'hcnox']].head())

# Apply the function iteratively to fill as many missing values as possible
for _ in range(5):  # Adjust the range if needed
    fill_missing_values(train_set, 'hc', 'nox', 'hcnox')

# Check for remaining missing values
print("Remaining missing values in the training set:")
print(train_set[['hc', 'nox', 'hcnox']].isnull().sum())

# Fill remaining missing values with the mean of the respective columns

train_set.fillna({'hc':train_set['hc'].mean()}, inplace=True)
train_set.fillna({'nox':train_set['nox'].mean()}, inplace=True)
train_set.fillna({'hcnox':train_set['hcnox'].mean()}, inplace=True)

#df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.
#testset
test_set.fillna({'hc':test_set['hc'].mean()}, inplace=True)
test_set.fillna({'nox':test_set['nox'].mean()}, inplace=True)
test_set.fillna({'hcnox':test_set['hcnox'].mean()}, inplace=True)

# Check the remaining missing values
print(train_set[['hc', 'nox', 'hcnox']].isnull().sum())
print(test_set[['hc', 'nox', 'hcnox']].isnull().sum())

"""# Step 27: Cleaning data from column particle through mean"""

# Calculate the mean of the 'Particles' column from the training set
mean_particles_train = train_set['Particles'].mean()

# Display the mean of the 'Particles' column from the training set
print(f"Mean of the 'Particles' column (Training set): {mean_particles_train}")

# Calculate the mean of the 'Particles' column from the test set
mean_particles_test = test_set['Particles'].mean()

# Display the mean of the 'Particles' column from the test set
print(f"Mean of the 'Particles' column (Test set): {mean_particles_test}")

#  fill missing values in 'Particles' with the mean:
train_set.fillna({'Particles': mean_particles_train}, inplace=True)
test_set.fillna({'Particles': mean_particles_train}, inplace=True)

# Verify the changes
print(train_set['Particles'].head())
print(test_set['Particles'].head())

print(train_set['Particles'].isnull().sum())
print(test_set['Particles'].isnull().sum())

"""# Step 28: Replacing missing values for numerical data with mean and for categorical data with mode"""

# Fill missing values in categorical columns
categorical_columns = ['Carrosserie', 'fuel_type', 'Gearbox', 'brand', 'Commercial_name']


for col in categorical_columns:
    train_set.fillna({col:train_set[col].mode()[0]}, inplace=True)
    test_set.fillna({col:test_set[col].mode()[0]}, inplace=True)

# Impute missing values on the training set
numerical_columns = ['Urban_consumption (l/100km)', 'Extra_urban_consumption(l/100km)',
                     'Consumption_mix(l/100km)', 'CO2', 'CO_type_I (g/km)']

for col in numerical_columns:

    train_set[col] = pd.to_numeric(train_set[col], errors='coerce')
    train_set.fillna({col:train_set[col].mean()}, inplace=True)

# Apply the same strategy to the test set (be cautious of data leakage)
for col in numerical_columns:
    test_set[col] = pd.to_numeric(test_set[col], errors='coerce')
    test_set.fillna({col:train_set[col].mean()}, inplace=True) # Use training mean

# Impute categorical missing values
categorical_columns = ['fuel_type', 'Champ_V9']

for col in categorical_columns:

    train_set.fillna({col:train_set[col].mode()[0]}, inplace=True)
    test_set.fillna({col:train_set[col].mode()[0]}, inplace=True)

# Check for missing values in the training set
missing_values_train = train_set.isnull().sum()

# Check for missing values in the test set
missing_values_test = test_set.isnull().sum()

# Display columns with missing values in the training set
print("Missing values in the training set:")
print(missing_values_train[missing_values_train > 0])

# Display columns with missing values in the test set
print("Missing values in the test set:")
print(missing_values_test[missing_values_test > 0])

# Verify the resulting columns
print(train_set.columns)
print(test_set.columns)

"""# Step 29: Drop Columns deemed unnecessary after data is clean

## NOW THERE ARE NO MISSING VALUES. WE WILL HENCE DROP COLUMNS WE DO NOT NEED TO FURTHER EXPLORE OUR TARGET VARIABLE CO2

Variables we keep


NUMERICAL

1. 'Consumption_mix (l/100km)'
2. 'hcnox'  
3. 'CO2'
4. 'power_maximal (kW)'
5.  'Administrative_power'
6. 'Empty_mass_max(kg)'
7. 'Empty_mass_min(kg)'

CATEGORICAL

1. 'Carosserie'
2.  'range'
3. 'gearbox'
4. 'brand'                       
5. 'Commercial_name'
6. 'Group'
7. 'Country'
8. 'CO2_class
"""

#  List of columns to keep
columns_to_keep = [
    'fuel_type',
    'Carrosserie',
    'range',
    'Gearbox',
    'power_maximal (kW)',
    'Administrative_power',
    'Empty_mass_max(kg)',
    'Empty_mass_min(kg)',
    'CO2',
    'hcnox',
    'hc',
    'nox',
    'brand',
    'Consumption_mix(l/100km)',  # Ensure this matches the exact name in your DataFrame
    'Commercial_name', # Ensure this matches the exact name in your DataFrame
    'Group',
    'Country',
    'CO2_class'
]

# Step 4: Cleanse the train and test sets by selecting only the desired columns
train_set_cleaned = train_set[columns_to_keep]
test_set_cleaned = test_set[columns_to_keep]

# Step 5: Display the first few rows of the cleansed train and test sets
print("Train Set:")
print(train_set_cleaned.head())

print("\nTest Set:")
print(test_set_cleaned.head())

print(train_set_cleaned.columns)
print(test_set_cleaned.columns)

"""# Step 30: Data visualization of CO2 distribution"""

import matplotlib.pyplot as plt

# Plot distribution of CO2 emissions in the train set
plt.figure(figsize=(10, 6))
plt.hist(train_set_cleaned['CO2'], bins=50, color='blue', alpha=0.7, label='Train Set')
plt.hist(test_set_cleaned['CO2'], bins=50, color='green', alpha=0.7, label='Test Set')
plt.xlabel('CO2 Emissions (g/km)')
plt.ylabel('Frequency')
plt.title('Distribution of CO2 Emissions in Train and Test Sets')
plt.legend()
plt.show()

import matplotlib.pyplot as plt

# Step 1: Sort by CO2 emissions
sorted_train_df = train_set_cleaned[['brand', 'Commercial_name', 'CO2']].sort_values(by='CO2', ascending=False)

# Step 2: Drop duplicate car models based on 'Commercial_name' to ensure uniqueness
unique_top_train_cars = sorted_train_df.drop_duplicates(subset=['Commercial_name'])

# Step 3: Select the top 20 or as many as available
top_train_cars = unique_top_train_cars.head(20).copy()  # Explicitly make a copy to avoid SettingWithCopyWarning

# Step 4: Create a label combining brand and commercial name using .loc
top_train_cars.loc[:, 'Car'] = top_train_cars['brand'] + " " + top_train_cars['Commercial_name']

# Step 5: Plot the top 20 cars with the highest CO2 emissions
plt.figure(figsize=(12, 8))
plt.barh(top_train_cars['Car'], top_train_cars['CO2'], color='skyblue')
plt.xlabel('CO2 Emissions (g/km)')
plt.ylabel('Car Model')
plt.title('Top 20 Cars with Highest CO2 Emissions - Training Set')
plt.gca().invert_yaxis()

# Highlight the car with the highest CO2 emissions
max_co2_train = top_train_cars['CO2'].max()
max_co2_car_train = top_train_cars[top_train_cars['CO2'] == max_co2_train]['Car'].values[0]
plt.barh(max_co2_car_train, max_co2_train, color='red')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Step 1: Sort by CO2 emissions
sorted_test_df = test_set_cleaned[['brand', 'Commercial_name', 'CO2']].sort_values(by='CO2', ascending=False)

# Step 2: Drop duplicate car models based on 'Commercial_name' to ensure uniqueness
unique_top_test_cars = sorted_test_df.drop_duplicates(subset=['Commercial_name'])

# Step 3: Select the top 20 or as many as available
top_test_cars = unique_top_test_cars.head(20).copy()  # Explicitly make a copy to avoid SettingWithCopyWarning

# Step 4: Create a label combining brand and commercial name using .loc
top_test_cars.loc[:, 'Car'] = top_test_cars['brand'] + " " + top_test_cars['Commercial_name']

# Step 5: Plot the top 20 cars with the highest CO2 emissions
plt.figure(figsize=(12, 8))
plt.barh(top_test_cars['Car'], top_test_cars['CO2'], color='skyblue')
plt.xlabel('CO2 Emissions (g/km)')
plt.ylabel('Car Model')
plt.title('Top 20 Cars with Highest CO2 Emissions - Test Set')
plt.gca().invert_yaxis()

# Highlight the car with the highest CO2 emissions
max_co2_test = top_test_cars['CO2'].max()
max_co2_car_test = top_test_cars[top_test_cars['CO2'] == max_co2_test]['Car'].values[0]
plt.barh(max_co2_car_test, max_co2_test, color='red')

plt.tight_layout()
plt.show()

# Manually add missing columns if they are dropped during splitting
train_set_cleaned, test_set_cleaned = train_set_cleaned.align(test_set_cleaned, join='left', axis=1, fill_value=0)

# Check if fuel_type columns were added back properly
fuel_type_columns_train = [col for col in train_set_cleaned.columns if 'fuel_type' in col]
fuel_type_columns_test = [col for col in test_set_cleaned.columns if 'fuel_type' in col]

print("Fuel type columns in train set:", fuel_type_columns_train)
print("Fuel type columns in test set:", fuel_type_columns_test)

"""# Step 31 Standardizing numeric and Encoding categorical variables"""

# Redefine categorical_columns correctly
categorical_columns = ['Carrosserie', 'range', 'Gearbox', 'brand', 'Commercial_name', 'fuel_type', 'Country', 'Group', 'CO2_class']

print(train_set_cleaned['Country'])
print(test_set_cleaned['Country'])

"""# Step 32: Encoding and transforming data"""

# Step 1: Concatenate train and test sets for consistent one-hot encoding
combined_set = pd.concat([train_set_cleaned, test_set_cleaned], axis=0)

# Step 2: Apply one-hot encoding to the combined set using pd.get_dummies
categorical_columns = ['Carrosserie', 'range', 'Gearbox', 'brand', 'Commercial_name', 'fuel_type', 'Country', 'Group', 'CO2_class']
combined_set_encoded = pd.get_dummies(combined_set, columns=categorical_columns, drop_first=False)

# Step 3: Split the combined set back into train and test sets
train_set_cleaned_encoded = combined_set_encoded.iloc[:len(train_set_cleaned), :]
test_set_cleaned_encoded = combined_set_encoded.iloc[len(train_set_cleaned):, :]

# Step 4: Verify the results
print("Training set after consistent one-hot encoding:")
print(train_set_cleaned_encoded.head())

print("\nTest set after consistent one-hot encoding:")
print(test_set_cleaned_encoded.head())

# Check for the presence of one-hot encoded fuel_type columns after encoding
fuel_type_columns_train = [col for col in train_set_cleaned.columns if 'fuel_type' in col]
fuel_type_columns_test = [col for col in test_set_cleaned.columns if 'fuel_type' in col]

print("Fuel type columns in train set after encoding:", fuel_type_columns_train)
print("Fuel type columns in test set after encoding:", fuel_type_columns_test)

print(train_set_cleaned.columns)
print(test_set_cleaned.columns)

# Convert boolean columns (True/False) to 1/0 in bulk
bool_cols_train = train_set_cleaned.select_dtypes(include=['bool']).astype(int)
bool_cols_test = test_set_cleaned.select_dtypes(include=['bool']).astype(int)

# Drop the old boolean columns from the original DataFrame
train_set_cleaned = train_set_cleaned.drop(columns=bool_cols_train.columns)
test_set_cleaned = test_set_cleaned.drop(columns=bool_cols_test.columns)

# Use pd.concat to re-attach the converted boolean columns to the DataFrame
train_set_cleaned = pd.concat([train_set_cleaned, bool_cols_train], axis=1)
test_set_cleaned = pd.concat([test_set_cleaned, bool_cols_test], axis=1)

# Verify that the columns are now 0/1 instead of True/False
print(train_set_cleaned.head())
print(test_set_cleaned.head())

# Verify that the fuel_type columns contain data (0s and 1s)
print(train_set_cleaned[fuel_type_columns_train].sum())  # Sum will tell if 0/1 values are present
print(test_set_cleaned[fuel_type_columns_test].sum())

"""** encoding is working correctly. The issue is simply that some categories are underrepresented or absent in the test set.
Low category representation (like fuel_type_GL and others) is normal after splitting and may or may not need addressing.
Optional resampling can be done to balance the categories, but it’s not strictly necessary unless we see poor model performance related to these categories.**
"""

# Step 1: List of numerical and categorical columns
numerical_columns = [
    'Consumption_mix(l/100km)',
    'hcnox',
    'CO2',
    'power_maximal (kW)',
    'Administrative_power',
    'Empty_mass_max(kg)',
    'Empty_mass_min(kg)']

from sklearn.preprocessing import StandardScaler


scaler = StandardScaler()


# Fit on training data
train_set_cleaned[numerical_columns] = scaler.fit_transform(train_set_cleaned[numerical_columns]) # Use numerical_columns instead of numeric_columns

# Transform test data
test_set_cleaned[numerical_columns] = scaler.transform(test_set_cleaned[numerical_columns]) # Use numerical_columns instead of numeric_columns

# Step 5: Verify the results
print("Training set after one-hot encoding:")
print(train_set_cleaned.head())

print("\nTest set after one-hot encoding:")
print(test_set_cleaned.head())

# Verify the mean and standard deviation after scaling
print(train_set_cleaned[numerical_columns].mean(axis=0))  # Should be close to 0
print(train_set_cleaned[numerical_columns].std(axis=0))   # Should be close to 1
print(test_set_cleaned[numerical_columns].mean(axis=0))  # Should be close to 0
print(test_set_cleaned[numerical_columns].std(axis=0))   # Should be close to 1

# Verify the resulting columns
print(train_set_cleaned['Country'])
print(test_set_cleaned['Country'])

"""# step 33 Label Encoder"""

# Separate the features and target (CO2_class)
X_train = train_set_cleaned.drop(columns=['CO2_class'])  # Drop the target column to get features
y_train = train_set_cleaned['CO2_class']  # Target (CO2 class) for training

X_test = test_set_cleaned.drop(columns=['CO2_class'])  # Drop the target column to get features
y_test = test_set_cleaned['CO2_class']  # Target (CO2 class) for testing

# Encode the target variable using LabelEncoder
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()

# Fit the encoder on the training set and transform both train and test sets
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Print the class mapping
print("Class mapping:", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))

from sklearn.preprocessing import LabelEncoder
import numpy as np

# Initialize the LabelEncoder
le = LabelEncoder()

# Step 1: Fit the LabelEncoder on the 'Model_file' column in the training set
train_set_cleaned['Model_file_encoded'] = le.fit_transform(train_set_cleaned['Model_file'])

# Step 2: Create a function to handle unseen labels in the test set
def label_encode_with_unseen(label):
    if label in le.classes_:
        return le.transform([label])[0]  # Transform if it's a known label
    else:
        return -1  # Assign -1 if it's an unseen label

# Apply the custom encoding function to the 'Model_file' column in the test set
test_set_cleaned['Model_file_encoded'] = test_set_cleaned['Model_file'].apply(label_encode_with_unseen)

# Step 3: Drop the original 'Model_file' column from both sets
train_set_cleaned = train_set_cleaned.drop(columns=['Model_file'])
test_set_cleaned = test_set_cleaned.drop(columns=['Model_file'])

# Now you can proceed with any further encoding or modeling
print("Training set after Label Encoding (Model_file):")
print(train_set_cleaned.head())

print("\nTest set after Label Encoding (Model_file):")
print(test_set_cleaned.head())

"""## MACHINE TRAINING"""

from sklearn.preprocessing import LabelEncoder

# List of categorical columns to label encode
categorical_columns = ['fuel_type', 'Carrosserie', 'range', 'Gearbox', 'brand']

# Step 1: Apply Label Encoding to the training set and handle unseen labels in the test set
for col in categorical_columns:
    le = LabelEncoder()

    # Fit the encoder on the training data
    train_set[col] = le.fit_transform(train_set[col])

    # Transform the test data, assigning -1 to unseen labels
    test_set[col] = test_set[col].apply(lambda x: le.transform([x])[0] if x in le.classes_ else -1)

# Now proceed with training as before
X_train = train_set.drop(columns=['CO2'])
y_train = train_set['CO2']
X_test = test_set.drop(columns=['CO2'])
y_test = test_set['CO2']

# Train a Random Forest model
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

# Predict on the test set
y_pred = rf_model.predict(X_test)

# Evaluate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy:.2f}")

from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import pandas as pd

# Assuming X_train and y_train are already defined
# Train a Random Forest model
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

# Get feature importance
feature_importance = rf_model.feature_importances_

# Create a DataFrame for visualization
feature_importance_df = pd.DataFrame({
    'feature': X_train.columns,
    'importance': feature_importance
})

# Sort by importance
feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)

# Display the most important features
print("Top 10 Important Features:")
print(feature_importance_df.head(10))

# Plot the feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['feature'].head(10), feature_importance_df['importance'].head(10), color='skyblue')
plt.xlabel('Importance')
plt.title('Top 10 Feature Importance')
plt.gca().invert_yaxis()  # To have the highest importance feature at the top
plt.show()

from sklearn.preprocessing import StandardScaler

# List of numerical columns that need scaling
numerical_columns = ['power_maximal (kW)', 'Administrative_power', 'Empty_mass_max(kg)',
                     'Empty_mass_min(kg)', 'CO2', 'hcnox', 'hc', 'nox', 'Consumption_mix(l/100km)']

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler on the training set and transform both training and test sets
X_train_scaled = X_train.copy()
X_test_scaled = X_test.copy()

X_train_scaled[numerical_columns] = scaler.fit_transform(X_train[numerical_columns])
X_test_scaled[numerical_columns] = scaler.transform(X_test[numerical_columns])

# Display the scaled training set
print("Training set after scaling:")
print(X_train_scaled.head())

# Now you can proceed with training your model using the scaled features
rf_model_scaled = RandomForestClassifier(random_state=42)
rf_model_scaled.fit(X_train_scaled, y_train)

# Predict on the scaled test set
y_pred_scaled = rf_model_scaled.predict(X_test_scaled)

# Evaluate accuracy
from sklearn.metrics import accuracy_score
accuracy_scaled = accuracy_score(y_test, y_pred_scaled)
print(f"Model Accuracy after Feature Scaling: {accuracy_scaled:.2f}")
