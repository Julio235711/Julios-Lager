# -*- coding: utf-8 -*-
"""DATA CLEANSING_CL_CLEANED

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ClexDCv9r3Kdk-aLPhSqKp3tfYcEwmLw

# Path for CL datasets

# Step 1: Uploading Datasets to Google Drive and import the libraries
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Step 2: Import Libraries"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

"""# Step 3 : Read the files making connection between google drive and google colab"""

# Path for CL Datasets

path_cl_2012 = '/content/drive/MyDrive/DataCO2Project/CLFiles/cl_2012.xlsx' # This is the path where the files are located
path_cl_2013 = '/content/drive/MyDrive/DataCO2Project/CLFiles/cl_2013.xlsx' # This is the path where the files are located
path_cl_2014 = '/content/drive/MyDrive/DataCO2Project/CLFiles/cl_2014.xlsx' # This is the path where the files are located

df_cl_2012 = pd.read_excel(path_cl_2012)
df_cl_2013 = pd.read_excel(path_cl_2013)
df_cl_2014 = pd.read_excel(path_cl_2014)

"""# Step 4: Making security copy files"""

df_cl_1 = df_cl_2012.copy(deep = True)
df_cl_2 = df_cl_2013.copy(deep = True)
df_cl_3 = df_cl_2014.copy(deep = True)

"""# Step 5: Data Inspection

"""

df_cl_1.info()

df_cl_2.info()

df_cl_3.info()

"""# Step 6 Normalizing names for every column name for every CL Dataset. First create the dictionaries for every table. There are three dictionaries because the 2013/14 Dataset has different column name for the field fuel type and the column names in the 2013 dataset are entirely different.

"""

Dictionary_2012 = {'lib_mrq':'brand', 'lib_mod_doss':'Model_file', 'lib_mod':'Model_UTAC',
                   'dscom':'Commercial_name', 'cnit':'Code_National_Identification_Type',
                   'tvv':'Type_Variante_Version(TVV)','typ_cbr':'fuel_type',
                   'hybride':'Hybride','puiss_admin_98':'Administrative_power','puiss_max':'power_maximal (kW)','typ_boite_nb_rapp':'Gearbox',
                   'conso_urb':'Urban_consumption (l/100km)',
                   'conso_exurb':'Extra_urban_consumption(l/100km)','conso_mixte':'Consumption_mix(l/100km)',
                   'co2':'CO2','co_typ_1':'CO_type_I (g/km)',
                   'hc':'hc','nox':'nox',
                   'hcnox':'hcnox','ptcl':'Particles', 'masse_ordma_min':'Empty_mass_min(kg)',
                   'masse_ordma_max':'Empty_mass_max(kg)',
                   'champ_v9':'Champ_V9','date_maj':'Last_update','Carosserie':'Carosserie',
                   'gamme':'range'}


Dictionary_2013 = {'Marque':'brand','Modèle dossier':'Model_file', 'Modèle UTAC':'Model_UTAC',
                   'Désignation commerciale':'Commercial_name', 'CNIT':'Code_National_Identification_Type',
                   'Type Variante Version (TVV)':'Type_Variante_Version(TVV)','Carburant':'fuel_type',
                   'Hybride':'Hybride','Puissance administrative':'Administrative_power','Puissance maximale (kW)':'power_maximal (kW)','Boîte de vitesse':'Gearbox',
                   'Consommation urbaine (l/100km)':'Urban_consumption (l/100km)',
                   'Consommation extra-urbaine (l/100km)':'Extra_urban_consumption(l/100km)','Consommation mixte (l/100km)':'Consumption_mix(l/100km)',
                   'CO2 (g/km)':'CO2','CO type I (g/km)':'CO_type_I (g/km)',
                   'hc':'hc','nox':'nox',
                   'hcnox':'hcnox','Particules (g/km)':'Particles', 'masse vide euro min (kg)':'Empty_mass_min(kg)',
                   'masse vide euro max (kg)':'Empty_mass_max(kg)',
                   'Champ V9':'Champ_V9','Date de mise à jour':'Last_update','Carosserie':'Carosserie',
                   'gamme':'range'}





Dictionary_2014 = {'lib_mrq':'brand','lib_mod_doss':'Model_file', 'lib_mod':'Model_UTAC',
                   'dscom':'Commercial_name', 'cnit':'Code_National_Identification_Type',
                   'tvv':'Type_Variante_Version(TVV)','cod_cbr':'fuel_type',
                   'hybride':'Hybride','puiss_admin_98':'Administrative_power','puiss_max':'power_maximal (kW)','typ_boite_nb_rapp':'Gearbox',
                   'conso_urb':'Urban_consumption (l/100km)',
                   'conso_exurb':'Extra_urban_consumption(l/100km)','conso_mixte':'Consumption_mix(l/100km)',
                   'co2':'CO2','co_typ_1':'CO_type_I (g/km)',
                   'hc':'hc','nox':'nox',
                   'hcnox':'hcnox','ptcl':'Particles', 'masse_ordma_min':'Empty_mass_min(kg)',
                   'masse_ordma_max':'Empty_mass_max(kg)',
                   'champ_v9':'Champ_V9','date_maj':'Last_update','Carosserie':'Carosserie',
                   'gamme':'range'}


df_cl_1 = df_cl_1.rename(Dictionary_2012, axis = 1)
df_cl_2 = df_cl_2.rename(Dictionary_2013, axis = 1)
df_cl_3 = df_cl_3.rename(Dictionary_2014, axis = 1)

"""# Step 6.1 Changing the datatype fitting the variable"""

df_cl_1['Urban_consumption (l/100km)'] = pd.to_numeric(df_cl_1['Urban_consumption (l/100km)'], errors='coerce')
df_cl_2['Urban_consumption (l/100km)'] = pd.to_numeric(df_cl_2['Urban_consumption (l/100km)'], errors='coerce')
df_cl_3['Urban_consumption (l/100km)'] = pd.to_numeric(df_cl_3['Urban_consumption (l/100km)'], errors='coerce')

df_cl_1['Extra_urban_consumption(l/100km)'] = pd.to_numeric(df_cl_1['Extra_urban_consumption(l/100km)'], errors='coerce')
df_cl_2['Extra_urban_consumption(l/100km)'] = pd.to_numeric(df_cl_2['Extra_urban_consumption(l/100km)'], errors='coerce')
df_cl_3['Extra_urban_consumption(l/100km)'] = pd.to_numeric(df_cl_3['Extra_urban_consumption(l/100km)'], errors='coerce')


df_cl_1['Consumption_mix(l/100km)'] = pd.to_numeric(df_cl_1['Consumption_mix(l/100km)'], errors='coerce')
df_cl_2['Consumption_mix(l/100km)'] = pd.to_numeric(df_cl_2['Consumption_mix(l/100km)'], errors='coerce')
df_cl_3['Consumption_mix(l/100km)'] = pd.to_numeric(df_cl_3['Consumption_mix(l/100km)'], errors='coerce')


df_cl_1['Last_update'] = pd.to_datetime(df_cl_1['Last_update'], errors='coerce')
df_cl_2['Last_update'] = pd.to_datetime(df_cl_2['Last_update'], errors='coerce')
df_cl_3['Last_update'] = pd.to_datetime(df_cl_3['Last_update'], errors='coerce')

"""# Step 7: Concatenate all the tables from CL and called it CL Union"""

cl_union_cleaned = pd.concat([df_cl_1,df_cl_2,df_cl_3], axis = 0) #Table that represents the total data from 2012 to 2014


cl_union_cleaned.info()

"""# Step 8: Check for missing values in the united dataset"""

cl_union_cleaned.isnull().sum().sort_values(ascending = False)*100/len(cl_union_cleaned)

cl_union_cleaned.head()

"""### **CHECKING MISSING VALUES**"""

display(cl_union_cleaned.isna().sum())

"""# Step 9: Check for duplicates"""

# Check all column names to identify any with '_x' or '_y'
print(cl_union_cleaned.columns)

# If 'fuel_type_x' is the correct one, rename it to 'fuel_type'
cl_union_cleaned.rename(columns={'fuel_type_x': 'fuel_type'}, inplace=True)

# Optionally, if 'fuel_type_y' exists and is redundant, you can drop it
cl_union_cleaned.drop(columns=['fuel_type_y'], inplace=True, errors='ignore')

# Verify the changes
print(cl_union_cleaned.columns)

cl_union_cleaned.isna().any(axis=1).count()

"""# Step 10: Summary Statistics: Use .describe() to get a summary of the data, including mean, median, min, max, and quartiles, which can help identify outliers and data inconsistencies.

"""

cl_union_cleaned.describe()

print("Column names and data types in the DataFrame:")
print(cl_union_cleaned.dtypes)

"""### **Standardizing THE DATA CL_UNION FROM COLUMN FUEL TYPEvand grouping after countries**

# Step 11: Substitute all blank values for the mode in fuel type and standardize in column fuel types
"""

import pandas as pd

def categorize_fuel_type(fuel_type):
    if pd.isna(fuel_type):
        return 'NaN'
    elif fuel_type == 'EL':
        return 'Electric'
    elif fuel_type == 'GP':
        return 'Liquified_Petroleum_Gas'
    elif fuel_type == 'FE':
        return 'Superethanol_E85'
    elif fuel_type == 'ES':
        return 'Petrol_95'
    elif fuel_type == 'GO':
        return 'Diesel'
    elif fuel_type == 'GP/ES':
        return 'Dual_fuel_petrol/Liquified_Petroleum_Gas'
    elif fuel_type == 'EE':
        return 'Hybrid_Petrol/Electric_plug_in'
    elif fuel_type == 'ES/GP':
        return 'Dual_fuel_petrol/Liquified_Petroleum_Gas'
    elif fuel_type == 'GN':
        return 'Single_fuel_Natural_Gas'
    elif fuel_type == 'GN/ES':
        return 'Dual_fuel_petrol/Natural_Gas'
    elif fuel_type == 'ES/GN':
        return 'Dual_fuel_petrol/Natural_Gas'
    elif fuel_type == 'EH':
        return 'Hybrid_Petrol/Electric_non_plug_in'
    elif fuel_type == 'GH':
        return 'Hybrid_Diesel/Electric_non_plug_in'
    elif fuel_type == 'GL':
        return 'Hybrid_Diesel/Electric_plug_in'
    else:
        return 'Diesel'

# Apply the categorization function to the 'fuel_type' column
cl_union_cleaned['fuel_type'] = cl_union_cleaned['fuel_type'].apply(categorize_fuel_type)

# Handle missing values by filling them with the mode of the 'fuel_mode' column
cl_union_cleaned['fuel_type'] = cl_union_cleaned['fuel_type'].fillna(cl_union_cleaned['fuel_type'].mode()[0])

# Check unique values in the 'fuel_mode' column
print(cl_union_cleaned['fuel_type'].unique())

"""## **Standardize the names in column fuel type **

```
# This is formatted as code
```


"""

cl_union_cleaned['fuel_type'].replace({'EL':'Electric', 'GP':'Liquified_Petroleum_Gas', 'FE':'Superethanol_E85', 'ES':'Petrol_95',
       'GO':'Diesel', 'GH':'Hybrid_Diesel/Electric_non_plug_in','EH':'Hybrid_Petrol/Electric_non_plug_in', 'GP/ES':'Dual_fuel_petrol/Liquified_Petroleum_Gas', 'EE':'Hybrid_Petrol/Electric_plug_in', 'ES/GP':'Dual_fuel_petrol/Liquified_Petroleum_Gas',
       'GN':'Single_fuel_Natural_Gas', 'GL': 'Hybrid_Diesel/Electric_plug_in','OTHER':'Other','GN/ES':'Dual_fuel_petrol/Natural_Gas','ES/GN':'Dual_fuel_petrol/Natural_Gas' },inplace = True) #To normalize the names of the types of Fuel


sns.heatmap(pd.crosstab(cl_union_cleaned['fuel_type'],cl_union_cleaned['fuel_type'],margins=True, margins_name= 'Totals',normalize = True, dropna = False),cmap="YlGnBu", annot=True, cbar=True);
plt.title('Fuel Type versus Fuel Mode for Autos -Normalize Data-');
plt.xlabel('fuel Mode');
plt.ylabel('Fuel Type');

"""Diesel Dominance:

Diesel fuel dominates the dataset, making up 87% of the total, as indicated by the dark blue color corresponding to Diesel in both the Fuel Type and Fuel Mode axes.
Petrol 95:

Petrol 95 is the second most common fuel type, representing about 11% of the data. This is also reflected in the corresponding Fuel Mode.
Minor Fuel Types:

Predominance of Traditional Fuels: The overwhelming presence of Diesel and Petrol 95 suggests that traditional fuel types still dominate the automotive sector within this dataset, with alternative fuels playing a minor role.

# STEP 13: CREATE A DICTIONARY FOR THE CAR BRANDS AND ITS COUNTRIES AND GROUPS
"""

# Mapping of car brands to their Group and Country
brand_group_country = {
    # UK
    'LOTUS': ('PROTON', 'UK'),
    'ASTON MARTIN': ('ASTON MARTIN', 'UK'),
    'JAGUAR': ('JLR', 'UK'),
    'JAGUAR LAND ROVER LIMITED': ('JLR', 'UK'),
    'LAND ROVER': ('JLR', 'UK'),
    # Sweden
    'VOLVO': ('GEELY', 'Sweden'),
    # France
    'DANGEL': ('DANGEL', 'France'),
    'CITROEN': ('PSA', 'France'),
    'PEUGEOT': ('PSA', 'France'),
    'DACIA': ('RENAULT', 'France'),
    'RENAULT': ('RENAULT', 'France'),
    'RENAULT-TECH': ('RENAULT', 'France'),
    'RENAULT TECH': ('RENAULT', 'France'),
    'MIA': ('MIA ELECTRIC', 'France'),
    # Russia
    'LADA': ('AVTOVAZ', 'Russia'),
    # Japan
    'SUBARU': ('SUBARU', 'Japan'),
    'HONDA': ('HONDA', 'Japan'),
    'MITSUBISHI': ('MITSUBISHI', 'Japan'),
    'SUZUKI': ('SUZUKI', 'Japan'),
    'MAZDA': ('MAZDA', 'Japan'),
    'INFINITI': ('NISSAN', 'Japan'),
    'NISSAN': ('NISSAN', 'Japan'),
    'DAIHATSU': ('TOYOTA', 'Japan'),
    'LEXUS': ('TOYOTA', 'Japan'),
    'TOYOTA': ('TOYOTA', 'Japan'),
    # USA
    'TESLA': ('TESLA', 'USA'),
    'FORD': ('FORD', 'USA'),
    'CADILLAC': ('GENERAL MOTORS', 'USA'),
    'CHEVROLET': ('GENERAL MOTORS', 'USA'),
    'OPEL': ('GENERAL MOTORS', 'USA'),
    'VAUXHALL': ('GENERAL MOTORS', 'USA'),
    # Germany
    'BMW': ('BMW', 'Germany'),
    'MINI': ('BMW', 'Germany'),
    'ROLLS ROYCE': ('BMW', 'Germany'),
    'ROLLS-ROYCE': ('BMW', 'Germany'),
    'MERCEDES': ('MERCEDES', 'Germany'),
    'SMART': ('MERCEDES', 'Germany'),
    'MERCEDES-BENZ': ('MERCEDES', 'Germany'),
    'MERCEDES-AMG': ('MERCEDES', 'Germany'),
    'MERCEDES AMG': ('MERCEDES', 'Germany'),
    'MAYBACH': ('MERCEDES', 'Germany'),
    'AUDI': ('VOLKSWAGEN', 'Germany'),
    'BENTLEY': ('VOLKSWAGEN', 'Germany'),
    'LAMBORGHINI': ('VOLKSWAGEN', 'Germany'),
    'PORSCHE': ('VOLKSWAGEN', 'Germany'),
    'QUATRO': ('VOLKSWAGEN', 'Germany'),
    'QUATTRO': ('VOLKSWAGEN', 'Germany'),
    'SEAT': ('VOLKSWAGEN', 'Germany'),
    'SKODA': ('VOLKSWAGEN', 'Germany'),
    'VOLKSWAGEN': ('VOLKSWAGEN', 'Germany'),
    # Korea
    'HYUNDAI': ('KOREA', 'Korea'),
    'KIA': ('KOREA', 'Korea'),
    'SSANGYONG': ('KOREA', 'Korea'),
    # Italy
    'ALFA ROMEO': ('FIAT', 'Italy'),
    'ALFA-ROMEO': ('FIAT', 'Italy'),
    'FERRARI': ('FIAT', 'Italy'),
    'FIAT': ('FIAT', 'Italy'),
    'JEEP': ('FIAT', 'Italy'),
    'LANCIA': ('FIAT', 'Italy'),
    'MASERATI': ('FIAT', 'Italy')
}

# Function to map 'brand' to 'Group' and 'Country' using the dictionary
def map_brand_to_group_country(brand):
    # Look up the brand in the dictionary, return ('Unknown', 'Unknown') if not found
    return brand_group_country.get(brand, ('Unknown', 'Unknown'))

# Apply the mapping to the 'brand' column and create two new columns: 'Group' and 'Country'
cl_union_cleaned[['Group', 'Country']] = cl_union_cleaned['brand'].apply(map_brand_to_group_country).apply(pd.Series)

# Verify the result
print(cl_union_cleaned[['brand', 'Group', 'Country']].head())

print(cl_union_cleaned.columns)

# Step 1: Extract unique brands from the 'brand' column in the dataset
unique_brands_in_dataset = cl_union_cleaned['brand'].unique()

# Step 2: Get the keys (brands) from the brand_group_country dictionary
brands_in_dict = set(brand_group_country.keys())

# Step 3: Find brands in the dataset that are missing in the dictionary
missing_brands = [brand for brand in unique_brands_in_dataset if brand not in brands_in_dict]

# Step 4: Display the missing brands
if missing_brands:
    print("Missing brands in the dictionary:", missing_brands)
else:
    print("No missing brands. All brands are accounted for in the dictionary.")

# Group by 'Group' and 'Country' and count the number of cars for each group
grouped_data = cl_union_cleaned.groupby(['brand','Group', 'Country']).size().reset_index(name='car_count')

# Sort the grouped data by 'car_count' in ascending order
grouped_data = grouped_data.sort_values(by='car_count', ascending=False)

# Display the sorted grouped data
print(grouped_data)

# Do NOT overwrite the original DataFrame `cl_union_cleaned`

# Group by 'Group' and 'Country' and count the number of cars for each group
grouped_data = cl_union_cleaned.groupby(['Group', 'Country']).size().reset_index(name='car_count')

# Sort the grouped data by 'car_count' in ascending order
grouped_data = grouped_data.sort_values(by='car_count', ascending=False)

# Display the sorted grouped data
print(grouped_data)

print(cl_union_cleaned.columns)

import matplotlib.pyplot as plt
import seaborn as sns

# Step 5: Visualize the grouped data
plt.figure(figsize=(12, 8))
sns.barplot(x='Group', y='car_count', hue='Country', data=grouped_data, palette='viridis')
plt.title('Number of Cars by Group and Country')
plt.xlabel('Group')
plt.ylabel('Number of Cars')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Group by 'Country' and sum the 'car_count'
country_grouped = cl_union_cleaned.groupby('Country').size().reset_index(name='car_count')

# Sort by 'car_count' in descending order
country_grouped = country_grouped.sort_values(by='car_count', ascending=False)

# Select the top 10 countries
top_10_countries = country_grouped.head(10)

# Display the top 10 countries
print(top_10_countries)

"""#Step 13: Matrix Correlation and Heatmap to determine important variables

### **Analysing target variable**
"""

# Correct column names based on the provided variables
columns = [
    'brand', 'Model_file', 'Model_UTAC', 'Commercial_name', 'Code_National_Identification_Type',
    'Type_Variante_Version(TVV)', 'fuel_type', 'Hybride', 'Administrative_power', 'power_maximal (kW)',
    'Gearbox', 'Urban_consumption (l/100km)', 'Extra_urban_consumption(l/100km)', 'Consumption_mix(l/100km)',
    'CO2', 'CO_type_I (g/km)', 'hc', 'nox', 'hcnox',
    'Particles', 'Empty_mass_min(kg)', 'Empty_mass_max(kg)', 'Champ_V9', 'Missing_data',
    'Carrosserie', 'range'
]

# Ensure the columns exist in the DataFrame
columns = [col for col in columns if col in cl_union_cleaned.columns]

# Select relevant variables and drop rows with missing CO2 values
filtered_df = cl_union_cleaned[columns].dropna(subset=['CO2'])

# Ensure no duplicate columns exist in filtered_df
assert filtered_df.columns.duplicated().sum() == 0, "There are still duplicate columns in filtered_df"

# Step 1: Identify the car that emits the most CO2 emissions
max_co2 = filtered_df['CO2'].max()
max_co2_car = filtered_df[filtered_df['CO2'] == max_co2]

print("Car with the highest CO2 emissions:")
print(max_co2_car[['brand', 'Model_file', 'Commercial_name', 'CO2']])

# Step 2: Analyze correlations between CO2 emissions and other numerical variables
numerical_columns = [
    'CO2', 'Administrative_power', 'power_maximal (kW)', 'Urban_consumption (l/100km)',
    'Extra_urban_consumption(l/100km)', 'Consumption_mix(l/100km)', 'CO_type_I (g/km)',
    'Hydrocarbon', 'nitrogen oxides', 'hydrocarbon and nitrogen oxides', 'Particles (g/km)',
    'Empty_mass_min(kg)', 'Empty_mass_max(kg)', 'range'
]

# Ensure the numerical columns exist in the DataFrame
numerical_columns = [col for col in numerical_columns if col in filtered_df.columns]
# Reset the index of the DataFrame to ensure unique labels
filtered_df = filtered_df.reset_index(drop=True)
# Step 3: Create a scatter plot matrix to visualize relationships between CO2 and other variables
sns.pairplot(filtered_df[numerical_columns])
plt.suptitle('Scatter Plot Matrix of CO2 and Related Variables', y=1.02)
plt.show()


# Print the correlation matrix to identify the variables that correlate the most with CO2 emissions
print("Correlation matrix with CO2 emissions:")

"""The scatter plot matrix shows:

Strong Positive Correlation between CO2 emissions and both Power Maximal (kW) and Administrative Power—higher power vehicles emit more CO2.
Moderate Positive Correlation between CO2 emissions and Empty Mass—heavier vehicles tend to have higher emissions.
CO2 Type aligns closely with CO2 emissions, indicating consistency or redundancy between these variables.
This matrix highlights power and weight as key drivers of CO2 emissions in vehicles.
"""

numerical_columns = [
    'CO2', 'Administrative_power', 'power_maximal (kW)', 'Urban_consumption (l/100km)',
    'Extra_urban_consumption(l/100km)', 'Consumption_mix(l/100km)', 'CO_type_I (g/km)',
    'Hydrocarbon', 'nitrogen oxides', 'hydrocarbon and nitrogen oxides', 'Particles (g/km)',
    'Empty_mass_min(kg)', 'Empty_mass_max(kg)', 'range'
    ]

# Ensure the numerical columns exist in the DataFrame
numerical_columns = [col for col in numerical_columns if col in filtered_df.columns]
# Create a heatmap to visualize the correlation matrix
# Select only numeric columns for correlation calculation
numerical_df = filtered_df[numerical_columns].select_dtypes(include=['number'])
correlation_matrix = numerical_df.corr()

plt.figure(figsize=(14, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix with CO2 Emissions')
plt.show()

print(correlation_matrix['CO2'].sort_values(ascending=False))

"""The correlation matrix reveals:

Strong Positive Correlation: CO2 emissions are strongly linked to vehicle weight—Empty Mass Min (0.66) and Empty Mass Max (0.60).
Moderate Positive Correlation: Administrative Power (0.50) is moderately correlated with CO2 emissions, indicating that higher power vehicles tend to emit more CO2.
Weaker Correlation: Power Maximal (0.38) has a weaker correlation with CO2 emissions.
Negative Correlation: CO2 Type (-0.12) shows a slight negative correlation with CO2 emissions, suggesting some data inconsistencies or nuances.
Overall, vehicle weight is the most significant factor influencing CO2 emissions, followed by power-related variables.

# Step 14: Creation of CO2 categories


> CO2 class
It is based on the level of CO2 emissions approved according to European regulations. 7 classes specifying the CO2
emission levels of a new vehicle appear on the "energy / CO2" label , which must be displayed on each vehicle presented at the point of sale. Each class corresponds to a letter (from A to G) and a color (green / yellow / orange / red).


*   Class A: CO2 emissions less than or equal to 100 g/km
*   Class B: from 101 to 120 g / km
*   Class C: from 121 to 140 g/km
*   Class D: from 141 to 160 g / km
*   Class E: from 161 to 200 g / km
*   Class F: from 201 to 250 g / km
*   Class G: greater than 250 g/km




The increasing diversity of propulsion technologies, and the evolution of homologation protocols, have gradually led to a decorrelation between homologated CO2 emissions and the energy/fuel consumption of vehicles in real use: some vehicles, with low or zero CO2 emissions , can nevertheless be very energy-intensive.
"""

# Define a function to classify the CO2 emissions
def classify_co2_emissions(co2_value):
    if co2_value <= 100:
        return 'A'
    elif 101 <= co2_value <= 120:
        return 'B'
    elif 121 <= co2_value <= 140:
        return 'C'
    elif 141 <= co2_value <= 160:
        return 'D'
    elif 161 <= co2_value <= 200:
        return 'E'
    elif 201 <= co2_value <= 250:
        return 'F'
    else:  # Greater than 250 g/km
        return 'G'

# Apply the function to the 'CO2' column to create the 'CO2_class' column
cl_union_cleaned['CO2_class'] = cl_union_cleaned['CO2'].apply(classify_co2_emissions)

# Check the first few rows to verify
print(cl_union_cleaned[['CO2', 'CO2_class']].head())

# Verify if 'CO2_class' is in the DataFrame
print(cl_union_cleaned.columns)

# Check the first few rows to ensure the column is present and correctly assigned
print(cl_union_cleaned[['CO2', 'CO2_class']].head())

"""# Step 15: Research the relationship between CO2 target variable and other variables

## **Research relationship between CO2 emissions and Consumption mix**
"""

co2_col = 'CO2'  # Replace with the correct column name for CO2
# Let's assume the correct column name for consumption mix is something like 'Consumption_mix_l_per_100km'
consumption_col ='Consumption_mix(l/100km)'  # Replace with the correct column name for consumption mix

# Convert relevant columns to numerical types if they are not already
cl_union_cleaned[co2_col] = pd.to_numeric(cl_union_cleaned[co2_col], errors='coerce')
cl_union_cleaned[consumption_col] = pd.to_numeric(cl_union_cleaned[consumption_col], errors='coerce')

# Select relevant variables and drop rows with missing values in any of the important columns
filtered_df = cl_union_cleaned[['brand', 'Model_file', 'Commercial_name', co2_col, consumption_col]].dropna(subset=[co2_col, consumption_col])

# Combine brand and commercial name for better labels
filtered_df['Car'] = filtered_df['brand'] + " " + filtered_df['Commercial_name']

# Create a scatter plot
plt.figure(figsize=(12, 8))

# Scatter plot for CO2 vs. Consumption_mix
scatter = plt.scatter(
    filtered_df[co2_col],
    filtered_df[consumption_col],
    alpha=0.6,
    c=filtered_df[consumption_col],  # Color by Consumption_mix for additional dimension
    cmap='viridis'
)

plt.colorbar(scatter, label='Consumption Mix (l/100km)' )
plt.xlabel('CO2 Emissions (g/km)')
plt.ylabel('Consumption_mix (l/100km)' )
plt.title('Scatter Plot of CO2 Emissions and Consumption Mix')

# Highlight the car with the highest CO2 emissions
max_co2 = filtered_df[co2_col].max()
max_co2_car = filtered_df[filtered_df[co2_col] == max_co2]
plt.scatter(max_co2_car[co2_col], max_co2_car[consumption_col], color='red', edgecolor='black', label='Highest CO2 Emitter')

# Add legend
plt.legend(loc='upper right')

# Show the plot
plt.tight_layout()
plt.show()

"""The scatter plot shows a strong positive correlation between CO2 emissions and fuel consumption. As vehicles consume more fuel (l/100km), their CO2 emissions increase. The highest CO2 emitter, highlighted in red, also has the highest fuel consumption, confirming that less efficient vehicles are major contributors to higher emissions. Outliers with very high emissions and consumption indicate potential areas for efficiency improvements."""

import pandas as pd


print("Column names in the DataFrame:")
print(cl_union_cleaned.columns.tolist())

# Correct column names based on the previous print output
consumption_col = 'Consumption_mix(l/100km)'  # Replace with the correct column name for consumption mix

# Convert the consumption_mix (l/100km) column to numerical type if it is not already
cl_union_cleaned[consumption_col] = pd.to_numeric(cl_union_cleaned[consumption_col], errors='coerce')

# Filter numeric columns
numeric_columns = cl_union_cleaned.select_dtypes(include=['float64', 'int64']).columns

# Calculate the correlation matrix using only numeric columns
correlation_matrix = cl_union_cleaned[numeric_columns].corr()

# Display the correlation of all variables with consumption_mix (l/100km)
consumption_correlations = correlation_matrix[consumption_col].sort_values(ascending=False)
print("Correlation of variables with consumption_mix (l/100km):")
print(consumption_correlations)

# Sort the DataFrame by consumption_mix (l/100km) in descending order and select the top 20 rows
top_20_consumption_cars = cl_union_cleaned.sort_values(by=consumption_col, ascending=False).head(20)

# Display the top 20 cars with the highest consumption_mix (l/100km) and relevant features
relevant_features = ['brand', 'Model_file', 'Administrative_power', 'power_maximal (kW)', 'Empty_mass_min(kg)', 'Empty_mass_max(kg)', consumption_col]
print("Top 20 cars with the highest consumption_mix (l/100km) and relevant features:")
print(top_20_consumption_cars[relevant_features])

# Step 1: Define a function to classify the CO2 emissions
def classify_co2_emissions(co2_value):
    if co2_value <= 100:
        return 'A'
    elif 101 <= co2_value <= 120:
        return 'B'
    elif 121 <= co2_value <= 140:
        return 'C'
    elif 141 <= co2_value <= 160:
        return 'D'
    elif 161 <= co2_value <= 200:
        return 'E'
    elif 201 <= co2_value <= 250:
        return 'F'
    else:  # Greater than 250 g/km
        return 'G'

# Step 2: Apply the function to classify CO2 emissions and create the 'CO2_class' column
cl_union_cleaned['CO2_class'] = cl_union_cleaned['CO2'].apply(classify_co2_emissions)

# Step 3: Group data by CO2 emission classes
grouped_by_co2_class = cl_union_cleaned.groupby('CO2_class')

# Step 4: Loop through each CO2 class and display the most common brands, models, and fuel types
for co2_class, group in grouped_by_co2_class:
    print(f"\nCO2 Emission Class: {co2_class}")

    # Most common brands in this CO2 class
    common_brands = group['brand'].value_counts().head(5)  # Display top 5 brands
    print("\nMost common brands:")
    print(common_brands)

    # Most common models in this CO2 class
    common_models = group['Model_file'].value_counts().head(5)  # Display top 5 models
    print("\nMost common models:")
    print(common_models)

    # Most common fuel types in this CO2 class
    common_fuel_types = group['fuel_type'].value_counts().head(5)  # Display top 5 fuel types
    print("\nMost common fuel types:")
    print(common_fuel_types)

print(cl_union_cleaned.columns)

import matplotlib.pyplot as plt
import seaborn as sns

# Define the CO2 classes for consistency
co2_classes = ['A', 'B', 'C', 'D', 'E', 'F', 'G']

# Step 1: Initialize a figure with subplots (adjust the grid to accommodate all classes)
fig, axs = plt.subplots(4, 2, figsize=(15, 24))  # Increased rows to 4
axs = axs.flatten()  # Flatten the axes array for easy iteration

# Step 2: Loop through each CO2 class and create a bar plot for most common brands
for i, co2_class in enumerate(co2_classes):
    # Filter the data for the current CO2 class
    group = cl_union_cleaned[cl_union_cleaned['CO2_class'] == co2_class]

    if group.empty:
        continue  # Skip if there's no data for this class

    # Get the top 5 most common brands
    common_brands = group['brand'].value_counts().head(5)

    # Plot the data
    sns.barplot(x=common_brands.values,  y=common_brands.index, ax=axs[i], palette='viridis')
    axs[i].set_title(f'Most Common Brands (CO2 Class {co2_class})')
    axs[i].set_xlabel('Count')
    axs[i].set_ylabel('Brand')

# Step 3: Adjust layout and show the figure
plt.tight_layout()
plt.show()

"""## common characteristics CO2 emissions below 95"""

import pandas as pd

# Assuming cl_union is the DataFrame that already contains your combined data
# Print column names to verify
print("Column names in the DataFrame:")
print(cl_union_cleaned.columns.tolist())

# Correct column names based on the previous print output
co2_col = 'CO2'  # Replace with the correct column name for CO2
fuel_type_col = 'fuel_type'  # Replace with the correct column name for fuel_type

# Convert the CO2 column to numerical type if it is not already
cl_union_cleaned[co2_col] = pd.to_numeric(cl_union_cleaned[co2_col], errors='coerce')

# Filter the DataFrame to include only cars with CO2 emissions of 95 or lower
filtered_df = cl_union_cleaned[cl_union_cleaned[co2_col] <= 95]

# Display the filtered DataFrame
print("Cars with CO2 emissions of 95 or lower:")
print(filtered_df)

# Describe the common characteristics CO2 emissions below 95
common_characteristics = filtered_df.describe(include='all')

# Additionally, you can analyze specific categorical columns like brand, model, and fuel type
common_brands = filtered_df['brand'].value_counts()
common_models = filtered_df['Model_file'].value_counts()
common_fuel_types = filtered_df['fuel_type'].value_counts()

print("\nMost common brands:")
print(common_brands)

print("\nMost common models:")
print(common_models)

print("\nMost common fuel types:")
print(common_fuel_types)

"""#Step 16: Emission Trend Analysis"""

df_cl_1['Year'] = 2012
df_cl_2['Year'] = 2013
df_cl_3['Year'] = 2014

# Standardize the column names before merging
df_cl_1.rename(columns={'Modèle dossier': 'Model_file'}, inplace=True)
df_cl_2.rename(columns={'Modèle dossier': 'Model_file'}, inplace=True)
df_cl_3.rename(columns={'Modèle dossier': 'Model_file'}, inplace=True)

cl_union_cleaned.dtypes

print(cl_union_cleaned['fuel_type'])

# Concatenate the DataFrames
cl_union_cleaned = pd.concat([df_cl_1, df_cl_2, df_cl_3], axis=0, ignore_index=True)

# Reapply the fuel type categorization function
cl_union_cleaned['fuel_type'] = cl_union_cleaned['fuel_type'].apply(categorize_fuel_type)

# Handle missing values by filling them with the mode of the 'fuel_type' column
cl_union_cleaned['fuel_type'] = cl_union_cleaned['fuel_type'].fillna(cl_union_cleaned['fuel_type'].mode()[0])

# Check unique values in the 'fuel_type' column
print(cl_union_cleaned['fuel_type'].unique())

# Also reapply any other transformations, such as 'CO2_class'
cl_union_cleaned['CO2_class'] = cl_union_cleaned['CO2'].apply(classify_co2_emissions)

# Reapply the Group and Country mappings (if needed)
cl_union_cleaned[['Group', 'Country']] = cl_union_cleaned['brand'].apply(map_brand_to_group_country).apply(pd.Series)

# Check the final state of the DataFrame
print(cl_union_cleaned.info())

print(cl_union_cleaned.columns)

# Group by Year and Model to get average CO2 emissions per model each year
yearly_emissions_model = cl_union_cleaned.groupby(['Year', 'Model_file'])['CO2'].mean().reset_index()

# Group by Year and Carrosserie (vehicle type) to get average CO2 emissions per type each year
yearly_emissions_type = cl_union_cleaned.groupby(['Year', 'Carrosserie'])['CO2'].mean().reset_index()

print(yearly_emissions_model.head())

# Select top 5 vehicle types for simplicity
top_vehicle_types = yearly_emissions_type['Carrosserie'].value_counts().head(5).index.tolist()

plt.figure(figsize=(14, 8))

for vehicle_type in top_vehicle_types:
    type_data = yearly_emissions_type[yearly_emissions_type['Carrosserie'] == vehicle_type]
    plt.plot(type_data['Year'], type_data['CO2'], marker='o', label=vehicle_type)

plt.xlabel('Year')
plt.ylabel('Average CO2 Emissions (g/km)')
plt.title('CO2 Emission Trends Over Time by Vehicle Type')
plt.legend(title='Vehicle Type')
plt.grid(True)
plt.show()

"""Coupe and Cabriolet:

Coupe vehicles have the highest CO2 emissions, starting above 210 g/km in 2012, slightly decreasing in 2013, and then increasing again by 2014.
Cabriolet vehicles start just below 200 g/km and show a consistent decline throughout the period.

Break vehicles have relatively stable CO2 emissions around 160 g/km, with a slight decline over the period.
Berline vehicles show a steady decrease from around 150 g/km in 2012 to about 140 g/km in 2014, indicating a trend toward improved efficiency.

Combispace:
Combispace vehicles exhibit the lowest CO2 emissions, starting slightly above 140 g/km and declining steadily to below 140 g/km by 2014.


High Emission Vehicle Types: Coupes and Cabriolets have the highest CO2 emissions, likely due to their design focus on performance and power, which typically results in higher fuel consumption.

Efficiency Improvements: Berline and Combispace vehicles show a clear trend of decreasing CO2 emissions, suggesting ongoing improvements in fuel efficiency or a shift towards greener technologies within these vehicle types.

Stable Emissions: Break vehicles show a relatively stable trend, with only slight improvements in emissions over the years.
"""

import matplotlib.pyplot as plt

# Group by Year and Model to get average CO2 emissions per model each year
yearly_emissions_brand = cl_union_cleaned.groupby(['Year', 'brand'])['CO2'].mean().reset_index()


# Get the top 5 models based on the combined data
top_brands = cl_union_cleaned['brand'].value_counts().head(10).index.tolist()
print("Top brands selected:", top_brands)

# Filter the DataFrame for these top models
filtered_yearly_emissions = yearly_emissions_brand [yearly_emissions_brand ['brand'].isin(top_brands)]

# Plot the data
plt.figure(figsize=(14, 8))

for model in top_brands:
    model_data = filtered_yearly_emissions[filtered_yearly_emissions['brand'] == model]
    plt.plot(model_data['Year'], model_data['CO2'], marker='o', label=model)

plt.xlabel('Year')
plt.ylabel('Average CO2 Emissions (g/km)')
plt.title('CO2 Emission Trends Over Time by Brand')
plt.legend(title='Brand')
plt.grid(True)
plt.show()

"""BMW consistently has the highest CO2 emissions among the brands, starting above 170 g/km in 2012 and steadily decreasing over the years, ending just below 160 g/km in 2014.

Mercedes-Benz and Volkswagen also start with relatively high CO2 emissions, around 160 g/km, and show a gradual decrease over time.

Renault, Citroen, and Opel have the lowest CO2 emissions, with a significant decline observed from 2012 to 2014. By 2014, these brands achieve CO2 emissions well below 130 g/km.

Skoda and Fiat show slight decreases in CO2 emissions, maintaining a mid-range position relative to other brands.

Audi exhibits a slight increase in CO2 emissions, contrasting with the general downward trend seen in other brands.

Interpretation:
High Emission Brands: BMW, Mercedes-Benz, and Volkswagen remain at the higher end of CO2 emissions, although all show improvements over time.

Efficient Brands: Renault, Citroen, and Opel demonstrate significant improvements, reaching the lowest CO2 emissions by 2014, indicating a strong focus on efficiency.

Mixed Trends: Audi's slight increase in emissions may suggest a shift in model offerings or other factors affecting fuel efficiency during this period.
"""

# Filter the data for the winning model across all years
winning_model = 'SKODA'  # Replace with the actual model name

# Filter the data for the specific model
model_data = cl_union_cleaned [cl_union_cleaned['brand'] == winning_model]

# Ensure that only numeric columns are aggregated
numeric_columns = model_data.select_dtypes(include=['number']).columns

# Group by 'Year' and calculate the mean for numeric columns only
spec_changes = model_data.groupby('Year')[numeric_columns].mean()

print("Technical specifications over time for the winning model:")
print(spec_changes)

# Plot the changes in key specifications
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 8))
plt.plot(spec_changes.index, spec_changes['power_maximal (kW)'], marker='o', label='Maximal Power (kW)')
plt.plot(spec_changes.index, spec_changes['Empty_mass_max(kg)'], marker='o', label='Empty Mass Max (kg)')
plt.plot(spec_changes.index, spec_changes['Administrative_power'], marker='o', label='Administrative Power')

plt.xlabel('Year')
plt.ylabel('Values')
plt.title(f'Changes in Technical Specifications for {winning_model}')
plt.legend()
plt.grid(True)
plt.show()

"""Maximal Power (kW): The maximal power remains stable around 100-200 kW throughout the period, indicating no significant changes in engine performance for SKODA vehicles during these years.

Empty Mass Max (kg): This metric starts at around 1425 kg in 2012, shows a slight decrease in 2013, and continues to decline slightly into 2014, ending just below 1400 kg. This suggests a gradual reduction in vehicle weight.

Administrative Power: This remains close to zero throughout the period, indicating that there were no significant changes or fluctuations in this aspect of the vehicles.

Interpretation:
Stable Power Performance: The consistent maximal power suggests SKODA maintained similar engine power output across these years, likely reflecting stability in the types of engines used.

Weight Reduction: The slight but consistent decrease in Empty Mass Max could indicate efforts by SKODA to make their vehicles lighter, potentially to enhance fuel efficiency or meet evolving regulatory standards.

Consistency in Administrative Power: The minimal fluctuation in administrative power reflects that this technical specification was likely standardized across models during these years.
"""

print(cl_union_cleaned['fuel_type'])

print(cl_union_cleaned['fuel_type'].unique())
print(cl_union_cleaned['fuel_type'].isnull().sum())  # Check for missing values

# Filter the data to include only SKODA vehicles
skoda_data = cl_union_cleaned[cl_union_cleaned['brand'] == 'SKODA']

# Group by Year and calculate the mean for numeric variables
numeric_features = ['power_maximal (kW)', 'Administrative_power', 'Empty_mass_max(kg)', 'Empty_mass_min(kg)', 'CO2']
skoda_numeric_changes = skoda_data.groupby('Year')[numeric_features].mean()

print("Numeric variables over time for SKODA:")
print(skoda_numeric_changes)

# Plot the changes in numeric variables
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 8))

for feature in numeric_features:
    if feature != 'CO2':  # Exclude CO2 from this plot, we'll analyze it separately
        plt.plot(skoda_numeric_changes.index, skoda_numeric_changes[feature], marker='o', label=feature)

plt.xlabel('Year')
plt.ylabel('Average Values')
plt.title('Changes in Numeric Variables for SKODA (2012-2014)')
plt.legend(title='Numeric Variables')
plt.grid(True)
plt.show()

"""Power Maximal (kW) and Administrative Power: These values remain relatively stable over the three years. Power Maximal is consistent around 100-200 kW, while Administrative Power stays close to zero, indicating little change in engine power output over time.

Empty Mass Max (kg) and Empty Mass Min (kg): Both metrics show high values, with Empty Mass Max around 1400 kg and Empty Mass Min showing a slight decline from about 1425 kg in 2012 to just below 1400 kg in 2014. This indicates a small reduction in vehicle weight over time.

Stable Power Output: The consistent Power Maximal and Administrative Power suggest that SKODA maintained a steady level of engine performance across these years.

Slight Decrease in Vehicle Weight: The small decline in both Empty Mass Max and Min may indicate efforts to reduce vehicle weight, potentially to improve fuel efficiency or meet regulatory standards.
"""

# Identify the relevant categorical columns
categorical_features = ['fuel_type', 'Carrosserie', 'range', 'Gearbox']  # Replace with your actual categorical columns

# Group by Year and calculate the mode (most common value) for each categorical feature
# Handle cases where mode returns only one value
skoda_categorical_changes = skoda_data.groupby('Year')[categorical_features].agg(lambda x: x.mode()[0] if not x.mode().empty else 'Unknown') # Replace None with 'Unknown'

print("Categorical variables over time for SKODA:")
print(skoda_categorical_changes)

# Plot the changes for each categorical variable
plt.figure(figsize=(14, 6))

for feature in categorical_features:
    plt.plot(skoda_categorical_changes.index, skoda_categorical_changes[feature], marker='o', label=feature)

plt.xlabel('Year')
plt.ylabel('Category')
plt.title('Changes in Categorical Variables for SKODA (2012-2014)')
plt.legend(title='Categorical Variables')
plt.grid(True)
plt.show()

"""Gearbox (M 5 vs. M 6): The chart shows a transition from M 5 (5-speed manual gearbox) to M 6 (6-speed manual gearbox). Over time, M 6 remains constant, while M 5 shows some fluctuations, indicating a shift in preference or standardization towards the 6-speed gearbox.

Range (MOY-INFER vs. MOY-SUPER): The MOY-INFER category shows an increase over time, while MOY-SUPER remains constant. This suggests a shift in the range classification for the SKODA vehicles, possibly towards more mid-range offerings.

Carrosserie (BERLINE): The BERLINE category remains constant throughout the period, indicating that this body type is a stable offering for SKODA without significant changes.

### ***moy-infer is a low-mid range vehicle***



# Moyenne Inférieure (moy-infer)
Segment: This typically refers to lower-middle or compact cars. These vehicles are often part of the "C-segment" in European car classification, which includes small family cars or compact cars.
Characteristics:
Size: Generally smaller in size compared to "moy-super gamme" cars, with more compact dimensions.
Price: Usually more affordable, making them accessible to a broader range of consumers.
Features: Basic to mid-range features, focusing on practicality and efficiency rather than luxury.

# Moyenne Supérieure (moy-super gamme): Larger, more expensive, and feature-rich cars, often mid-size sedans or premium vehicles.

Hence, Skoda changed their size and features to make the care more practical and efficient resulting in lower CO2 emissions

#Step 17: Data Cleansing Preparation
"""

print(cl_union_cleaned['fuel_type'])

# Step 2: Initial inspection

print(cl_union_cleaned.info())  # Check data types and non-null counts
print(cl_union_cleaned.describe()) # Summary statistics
print(cl_union_cleaned.isnull().sum())  # Check for missing values

# Step 3: Backup the original dataset
cl_union_backup = cl_union_cleaned.copy()

# Step 4: Check for duplicates
print(cl_union_cleaned.duplicated().sum())

"""

>




---



 Step 5: Plan and apply data cleaning strategies
handle missing values, convert data types, drop duplicates
cl_union.drop_duplicates(inplace=True)
cl_union['date_column'] = pd.to_datetime(cl_union['date_column'])

Step 6: Segment or filter the data if needed
cl_union_filtered = cl_union[cl_union['year'] >= 2014]

Step 7: Data exploration and further cleaning
sns.histplot(cl_union['CO2'])
sns.boxplot(x=cl_union['brand'], y=cl_union['CO2'])



---


>

"""

cl_union_cleaned.info()

"""# Step 18: Deletion of Column Hybride"""

cl_union_cleaned = cl_union_cleaned.drop(columns=['Hybride'])

# Verify that the column has been removed
print("Columns after dropping 'Hybride':")
print(cl_union_cleaned.columns.tolist())

"""# Step 19: Deletion of fuel types with electric or hybrid engine"""

print(cl_union_cleaned['fuel_type'])

# Correct column names based on the previous print output
fuel_type_col = 'fuel_type'  # Replace with the correct column name for fuel_type

# Verify unique values in the fuel_type column before removal
print("Unique values in the fuel_type column before removal:")
print(cl_union_cleaned[fuel_type_col].unique())

# Remove rows where fuel_type is 'EL','EE', 'EH', 'GL', 'GH'
cl_union_cleaned =cl_union_cleaned [~cl_union_cleaned[fuel_type_col].isin(['Electric', 'Hybrid_Petrol/Electric_plug_in',  'Hybrid_Petrol/Electric_non_plug_in','Hybrid_Diesel/Electric_non_plug_in', 'Hybrid_Diesel/Electric_plug_in'])]

# Verify that the rows have been removed
print("Unique values in the fuel_type column after removal:")
print(cl_union_cleaned[fuel_type_col].unique())

import matplotlib.pyplot as plt

# Get the top 5 models based on the combined data
top_models = cl_union_cleaned['Model_file'].value_counts().head(5).index.tolist()
print("Top models selected:", top_models)

# Filter the DataFrame for these top models
filtered_yearly_emissions = yearly_emissions_model[yearly_emissions_model['Model_file'].isin(top_models)]

# Plot the data
plt.figure(figsize=(14, 8))

for model in top_models:
    model_data = filtered_yearly_emissions[filtered_yearly_emissions['Model_file'] == model]
    plt.plot(model_data['Year'], model_data['CO2'], marker='o', label=model)

plt.xlabel('Year')
plt.ylabel('Average CO2 Emissions (g/km)')
plt.title('CO2 Emission Trends Over Time by Vehicle Model')
plt.legend(title='Vehicle Model')
plt.grid(True)
plt.show()

"""Sprinter consistently has the highest CO2 emissions, remaining stable at over 220 g/km throughout the period from 2012 to 2014.

Viano and Vito also maintain stable CO2 emissions around 210 g/km, showing no significant changes during these years.

Crafter shows a decreasing trend from 210 g/km in 2012 to below 200 g/km in 2013, but then increases again slightly in 2014, indicating some fluctuations in emissions.

Classe E has the lowest CO2 emissions among the models, starting at around 160 g/km in 2012 and steadily decreasing to below 150 g/km by 2014.

#Step 20: Data visualization and deletion of the minibus category
## we drop the minibus as it does not fit our analysis of conventional cars
"""

# Update these variables according to your actual column names
carrosserie_col = 'Carrosserie'  # Replace with the correct column name for Carrosserie

# Remove rows where Carrosserie is 'MINIBUS'
cl_union_cleaned = cl_union_cleaned[cl_union_cleaned[carrosserie_col] != 'MINIBUS']

_
# Verify that the rows have been removed by printing unique values in the Carrosserie column
print("Unique values in the Carrosserie column after removing 'MINIBUS':")
print(cl_union_cleaned[carrosserie_col].unique())

print(cl_union_cleaned.columns)

"""##Numbers of cars over time"""

# Count the number of cars produced each year
production_counts = cl_union_cleaned.groupby('Year').size()

print("Number of cars produced each year:")
print(production_counts)

# Plot the production counts
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(production_counts.index, production_counts.values, marker='o', linestyle='-', color='blue')
plt.xlabel('Year')
plt.ylabel('Number of Cars Produced')
plt.title('Number of Cars Produced Each Year (2012-2014)')
plt.grid(True)
plt.show()

"""Car production peaked in 2013 at around 11,000 units, followed by a sharp decline in 2014 back to 2012 levels of approximately 7,500 units. This suggests a temporary surge in production in 2013 that was not maintained.
Production Spike in 2013: The peak in 2013 suggests a potential increase in demand, expansion of manufacturing capacity, or the introduction of new models that year.
Decline in 2014: The return to 2012 production levels in 2014 could indicate market saturation, economic factors, or strategic shifts in production focus.
"""

import pandas as pd
import matplotlib.pyplot as plt

# Group the data by 'Year' and calculate the average CO2 emissions for each year
average_co2_per_year = cl_union_cleaned.groupby('Year')['CO2'].mean()

# Display the results
print("Average CO2 emissions for each year:")
print(average_co2_per_year)

# Visualize the average CO2 emissions over the years
plt.figure(figsize=(10, 6))
plt.plot(average_co2_per_year.index, average_co2_per_year.values, marker='o', linestyle='-', color='blue')
plt.xlabel('Year')
plt.ylabel('Average CO2 Emissions (g/km)')
plt.title('Average CO2 Emissions Per Year (2012-2014)')
plt.grid(True)
plt.show()

"""
The graph shows a steady decline in average CO2 emissions from vehicles between 2012 and 2014, dropping from 152 g/km to 145 g/km. This indicates a continuous improvement in vehicle efficiency or environmental regulations during this period, reflecting positive progress towards reducing emissions."""

# Count the number of each car model produced across all years
model_production_counts = cl_union_cleaned['Model_file'].value_counts()

print("Top 10 most produced car models:")
print(model_production_counts.head(10))

# Get the top 10 most produced models
top_10_models = model_production_counts.head(10).index

# Filter the dataset to include only the top 10 most produced models
top_models_data = cl_union_cleaned[cl_union_cleaned['Model_file'].isin(top_10_models)]

# Calculate the average CO2 emissions for these top models
top_models_co2 = top_models_data.groupby('Model_file')['CO2'].mean().sort_values()

print("Average CO2 emissions of the top 10 most produced car models:")
print(top_models_co2)

# Plot the CO2 emissions of the top 10 most produced models
plt.figure(figsize=(10, 6))
top_models_co2.plot(kind='bar', color='green')
plt.xlabel('Car Model')
plt.ylabel('Average CO2 Emissions (g/km)')
plt.title('Average CO2 Emissions of the Top 10 Most Produced Car Models (2012-2014)')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

"""Classe M has the highest average CO2 emissions, exceeding 175 g/km.
Classe B and Classe A have the lowest average CO2 emissions, around 125 g/km.
Other models like Classe C, Classe E, Serie 3, Caddy, Classe GLK, Serie 5, and Classe S have average CO2 emissions ranging between 130 g/km and 160 g/km.

Classe M stands out as the model with the highest CO2 emissions, suggesting it may be a larger, less efficient vehicle compared to others.
Classe B and Classe A are the most efficient among the top 10, likely indicating smaller or more fuel-efficient designs.

# Step 21: Data visualization and analysis – drop classe M  AS IT EMITS TOO MUCH CO2
"""

# Drop rows where the car model is 'Classe S'
cl_union_cleaned = cl_union_cleaned[cl_union_cleaned['Model_file'] != 'Classe S']

# Verify that 'Classe S' has been dropped
print(cl_union_cleaned['Model_file'].unique())

print(cl_union_cleaned.columns)

"""# Step 22: Determining outliers and deletion

# **which cars are outliers based on carosserie and maximal engine power**
"""

# Import necessary libraries
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Print column names to verify
print("Column names in the DataFrame:")
print(cl_union_cleaned.columns.tolist())

# Correct column names based on the previous print output
carrosserie_col = 'Carrosserie'  # Replace with the correct column name for Carrosserie
power_col = 'power_maximal (kW)'  # Replace with the correct column name for power_maximal

# Convert relevant columns to numerical types if they are not already
cl_union_cleaned[power_col] = pd.to_numeric(cl_union_cleaned[power_col], errors='coerce')

# Select relevant variables and drop rows with missing values in any of the important columns
filtered_df = cl_union_cleaned[[carrosserie_col, power_col]].dropna(subset=[power_col])

# Create a boxplot for power_maximal (kW) grouped by Carrosserie
plt.figure(figsize=(12, 8))
sns.boxplot(x=power_col, y=carrosserie_col, data=filtered_df, palette='viridis')

# Set labels and title
plt.xlabel('Power Maximal (kW)')
plt.ylabel('Carrosserie')
plt.title('Boxplot of Power Maximal (kW) by Carrosserie')

# Show the plot
plt.tight_layout()
plt.show()

"""Berline, Coupe, and TS Terrains/Chemins categories still have the highest power outputs with many outliers, particularly in the higher power ranges (above 200 kW).
Other categories like Break, Cabriolet, Monospace, and Combispace have relatively lower power outputs, with fewer outliers.
The removal of "Classe M" appears to have reduced the overall range of power outputs, particularly in categories where "Classe M" might have contributed significantly to higher power outliers.

Impact of Removing Classe M: The removal of "Classe M" models has likely reduced some of the extreme outliers in terms of power output, indicating that "Classe M" was a significant contributor to high-power vehicles.
Remaining Trends: The general trend of high power concentration in Berline, Coupe, and TS Terrains/Chemins remains consistent, suggesting these categories are still associated with higher-performance vehicles.

Removing outliers before splitting ensures that both the training and test sets are drawn from the same underlying data distribution without extreme values. This approach can lead to a more homogeneous and stable model training process and are more representative of the general data population.
"""

# Define the thresholds for outlier removal
high_power_threshold = 300  # Outliers for high-powered categories
low_power_threshold = 150   # Outliers for lower-powered categories

# Define the categories considered high-powered and low-powered
high_power_categories = ['BERLINE', 'COUPE', 'CABRIOLET', 'BREAK', 'TS TERRAINS/CHEMINS']
low_power_categories = ['MONOSPACE', 'COMBISPACE', 'MINIBUS']

# Create a filter to remove outliers for high-powered categories
high_power_filter = (cl_union_cleaned['Carrosserie'].isin(high_power_categories)) & (cl_union_cleaned['power_maximal (kW)'] <= high_power_threshold)

# Create a filter to remove outliers for low-powered categories
low_power_filter = (cl_union_cleaned['Carrosserie'].isin(low_power_categories)) & (cl_union_cleaned['power_maximal (kW)'] <= low_power_threshold)

# Keep the non-outliers for both categories
non_outliers = cl_union_cleaned[high_power_filter | low_power_filter]

# Verify the number of rows before and after removing outliers
print(f"Original number of rows: {cl_union_cleaned.shape[0]}")
print(f"Number of rows after removing outliers: {non_outliers.shape[0]}")

# Optionally, save the cleaned DataFrame
# non_outliers.to_csv('cleaned_data.csv', index=False)

print(cl_union_cleaned.columns)

"""# Step 23: Deletion of variable Last_update as it is missing too much data

Removing the column last update as it is irrelevant and misses a big chunk of data
"""

cl_union_cleaned = cl_union_cleaned.drop(columns=['Last_update'])

print(cl_union_cleaned.columns)

# Identify unique categories for each categorical column before splitting
Carrosserie_categories = cl_union_cleaned['Carrosserie'].unique().tolist()
fuel_type_categories = cl_union_cleaned['fuel_type'].unique().tolist()
Gearbox_categories = cl_union_cleaned['Gearbox'].unique().tolist()

print("Carrosserie Categories:", Carrosserie_categories)
print("Fuel Type Categories:", fuel_type_categories)
print("Gearbox Categories:", Gearbox_categories)

print(cl_union_cleaned.columns)

cl_union_cleaned.to_csv('/content/drive/My Drive/cl_union_cleaned.csv', index=False)
cl_union_cleaned_verify = pd.read_csv('/content/drive/My Drive/cl_union_cleaned.csv')
print(cl_union_cleaned_verify)

"""# Step 24: Data Separation – import sklearn library"""

from sklearn.model_selection import train_test_split

"""# Step 25: Splitting Cl Dataset into train (80) and test (20) data"""

train_set, test_set = train_test_split(cl_union_cleaned, test_size=0.2, random_state=42)

# Verify the split
print(f"Training set size: {train_set.shape[0]} rows")
print(f"Test set size: {test_set.shape[0]} rows")

from sklearn.preprocessing import OneHotEncoder

# Use the identified unique categories for OneHotEncoder
encoder = OneHotEncoder(categories=[Carrosserie_categories, fuel_type_categories, Gearbox_categories], drop=None) # Remove sparse argument

# Fit the encoder on the training set and transform both train and test sets
encoded_train = encoder.fit_transform(train_set[['Carrosserie', 'fuel_type', 'Gearbox']])
encoded_test = encoder.transform(test_set[['Carrosserie', 'fuel_type', 'Gearbox']])

# Convert the encoded data back into DataFrames
encoded_train_df = pd.DataFrame(encoded_train.toarray(), columns=encoder.get_feature_names_out(['Carrosserie', 'fuel_type', 'Gearbox'])) # Call toarray() to convert to dense array
encoded_test_df = pd.DataFrame(encoded_test.toarray(), columns=encoder.get_feature_names_out(['Carrosserie', 'fuel_type', 'Gearbox'])) # Call toarray() to convert to dense array

# Reset the indices and concatenate the encoded columns with the original DataFrame
train_set = pd.concat([train_set.reset_index(drop=False), encoded_train_df], axis=1)
test_set = pd.concat([test_set.reset_index(drop=False), encoded_test_df], axis=1)

# Drop the original categorical columns if no longer needed
train_set = train_set.drop(columns=['Carrosserie', 'fuel_type', 'Gearbox'])
test_set = test_set.drop(columns=['Carrosserie', 'fuel_type', 'Gearbox'])

# Verify the resulting columns
print(train_set.columns)
print(test_set.columns)

from sklearn.preprocessing import OneHotEncoder

# List of all categorical columns to encode
categorical_columns = ['range', 'brand', 'Group', 'Country', 'CO2_class']

# Identify unique categories for each column (if needed) or let OneHotEncoder handle it automatically
encoder = OneHotEncoder(drop=None)  # Automatically detects categories from the data

# Fit the encoder on the training set and transform both train and test sets
encoded_train = encoder.fit_transform(train_set[categorical_columns])
encoded_test = encoder.transform(test_set[categorical_columns])

# Convert the encoded data back into DataFrames
encoded_train_df = pd.DataFrame(encoded_train.toarray(), columns=encoder.get_feature_names_out(categorical_columns))
encoded_test_df = pd.DataFrame(encoded_test.toarray(), columns=encoder.get_feature_names_out(categorical_columns))

# Reset the indices and concatenate the encoded columns with the original DataFrame
train_set = pd.concat([train_set.reset_index(drop=True), encoded_train_df], axis=1)
test_set = pd.concat([test_set.reset_index(drop=True), encoded_test_df], axis=1)

# Drop the original categorical columns
train_set = train_set.drop(columns=categorical_columns)
test_set = test_set.drop(columns=categorical_columns)

# Verify the resulting columns
print(train_set.columns)
print(test_set.columns)

"""# Step 26: Cleaning the Data From Column hc, nox and hcnox"""

df = cl_union_cleaned

# Define the function to fill missing values

def fill_missing_values(df, col_hc, col_nox, col_hcnox):
    # Calculation of missing values in 'hcnox'
    mask_hcnox = pd.isna(df[col_hcnox]) & pd.notna(df[col_hc]) & pd.notna(df[col_nox])
    df.loc[mask_hcnox, col_hcnox] = df.loc[mask_hcnox, col_hc] + df.loc[mask_hcnox, col_nox]

    # Calculation of missing values in 'hc'
    mask_hc = pd.isna(df[col_hc]) & pd.notna(df[col_nox]) & pd.notna(df[col_hcnox])
    df.loc[mask_hc, col_hc] = df.loc[mask_hc, col_hcnox] - df.loc[mask_hc, col_nox]

    # Calculation of missing values in 'nox'
    mask_nox = pd.isna(df[col_nox]) & pd.notna(df[col_hc]) & pd.notna(df[col_hcnox])
    df.loc[mask_nox, col_nox] = df.loc[mask_nox, col_hcnox] - df.loc[mask_nox, col_hc]

# Split the original cl_union dataset into train and test sets
train_set, test_set = train_test_split(cl_union_cleaned, test_size=0.2, random_state=42)

# Apply the function to the training set
fill_missing_values(train_set, 'hc', 'nox', 'hcnox')

# Apply the function to the test set
fill_missing_values(test_set, 'hc', 'nox', 'hcnox')

# Verify the results
print(train_set[['hc', 'nox', 'hcnox']].head())
print(test_set[['hc', 'nox', 'hcnox']].head())

# Apply the function iteratively to fill as many missing values as possible
for _ in range(5):  # Adjust the range if needed
    fill_missing_values(train_set, 'hc', 'nox', 'hcnox')

# Check for remaining missing values
print("Remaining missing values in the training set:")
print(train_set[['hc', 'nox', 'hcnox']].isnull().sum())

# Fill remaining missing values with the mean of the respective columns

train_set.fillna({'hc':train_set['hc'].mean()}, inplace=True)
train_set.fillna({'nox':train_set['nox'].mean()}, inplace=True)
train_set.fillna({'hcnox':train_set['hcnox'].mean()}, inplace=True)

#df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.
#testset
test_set.fillna({'hc':test_set['hc'].mean()}, inplace=True)
test_set.fillna({'nox':test_set['nox'].mean()}, inplace=True)
test_set.fillna({'hcnox':test_set['hcnox'].mean()}, inplace=True)

# Check the remaining missing values
print(train_set[['hc', 'nox', 'hcnox']].isnull().sum())
print(test_set[['hc', 'nox', 'hcnox']].isnull().sum())

"""# Step 27: Cleaning data from column particle through mean"""

# Calculate the mean of the 'Particles' column from the training set
mean_particles_train = train_set['Particles'].mean()

# Display the mean of the 'Particles' column from the training set
print(f"Mean of the 'Particles' column (Training set): {mean_particles_train}")

# Calculate the mean of the 'Particles' column from the test set
mean_particles_test = test_set['Particles'].mean()

# Display the mean of the 'Particles' column from the test set
print(f"Mean of the 'Particles' column (Test set): {mean_particles_test}")

#  fill missing values in 'Particles' with the mean:
train_set.fillna({'Particles': mean_particles_train}, inplace=True)
test_set.fillna({'Particles': mean_particles_train}, inplace=True)

# Verify the changes
print(train_set['Particles'].head())
print(test_set['Particles'].head())

print(train_set['Particles'].isnull().sum())
print(test_set['Particles'].isnull().sum())

"""# Step 28: Replacing missing values for numerical data with mean and for categorical data with mode"""

# Fill missing values in categorical columns
categorical_columns = ['Carrosserie', 'fuel_type', 'Gearbox', 'brand', 'Commercial_name']


for col in categorical_columns:
    train_set.fillna({col:train_set[col].mode()[0]}, inplace=True)
    test_set.fillna({col:test_set[col].mode()[0]}, inplace=True)

# Impute missing values on the training set
numerical_columns = ['Urban_consumption (l/100km)', 'Extra_urban_consumption(l/100km)',
                     'Consumption_mix(l/100km)', 'CO2', 'CO_type_I (g/km)']

for col in numerical_columns:

    train_set[col] = pd.to_numeric(train_set[col], errors='coerce')
    train_set.fillna({col:train_set[col].mean()}, inplace=True)

# Apply the same strategy to the test set (be cautious of data leakage)
for col in numerical_columns:
    test_set[col] = pd.to_numeric(test_set[col], errors='coerce')
    test_set.fillna({col:train_set[col].mean()}, inplace=True) # Use training mean

# Impute categorical missing values
categorical_columns = ['fuel_type', 'Champ_V9']

for col in categorical_columns:

    train_set.fillna({col:train_set[col].mode()[0]}, inplace=True)
    test_set.fillna({col:train_set[col].mode()[0]}, inplace=True)

# Check for missing values in the training set
missing_values_train = train_set.isnull().sum()

# Check for missing values in the test set
missing_values_test = test_set.isnull().sum()

# Display columns with missing values in the training set
print("Missing values in the training set:")
print(missing_values_train[missing_values_train > 0])

# Display columns with missing values in the test set
print("Missing values in the test set:")
print(missing_values_test[missing_values_test > 0])

# Verify the resulting columns
print(train_set.columns)
print(test_set.columns)

"""# Step 29: Drop Columns deemed unnecessary after data is clean

## NOW THERE ARE NO MISSING VALUES. WE WILL HENCE DROP COLUMNS WE DO NOT NEED TO FURTHER EXPLORE OUR TARGET VARIABLE CO2

Variables we keep


NUMERICAL

1. 'Consumption_mix (l/100km)'
2. 'hcnox'  
3. 'CO2'
4. 'power_maximal (kW)'
5.  'Administrative_power'
6. 'Empty_mass_max(kg)'
7. 'Empty_mass_min(kg)'

CATEGORICAL

1. 'Carosserie'
2.  'range'
3. 'gearbox'
4. 'brand'                       
5. 'Commercial_name'
6. 'Group'
7. 'Country'
8. 'CO2_class
"""

#  List of columns to keep
columns_to_keep = [
    'fuel_type',
    'Carrosserie',
    'range',
    'Gearbox',
    'power_maximal (kW)',
    'Administrative_power',
    'Empty_mass_max(kg)',
    'Empty_mass_min(kg)',
    'CO2',
    'hcnox',
    'hc',
    'nox',
    'brand',
    'Consumption_mix(l/100km)',  # Ensure this matches the exact name in your DataFrame
    'Commercial_name', # Ensure this matches the exact name in your DataFrame
    'Group',
    'Country',
    'CO2_class'
]

# Step 4: Cleanse the train and test sets by selecting only the desired columns
train_set_cleaned = train_set[columns_to_keep]
test_set_cleaned = test_set[columns_to_keep]

# Step 5: Display the first few rows of the cleansed train and test sets
print("Train Set:")
print(train_set_cleaned.head())

print("\nTest Set:")
print(test_set_cleaned.head())

print(train_set_cleaned.columns)
print(test_set_cleaned.columns)

"""# Step 30: Data visualization of CO2 distribution"""

import matplotlib.pyplot as plt

# Plot distribution of CO2 emissions in the train set
plt.figure(figsize=(10, 6))
plt.hist(train_set_cleaned['CO2'], bins=50, color='blue', alpha=0.7, label='Train Set')
plt.hist(test_set_cleaned['CO2'], bins=50, color='green', alpha=0.7, label='Test Set')
plt.xlabel('CO2 Emissions (g/km)')
plt.ylabel('Frequency')
plt.title('Distribution of CO2 Emissions in Train and Test Sets')
plt.legend()
plt.show()

import matplotlib.pyplot as plt

# Step 1: Sort by CO2 emissions
sorted_train_df = train_set_cleaned[['brand', 'Commercial_name', 'CO2']].sort_values(by='CO2', ascending=False)

# Step 2: Drop duplicate car models based on 'Commercial_name' to ensure uniqueness
unique_top_train_cars = sorted_train_df.drop_duplicates(subset=['Commercial_name'])

# Step 3: Select the top 20 or as many as available
top_train_cars = unique_top_train_cars.head(20).copy()  # Explicitly make a copy to avoid SettingWithCopyWarning

# Step 4: Create a label combining brand and commercial name using .loc
top_train_cars.loc[:, 'Car'] = top_train_cars['brand'] + " " + top_train_cars['Commercial_name']

# Step 5: Plot the top 20 cars with the highest CO2 emissions
plt.figure(figsize=(12, 8))
plt.barh(top_train_cars['Car'], top_train_cars['CO2'], color='skyblue')
plt.xlabel('CO2 Emissions (g/km)')
plt.ylabel('Car Model')
plt.title('Top 20 Cars with Highest CO2 Emissions - Training Set')
plt.gca().invert_yaxis()

# Highlight the car with the highest CO2 emissions
max_co2_train = top_train_cars['CO2'].max()
max_co2_car_train = top_train_cars[top_train_cars['CO2'] == max_co2_train]['Car'].values[0]
plt.barh(max_co2_car_train, max_co2_train, color='red')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Step 1: Sort by CO2 emissions
sorted_test_df = test_set_cleaned[['brand', 'Commercial_name', 'CO2']].sort_values(by='CO2', ascending=False)

# Step 2: Drop duplicate car models based on 'Commercial_name' to ensure uniqueness
unique_top_test_cars = sorted_test_df.drop_duplicates(subset=['Commercial_name'])

# Step 3: Select the top 20 or as many as available
top_test_cars = unique_top_test_cars.head(20).copy()  # Explicitly make a copy to avoid SettingWithCopyWarning

# Step 4: Create a label combining brand and commercial name using .loc
top_test_cars.loc[:, 'Car'] = top_test_cars['brand'] + " " + top_test_cars['Commercial_name']

# Step 5: Plot the top 20 cars with the highest CO2 emissions
plt.figure(figsize=(12, 8))
plt.barh(top_test_cars['Car'], top_test_cars['CO2'], color='skyblue')
plt.xlabel('CO2 Emissions (g/km)')
plt.ylabel('Car Model')
plt.title('Top 20 Cars with Highest CO2 Emissions - Test Set')
plt.gca().invert_yaxis()

# Highlight the car with the highest CO2 emissions
max_co2_test = top_test_cars['CO2'].max()
max_co2_car_test = top_test_cars[top_test_cars['CO2'] == max_co2_test]['Car'].values[0]
plt.barh(max_co2_car_test, max_co2_test, color='red')

plt.tight_layout()
plt.show()

# Manually add missing columns if they are dropped during splitting
train_set_cleaned, test_set_cleaned = train_set_cleaned.align(test_set_cleaned, join='left', axis=1, fill_value=0)

# Check if fuel_type columns were added back properly
fuel_type_columns_train = [col for col in train_set_cleaned.columns if 'fuel_type' in col]
fuel_type_columns_test = [col for col in test_set_cleaned.columns if 'fuel_type' in col]

print("Fuel type columns in train set:", fuel_type_columns_train)
print("Fuel type columns in test set:", fuel_type_columns_test)

"""# Step 31 Standardizing numeric and Encoding categorical variables"""

# Redefine categorical_columns correctly
categorical_columns = ['Carrosserie', 'range', 'Gearbox', 'brand', 'Commercial_name', 'fuel_type', 'Country', 'Group', 'CO2_class']

print(train_set_cleaned['Country'])
print(test_set_cleaned['Country'])

"""# Step 32: Encoding and transforming data"""

# Step 1: Concatenate train and test sets for consistent one-hot encoding
combined_set = pd.concat([train_set_cleaned, test_set_cleaned], axis=0)

# Step 2: Apply one-hot encoding to the combined set using pd.get_dummies
categorical_columns = ['Carrosserie', 'range', 'Gearbox', 'brand', 'Commercial_name', 'fuel_type', 'Country', 'Group', 'CO2_class']
combined_set_encoded = pd.get_dummies(combined_set, columns=categorical_columns, drop_first=False)

# Step 3: Split the combined set back into train and test sets
train_set_cleaned_encoded = combined_set_encoded.iloc[:len(train_set_cleaned), :]
test_set_cleaned_encoded = combined_set_encoded.iloc[len(train_set_cleaned):, :]

# Step 4: Verify the results
print("Training set after consistent one-hot encoding:")
print(train_set_cleaned_encoded.head())

print("\nTest set after consistent one-hot encoding:")
print(test_set_cleaned_encoded.head())

# Check for the presence of one-hot encoded fuel_type columns after encoding
fuel_type_columns_train = [col for col in train_set_cleaned.columns if 'fuel_type' in col]
fuel_type_columns_test = [col for col in test_set_cleaned.columns if 'fuel_type' in col]

print("Fuel type columns in train set after encoding:", fuel_type_columns_train)
print("Fuel type columns in test set after encoding:", fuel_type_columns_test)

print(train_set_cleaned.columns)
print(test_set_cleaned.columns)

# Convert boolean columns (True/False) to 1/0 in bulk
bool_cols_train = train_set_cleaned.select_dtypes(include=['bool']).astype(int)
bool_cols_test = test_set_cleaned.select_dtypes(include=['bool']).astype(int)

# Drop the old boolean columns from the original DataFrame
train_set_cleaned = train_set_cleaned.drop(columns=bool_cols_train.columns)
test_set_cleaned = test_set_cleaned.drop(columns=bool_cols_test.columns)

# Use pd.concat to re-attach the converted boolean columns to the DataFrame
train_set_cleaned = pd.concat([train_set_cleaned, bool_cols_train], axis=1)
test_set_cleaned = pd.concat([test_set_cleaned, bool_cols_test], axis=1)

# Verify that the columns are now 0/1 instead of True/False
print(train_set_cleaned.head())
print(test_set_cleaned.head())

# Verify that the fuel_type columns contain data (0s and 1s)
print(train_set_cleaned[fuel_type_columns_train].sum())  # Sum will tell if 0/1 values are present
print(test_set_cleaned[fuel_type_columns_test].sum())

"""** encoding is working correctly. The issue is simply that some categories are underrepresented or absent in the test set.
Low category representation (like fuel_type_GL and others) is normal after splitting and may or may not need addressing.
Optional resampling can be done to balance the categories, but it’s not strictly necessary unless we see poor model performance related to these categories.**
"""

# Step 1: List of numerical and categorical columns
numerical_columns = [
    'Consumption_mix(l/100km)',
    'hcnox',
    'CO2',
    'power_maximal (kW)',
    'Administrative_power',
    'Empty_mass_max(kg)',
    'Empty_mass_min(kg)']

from sklearn.preprocessing import StandardScaler


scaler = StandardScaler()


# Fit on training data
train_set_cleaned[numerical_columns] = scaler.fit_transform(train_set_cleaned[numerical_columns]) # Use numerical_columns instead of numeric_columns

# Transform test data
test_set_cleaned[numerical_columns] = scaler.transform(test_set_cleaned[numerical_columns]) # Use numerical_columns instead of numeric_columns

# Step 5: Verify the results
print("Training set after one-hot encoding:")
print(train_set_cleaned.head())

print("\nTest set after one-hot encoding:")
print(test_set_cleaned.head())

# Verify the mean and standard deviation after scaling
print(train_set_cleaned[numerical_columns].mean(axis=0))  # Should be close to 0
print(train_set_cleaned[numerical_columns].std(axis=0))   # Should be close to 1
print(test_set_cleaned[numerical_columns].mean(axis=0))  # Should be close to 0
print(test_set_cleaned[numerical_columns].std(axis=0))   # Should be close to 1

# Verify the resulting columns
print(train_set_cleaned['Country'])
print(test_set_cleaned['Country'])

"""# step 33 Label Encoder"""

# Separate the features and target (CO2_class)
X_train = train_set_cleaned.drop(columns=['CO2_class'])  # Drop the target column to get features
y_train = train_set_cleaned['CO2_class']  # Target (CO2 class) for training

X_test = test_set_cleaned.drop(columns=['CO2_class'])  # Drop the target column to get features
y_test = test_set_cleaned['CO2_class']  # Target (CO2 class) for testing

# Encode the target variable using LabelEncoder
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()

# Fit the encoder on the training set and transform both train and test sets
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Print the class mapping
print("Class mapping:", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))

from sklearn.preprocessing import LabelEncoder
import numpy as np

# Initialize the LabelEncoder
le = LabelEncoder()

# Step 1: Fit the LabelEncoder on the 'Model_file' column in the training set
train_set_cleaned['Model_file_encoded'] = le.fit_transform(train_set_cleaned['Model_file'])

# Step 2: Create a function to handle unseen labels in the test set
def label_encode_with_unseen(label):
    if label in le.classes_:
        return le.transform([label])[0]  # Transform if it's a known label
    else:
        return -1  # Assign -1 if it's an unseen label

# Apply the custom encoding function to the 'Model_file' column in the test set
test_set_cleaned['Model_file_encoded'] = test_set_cleaned['Model_file'].apply(label_encode_with_unseen)

# Step 3: Drop the original 'Model_file' column from both sets
train_set_cleaned = train_set_cleaned.drop(columns=['Model_file'])
test_set_cleaned = test_set_cleaned.drop(columns=['Model_file'])

# Now you can proceed with any further encoding or modeling
print("Training set after Label Encoding (Model_file):")
print(train_set_cleaned.head())

print("\nTest set after Label Encoding (Model_file):")
print(test_set_cleaned.head())

"""## MACHINE TRAINING"""

from sklearn.preprocessing import LabelEncoder

# List of categorical columns to label encode
categorical_columns = ['fuel_type', 'Carrosserie', 'range', 'Gearbox', 'brand']

# Step 1: Apply Label Encoding to the training set and handle unseen labels in the test set
for col in categorical_columns:
    le = LabelEncoder()

    # Fit the encoder on the training data
    train_set[col] = le.fit_transform(train_set[col])

    # Transform the test data, assigning -1 to unseen labels
    test_set[col] = test_set[col].apply(lambda x: le.transform([x])[0] if x in le.classes_ else -1)

# Now proceed with training as before
X_train = train_set.drop(columns=['CO2'])
y_train = train_set['CO2']
X_test = test_set.drop(columns=['CO2'])
y_test = test_set['CO2']

# Train a Random Forest model
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

# Predict on the test set
y_pred = rf_model.predict(X_test)

# Evaluate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy:.2f}")

from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import pandas as pd

# Assuming X_train and y_train are already defined
# Train a Random Forest model
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

# Get feature importance
feature_importance = rf_model.feature_importances_

# Create a DataFrame for visualization
feature_importance_df = pd.DataFrame({
    'feature': X_train.columns,
    'importance': feature_importance
})

# Sort by importance
feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)

# Display the most important features
print("Top 10 Important Features:")
print(feature_importance_df.head(10))

# Plot the feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['feature'].head(10), feature_importance_df['importance'].head(10), color='skyblue')
plt.xlabel('Importance')
plt.title('Top 10 Feature Importance')
plt.gca().invert_yaxis()  # To have the highest importance feature at the top
plt.show()

from sklearn.preprocessing import StandardScaler

# List of numerical columns that need scaling
numerical_columns = ['power_maximal (kW)', 'Administrative_power', 'Empty_mass_max(kg)',
                     'Empty_mass_min(kg)', 'CO2', 'hcnox', 'hc', 'nox', 'Consumption_mix(l/100km)']

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler on the training set and transform both training and test sets
X_train_scaled = X_train.copy()
X_test_scaled = X_test.copy()

X_train_scaled[numerical_columns] = scaler.fit_transform(X_train[numerical_columns])
X_test_scaled[numerical_columns] = scaler.transform(X_test[numerical_columns])

# Display the scaled training set
print("Training set after scaling:")
print(X_train_scaled.head())

# Now you can proceed with training your model using the scaled features
rf_model_scaled = RandomForestClassifier(random_state=42)
rf_model_scaled.fit(X_train_scaled, y_train)

# Predict on the scaled test set
y_pred_scaled = rf_model_scaled.predict(X_test_scaled)

# Evaluate accuracy
from sklearn.metrics import accuracy_score
accuracy_scaled = accuracy_score(y_test, y_pred_scaled)
print(f"Model Accuracy after Feature Scaling: {accuracy_scaled:.2f}")


# -*- coding: utf-8 -*-
"""MACHINE LEARNING copy

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-3GwyEvJBjDkXzzIjRp0LcVw467_-_bV
"""

from google.colab import drive

   drive.mount('/content/drive')

import pandas as pd

cl_union_cleaned = pd.read_csv('/content/drive/MyDrive/DATA ANALYST/cl_union_cleaned_today.csv')

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

print(cl_union_cleaned.info)

# Identify unique categories for each categorical column before splitting
Carrosserie_categories = cl_union_cleaned['Carrosserie'].unique().tolist()
fuel_type_categories = cl_union_cleaned['fuel_type'].unique().tolist()
Gearbox_categories = cl_union_cleaned['Gearbox'].unique().tolist()

print("Carrosserie Categories:", Carrosserie_categories)
print("Fuel Type Categories:", fuel_type_categories)
print("Gearbox Categories:", Gearbox_categories)

"""# Step 24: Data Separation – import sklearn library"""

from sklearn.model_selection import train_test_split

"""# Step 25: Splitting Cl Dataset into train (80) and test (20) data"""

train_set, test_set = train_test_split(cl_union_cleaned, test_size=0.2, random_state=42)

# Verify the split
print(f"Training set size: {train_set.shape[0]} rows")
print(f"Test set size: {test_set.shape[0]} rows")

from sklearn.preprocessing import OneHotEncoder

# Use the identified unique categories for OneHotEncoder
encoder = OneHotEncoder(categories=[Carrosserie_categories, fuel_type_categories, Gearbox_categories], drop=None) # Remove sparse argument

# Fit the encoder on the training set and transform both train and test sets
encoded_train = encoder.fit_transform(train_set[['Carrosserie', 'fuel_type', 'Gearbox']])
encoded_test = encoder.transform(test_set[['Carrosserie', 'fuel_type', 'Gearbox']])

# Convert the encoded data back into DataFrames
encoded_train_df = pd.DataFrame(encoded_train.toarray(), columns=encoder.get_feature_names_out(['Carrosserie', 'fuel_type', 'Gearbox'])) # Call toarray() to convert to dense array
encoded_test_df = pd.DataFrame(encoded_test.toarray(), columns=encoder.get_feature_names_out(['Carrosserie', 'fuel_type', 'Gearbox'])) # Call toarray() to convert to dense array

# Reset the indices and concatenate the encoded columns with the original DataFrame
train_set = pd.concat([train_set.reset_index(drop=False), encoded_train_df], axis=1)
test_set = pd.concat([test_set.reset_index(drop=False), encoded_test_df], axis=1)

# Drop the original categorical columns if no longer needed
train_set = train_set.drop(columns=['Carrosserie', 'fuel_type', 'Gearbox'])
test_set = test_set.drop(columns=['Carrosserie', 'fuel_type', 'Gearbox'])

# Verify the resulting columns
print(train_set.columns)
print(test_set.columns)

print(cl_union_cleaned.columns)

from sklearn.preprocessing import OneHotEncoder

# List of all categorical columns to encode
categorical_columns = ['range', 'brand', 'Group', 'Country', 'CO2_class']

# Identify unique categories for each column (if needed) or let OneHotEncoder handle it automatically
encoder = OneHotEncoder(drop=None)  # Automatically detects categories from the data

# Fit the encoder on the training set and transform both train and test sets
encoded_train = encoder.fit_transform(train_set[categorical_columns])
encoded_test = encoder.transform(test_set[categorical_columns])

# Convert the encoded data back into DataFrames
encoded_train_df = pd.DataFrame(encoded_train.toarray(), columns=encoder.get_feature_names_out(categorical_columns))
encoded_test_df = pd.DataFrame(encoded_test.toarray(), columns=encoder.get_feature_names_out(categorical_columns))

# Reset the indices and concatenate the encoded columns with the original DataFrame
train_set = pd.concat([train_set.reset_index(drop=True), encoded_train_df], axis=1)
test_set = pd.concat([test_set.reset_index(drop=True), encoded_test_df], axis=1)

# Drop the original categorical columns
train_set = train_set.drop(columns=categorical_columns)
test_set = test_set.drop(columns=categorical_columns)

# Verify the resulting columns
print(train_set.columns)
print(test_set.columns)

"""# Step 26: Cleaning the Data From Column hc, nox and hcnox"""

df = cl_union_cleaned

# Define the function to fill missing values

def fill_missing_values(df, col_hc, col_nox, col_hcnox):
    # Calculation of missing values in 'hcnox'
    mask_hcnox = pd.isna(df[col_hcnox]) & pd.notna(df[col_hc]) & pd.notna(df[col_nox])
    df.loc[mask_hcnox, col_hcnox] = df.loc[mask_hcnox, col_hc] + df.loc[mask_hcnox, col_nox]

    # Calculation of missing values in 'hc'
    mask_hc = pd.isna(df[col_hc]) & pd.notna(df[col_nox]) & pd.notna(df[col_hcnox])
    df.loc[mask_hc, col_hc] = df.loc[mask_hc, col_hcnox] - df.loc[mask_hc, col_nox]

    # Calculation of missing values in 'nox'
    mask_nox = pd.isna(df[col_nox]) & pd.notna(df[col_hc]) & pd.notna(df[col_hcnox])
    df.loc[mask_nox, col_nox] = df.loc[mask_nox, col_hcnox] - df.loc[mask_nox, col_hc]

# Split the original cl_union dataset into train and test sets
train_set, test_set = train_test_split(cl_union_cleaned, test_size=0.2, random_state=42)

# Apply the function to the training set
fill_missing_values(train_set, 'hc', 'nox', 'hcnox')

# Apply the function to the test set
fill_missing_values(test_set, 'hc', 'nox', 'hcnox')

# Verify the results
print(train_set[['hc', 'nox', 'hcnox']].head())
print(test_set[['hc', 'nox', 'hcnox']].head())

# Apply the function iteratively to fill as many missing values as possible
for _ in range(5):  # Adjust the range if needed
    fill_missing_values(train_set, 'hc', 'nox', 'hcnox')

# Check for remaining missing values
print("Remaining missing values in the training set:")
print(train_set[['hc', 'nox', 'hcnox']].isnull().sum())

# Fill remaining missing values with the mean of the respective columns

train_set.fillna({'hc':train_set['hc'].mean()}, inplace=True)
train_set.fillna({'nox':train_set['nox'].mean()}, inplace=True)
train_set.fillna({'hcnox':train_set['hcnox'].mean()}, inplace=True)

#df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.
#testset
test_set.fillna({'hc':test_set['hc'].mean()}, inplace=True)
test_set.fillna({'nox':test_set['nox'].mean()}, inplace=True)
test_set.fillna({'hcnox':test_set['hcnox'].mean()}, inplace=True)

# Check the remaining missing values
print(train_set[['hc', 'nox', 'hcnox']].isnull().sum())
print(test_set[['hc', 'nox', 'hcnox']].isnull().sum())

"""# Step 27: Cleaning data from column particle through mean"""

# Calculate the mean of the 'Particles' column from the training set
mean_particles_train = train_set['Particles'].mean()

# Display the mean of the 'Particles' column from the training set
print(f"Mean of the 'Particles' column (Training set): {mean_particles_train}")

# Calculate the mean of the 'Particles' column from the test set
mean_particles_test = test_set['Particles'].mean()

# Display the mean of the 'Particles' column from the test set
print(f"Mean of the 'Particles' column (Test set): {mean_particles_test}")

#  fill missing values in 'Particles' with the mean:
train_set.fillna({'Particles': mean_particles_train}, inplace=True)
test_set.fillna({'Particles': mean_particles_train}, inplace=True)

# Verify the changes
print(train_set['Particles'].head())
print(test_set['Particles'].head())

print(train_set['Particles'].isnull().sum())
print(test_set['Particles'].isnull().sum())

"""# Step 28: Replacing missing values for numerical data with mean and for categorical data with mode"""

# Fill missing values in categorical columns
categorical_columns = ['Carrosserie', 'fuel_type', 'Gearbox', 'brand', 'Model_file']


for col in categorical_columns:
    train_set.fillna({col:train_set[col].mode()[0]}, inplace=True)
    test_set.fillna({col:test_set[col].mode()[0]}, inplace=True)

# Impute missing values on the training set
numerical_columns = ['Urban_consumption (l/100km)', 'Extra_urban_consumption(l/100km)',
                     'Consumption_mix(l/100km)', 'CO2', 'CO_type_I (g/km)', 'Year']

for col in numerical_columns:

    train_set[col] = pd.to_numeric(train_set[col], errors='coerce')
    train_set.fillna({col:train_set[col].mean()}, inplace=True)

# Apply the same strategy to the test set (be cautious of data leakage)
for col in numerical_columns:
    test_set[col] = pd.to_numeric(test_set[col], errors='coerce')
    test_set.fillna({col:train_set[col].mean()}, inplace=True) # Use training mean

# Impute categorical missing values
categorical_columns = ['fuel_type', 'Champ_V9']

for col in categorical_columns:

    train_set.fillna({col:train_set[col].mode()[0]}, inplace=True)
    test_set.fillna({col:train_set[col].mode()[0]}, inplace=True)

# Check for missing values in the training set
missing_values_train = train_set.isnull().sum()

# Check for missing values in the test set
missing_values_test = test_set.isnull().sum()

# Display columns with missing values in the training set
print("Missing values in the training set:")
print(missing_values_train[missing_values_train > 0])

# Display columns with missing values in the test set
print("Missing values in the test set:")
print(missing_values_test[missing_values_test > 0])

# Verify the resulting columns
print(train_set.columns)
print(test_set.columns)

"""# Step 29: Drop Columns deemed unnecessary after data is clean

## NOW THERE ARE NO MISSING VALUES. WE WILL HENCE DROP COLUMNS WE DO NOT NEED TO FURTHER EXPLORE OUR TARGET VARIABLE CO2

Variables we keep


NUMERICAL

1. 'Consumption_mix (l/100km)'
2. 'hcnox'  
3. 'CO2'
4. 'power_maximal (kW)'
5.  'Administrative_power'
6. 'Empty_mass_max(kg)'
7. 'Empty_mass_min(kg)'
8. 'Year'

CATEGORICAL

1. 'Carosserie'
2.  'range'
3. 'gearbox'
4. 'brand'                       
5. 'Model_file'
6. 'Group'
7. 'Country'
8. 'CO2_class '
9. 'fuel_type'
"""

#  List of columns to keep
columns_to_keep = [
    'fuel_type',
    'Carrosserie',
    'range',
    'Gearbox',
    'power_maximal (kW)',
    'Administrative_power',
    'Empty_mass_max(kg)',
    'Empty_mass_min(kg)',
    'CO2',
    'hcnox',
    'hc',
    'nox',
    'Year',
    'brand',
    'Consumption_mix(l/100km)',  # Ensure this matches the exact name in your DataFrame
    'Model_file', # Ensure this matches the exact name in your DataFrame
    'Group',
    'Country',
    'CO2_class'
]

# Step 4: Cleanse the train and test sets by selecting only the desired columns
train_set_cleaned = train_set[columns_to_keep]
test_set_cleaned = test_set[columns_to_keep]

# Step 5: Display the first few rows of the cleansed train and test sets
print("Train Set:")
print(train_set_cleaned.head())

print("\nTest Set:")
print(test_set_cleaned.head())

print(train_set_cleaned.columns)
print(test_set_cleaned.columns)

"""# Step 30: Data visualization of CO2 distribution"""

import matplotlib.pyplot as plt

# Plot distribution of CO2 emissions in the train set
plt.figure(figsize=(10, 6))
plt.hist(train_set_cleaned['CO2'], bins=50, color='blue', alpha=0.7, label='Train Set')
plt.hist(test_set_cleaned['CO2'], bins=50, color='green', alpha=0.7, label='Test Set')
plt.xlabel('CO2 Emissions (g/km)')
plt.ylabel('Frequency')
plt.title('Distribution of CO2 Emissions in Train and Test Sets')
plt.legend()
plt.show()

import matplotlib.pyplot as plt

# Step 1: Sort by CO2 emissions
sorted_train_df = train_set_cleaned[['brand', 'Model_file', 'CO2']].sort_values(by='CO2', ascending=False)

# Step 2: Drop duplicate car models based on 'Commercial_name' to ensure uniqueness
unique_top_train_cars = sorted_train_df.drop_duplicates(subset=['Model_file'])

# Step 3: Select the top 20 or as many as available
top_train_cars = unique_top_train_cars.head(20).copy()  # Explicitly make a copy to avoid SettingWithCopyWarning

# Step 4: Create a label combining brand and commercial name using .loc
top_train_cars.loc[:, 'Car'] = top_train_cars['brand'] + " " + top_train_cars['Model_file']

# Step 5: Plot the top 20 cars with the highest CO2 emissions
plt.figure(figsize=(12, 8))
plt.barh(top_train_cars['Car'], top_train_cars['CO2'], color='skyblue')
plt.xlabel('CO2 Emissions (g/km)')
plt.ylabel('Car Model')
plt.title('Top 20 Cars with Highest CO2 Emissions - Training Set')
plt.gca().invert_yaxis()

# Highlight the car with the highest CO2 emissions
max_co2_train = top_train_cars['CO2'].max()
max_co2_car_train = top_train_cars[top_train_cars['CO2'] == max_co2_train]['Car'].values[0]
plt.barh(max_co2_car_train, max_co2_train, color='red')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Step 1: Sort by CO2 emissions
sorted_test_df = test_set_cleaned[['brand', 'Model_file', 'CO2']].sort_values(by='CO2', ascending=False)

# Step 2: Drop duplicate car models based on 'Commercial_name' to ensure uniqueness
unique_top_test_cars = sorted_test_df.drop_duplicates(subset=['Model_file'])

# Step 3: Select the top 20 or as many as available
top_test_cars = unique_top_test_cars.head(20).copy()  # Explicitly make a copy to avoid SettingWithCopyWarning

# Step 4: Create a label combining brand and commercial name using .loc
top_test_cars.loc[:, 'Car'] = top_test_cars['brand'] + " " + top_test_cars['Model_file']

# Step 5: Plot the top 20 cars with the highest CO2 emissions
plt.figure(figsize=(12, 8))
plt.barh(top_test_cars['Car'], top_test_cars['CO2'], color='skyblue')
plt.xlabel('CO2 Emissions (g/km)')
plt.ylabel('Car Model')
plt.title('Top 20 Cars with Highest CO2 Emissions - Test Set')
plt.gca().invert_yaxis()

# Highlight the car with the highest CO2 emissions
max_co2_test = top_test_cars['CO2'].max()
max_co2_car_test = top_test_cars[top_test_cars['CO2'] == max_co2_test]['Car'].values[0]
plt.barh(max_co2_car_test, max_co2_test, color='red')

plt.tight_layout()
plt.show()

# Manually add missing columns if they are dropped during splitting
train_set_cleaned, test_set_cleaned = train_set_cleaned.align(test_set_cleaned, join='left', axis=1, fill_value=0)

# Check if fuel_type columns were added back properly
fuel_type_columns_train = [col for col in train_set_cleaned.columns if 'fuel_type' in col]
fuel_type_columns_test = [col for col in test_set_cleaned.columns if 'fuel_type' in col]

print("Fuel type columns in train set:", fuel_type_columns_train)

"""# Step 31 Standardizing numeric and Encoding categorical variables"""

# Redefine categorical_columns correctly
categorical_columns = ['Carrosserie', 'range', 'Gearbox', 'brand', 'Model_file', 'fuel_type', 'Country', 'Group', 'CO2_class']

"""# Step 32: Encoding and transforming data"""

# Step 1: Make a copy of the original dataset before any encoding
train_set_cleaned_backup = train_set_cleaned.copy()
test_set_cleaned_backup = test_set_cleaned.copy()

# Step 2: Apply one-hot encoding only to the selected categorical columns
categorical_columns = ['fuel_type', 'Carrosserie', 'range', 'Gearbox', 'brand', 'Model_file', 'Country', 'Group', 'CO2_class']

# Use pd.get_dummies only on the selected categorical columns
train_set_encoded = pd.get_dummies(train_set_cleaned[categorical_columns], drop_first=False)
test_set_encoded = pd.get_dummies(test_set_cleaned[categorical_columns], drop_first=False)

# Step 3: Concatenate the encoded columns back with the original datasets
train_set_cleaned = pd.concat([train_set_cleaned_backup.drop(columns=categorical_columns), train_set_encoded], axis=1)
test_set_cleaned = pd.concat([test_set_cleaned_backup.drop(columns=categorical_columns), test_set_encoded], axis=1)

# Step 4: Align train and test sets to ensure they have the same dummy variables
train_set_cleaned, test_set_cleaned = train_set_cleaned.align(test_set_cleaned, join='left', axis=1, fill_value=0)

# Verify the dataset structure after encoding
print("Training set after one-hot encoding and reattaching columns:")
print(train_set_cleaned.head())

print("\nTest set after one-hot encoding and reattaching columns:")
print(test_set_cleaned.head())

# Check for the presence of one-hot encoded fuel_type columns after encoding
fuel_type_columns_train = [col for col in train_set_cleaned.columns if 'fuel_type' in col]
fuel_type_columns_test = [col for col in test_set_cleaned.columns if 'fuel_type' in col]

print("Fuel type columns in train set after encoding:", fuel_type_columns_train)
print("Fuel type columns in test set after encoding:", fuel_type_columns_test)

print(train_set_cleaned.columns)
print(test_set_cleaned.columns)

# Convert boolean columns (True/False) to 1/0 in bulk
bool_cols_train = train_set_cleaned.select_dtypes(include=['bool']).astype(int)
bool_cols_test = test_set_cleaned.select_dtypes(include=['bool']).astype(int)

# Drop the old boolean columns from the original DataFrame
train_set_cleaned = train_set_cleaned.drop(columns=bool_cols_train.columns)
test_set_cleaned = test_set_cleaned.drop(columns=bool_cols_test.columns)

# Use pd.concat to re-attach the converted boolean columns to the DataFrame
train_set_cleaned = pd.concat([train_set_cleaned, bool_cols_train], axis=1)
test_set_cleaned = pd.concat([test_set_cleaned, bool_cols_test], axis=1)

# Verify that the columns are now 0/1 instead of True/False
print(train_set_cleaned.head())
print(test_set_cleaned.head())

# Verify that the fuel_type columns contain data (0s and 1s)
print(train_set_cleaned[fuel_type_columns_train].sum())  # Sum will tell if 0/1 values are present
print(test_set_cleaned[fuel_type_columns_test].sum())

"""** encoding is working correctly. The issue is simply that some categories are underrepresented or absent in the test set.
Low category representation (like fuel_type_GL and others) is normal after splitting and may or may not need addressing.
Optional resampling can be done to balance the categories, but it’s not strictly necessary unless we see poor model performance related to these categories.**
"""

from sklearn.preprocessing import StandardScaler

# List of numerical columns
numerical_columns = [
    'Consumption_mix(l/100km)',
    'hcnox',
    'CO2',
    'Year',
    'power_maximal (kW)',
    'Administrative_power',
    'Empty_mass_max(kg)',
    'Empty_mass_min(kg)']

# Initialize StandardScaler
scaler = StandardScaler()

# Step 1: Fit the scaler on training data and transform it
train_scaled = scaler.fit_transform(train_set_cleaned[numerical_columns])

# Step 2: Transform the test data using the fitted scaler
test_scaled = scaler.transform(test_set_cleaned[numerical_columns])

# Step 3: Convert the scaled data back into DataFrames to keep column names
train_scaled_df = pd.DataFrame(train_scaled, columns=numerical_columns, index=train_set_cleaned.index)
test_scaled_df = pd.DataFrame(test_scaled, columns=numerical_columns, index=test_set_cleaned.index)

# Step 4: Concatenate the scaled numerical columns back with the original DataFrames
train_set_cleaned = pd.concat([train_set_cleaned.drop(columns=numerical_columns), train_scaled_df], axis=1)
test_set_cleaned = pd.concat([test_set_cleaned.drop(columns=numerical_columns), test_scaled_df], axis=1)

# Step 5: Verify the results
print("Training set after scaling and preserving categorical columns:")
print(train_set_cleaned.head())

print("\nTest set after scaling and preserving categorical columns:")
print(test_set_cleaned.head())

# Verify the mean and standard deviation after scaling
print(train_set_cleaned[numerical_columns].mean(axis=0))  # Should be close to 0
print(train_set_cleaned[numerical_columns].std(axis=0))   # Should be close to 1
print(test_set_cleaned[numerical_columns].mean(axis=0))  # Should be close to 0
print(test_set_cleaned[numerical_columns].std(axis=0))   # Should be close to 1

# Check if 'CO2_class' is present in the dataset
print("Columns in train_set_cleaned:", train_set_cleaned.columns)
print("Columns in test_set_cleaned:", test_set_cleaned.columns)

"""# step 33 Label Encoder"""

# Step 1: Identify the one-hot encoded columns for CO2_class
co2_class_columns = [col for col in train_set_cleaned.columns if col.startswith('CO2_class_')]

# Step 2: Separate features and target (using one-hot encoded CO2_class columns)
X_train = train_set_cleaned.drop(columns=co2_class_columns)  # Features
y_train = train_set_cleaned[co2_class_columns]  # One-hot encoded target

X_test = test_set_cleaned.drop(columns=co2_class_columns)  # Features
y_test = test_set_cleaned[co2_class_columns]  # One-hot encoded target

# Step 3: Verify the shape of the features and target
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)

print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

# Step 4: Proceed with your model training
# (For example, if using a model like Logistic Regression, RandomForest, etc.)

"""#Step 34 Training Models

# Logistic Regression
"""

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Step 1: Ensure that the feature names in X_train and X_test are aligned
X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)

# Step 2: Convert one-hot encoded y_train and y_test back to class labels (1D array)
y_train_single = np.argmax(y_train.values, axis=1)  # Convert one-hot to single class labels
y_test_single = np.argmax(y_test.values, axis=1)    # Convert one-hot to single class labels

# Step 3: Train the Logistic Regression model
logistic_model = LogisticRegression(max_iter=1000)
logistic_model.fit(X_train, y_train_single)  # Use single class labels (1D array)

# Step 4: Make predictions on the test set
y_pred_logistic = logistic_model.predict(X_test)

# Step 5: Evaluate the model
print("Logistic Regression Accuracy:", accuracy_score(y_test_single, y_pred_logistic))
print("Logistic Regression Classification Report:\n", classification_report(y_test_single, y_pred_logistic))

"""Accuracy:

95% accuracy means that 95% of the predictions on the test set were correct.
Precision, Recall, F1-Score:

These metrics are provided for each CO2 class (labeled 0-6).
Precision: Measures how many of the positive predictions were correct.
Recall: Measures how many of the actual positives were correctly identified.
F1-Score: The harmonic mean of precision and recall, balancing the two.
Macro Average:

The macro average gives the average precision, recall, and F1-score across all classes equally.
Weighted Average:

The weighted average accounts for the number of instances in each class, giving a better sense of overall performance when the classes are imbalanced.
Interpretation:
Class 6 (which could be CO2_class_G, based on your data) has an exceptionally high precision, recall, and F1-score (nearly perfect), suggesting the model is very confident in predicting this class.
Class 0 (potentially CO2_class_A) has a lower recall (0.88), meaning that the model might be missing some instances of this class, although it still performs well overall.

# Random Forest Classifier
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Step 1: Initialize and train the Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust n_estimators for more trees
rf_classifier.fit(X_train, y_train_single)  # Train on the training set

# Step 2: Make predictions on the test set
y_pred_rf = rf_classifier.predict(X_test)

# Step 3: Evaluate the Random Forest Classifier
print("Random Forest Classifier Accuracy:", accuracy_score(y_test_single, y_pred_rf))
print("Random Forest Classifier Classification Report:\n", classification_report(y_test_single, y_pred_rf))

"""Accuracy:

100% accuracy indicates that the Random Forest Classifier is making perfect predictions.
Precision, Recall, and F1-Score:

All classes (0 through 6) have very high precision, recall, and F1-scores, with many values approaching 1.00, indicating near-perfect performance across all classes.
The macro average and weighted average are also very high at 0.99, showing consistent performance across both large and small classes.
Comparison with Logistic Regression:
Random Forest Classifier outperformed Logistic Regression across all key metrics:
Accuracy: 100% (Random Forest) vs. 95% (Logistic Regression).
Precision, Recall, F1-score: Random Forest has more consistent and higher values across all classes.
Analysis:
Random Forest is likely handling the complexity of the data better than Logistic Regression, which assumes a linear relationship between the features and the target. Random Forest’s ability to capture non-linear patterns makes it a powerful model for this type of problem.
The near-perfect performance of Random Forest suggests it is a robust model for this dataset.

# Check Feature Importance in Random Forest:
"""

import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Access the feature importances from the trained Random Forest model
feature_importances = rf_classifier.feature_importances_

# Step 2: Create a DataFrame to hold feature names and their corresponding importance
feature_importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': feature_importances
})

# Step 3: Sort the DataFrame by importance in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Step 4: Plot the feature importances as a bar chart
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.title('Feature Importance in Random Forest Classifier')
plt.gca().invert_yaxis()  # Invert y-axis to show the most important features at the top
plt.show()

"""# Dimension Reduction"""

import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Access the feature importances from the trained Random Forest model
feature_importances = rf_classifier.feature_importances_

# Step 2: Create a DataFrame to hold feature names and their corresponding importance
feature_importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': feature_importances
})

# Step 3: Sort the DataFrame by importance in descending order and keep only the top 10 features
top_n = 10  # You can adjust this number to show more or fewer features
top_features = feature_importance_df.sort_values(by='Importance', ascending=False).head(top_n)

# Step 4: Plot the top feature importances as a bar chart
plt.figure(figsize=(10, 6))
plt.barh(top_features['Feature'], top_features['Importance'], color='skyblue')
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.title(f'Top {top_n} Important Features in Random Forest Classifier')
plt.gca().invert_yaxis()  # Invert y-axis to show the most important features at the top
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Access the feature importances from the trained Random Forest model
feature_importances = rf_classifier.feature_importances_

# Step 2: Create a DataFrame to hold feature names and their corresponding importance
feature_importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': feature_importances
})

# Step 3: Sort the DataFrame by importance in descending order and keep only the top 20 features
top_n = 20  # Adjust this to show more or fewer features
top_features = feature_importance_df.sort_values(by='Importance', ascending=False).head(top_n)

# Step 4: Plot the top feature importances as a bar chart
plt.figure(figsize=(10, 6))
plt.barh(top_features['Feature'], top_features['Importance'], color='skyblue')
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.title(f'Top {top_n} Important Features in Random Forest Classifier')
plt.gca().invert_yaxis()  # Invert y-axis to show the most important features at the top
plt.show()

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from scipy.stats import randint

# Define the hyperparameters to search over
param_distributions = {
    'n_estimators': randint(50, 300),  # Randomly choose number of trees
    'max_depth': [None, 10, 20, 30, 40],  # Randomly choose depth
    'min_samples_split': randint(2, 10),  # Randomly choose minimum samples for split
    'min_samples_leaf': randint(1, 10),   # Randomly choose minimum samples at a leaf node
    'max_features': ['auto', 'sqrt', 'log2']  # Different feature selection methods
}

# Initialize the RandomForestClassifier
rf_model = RandomForestClassifier(random_state=42)

# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=rf_model, param_distributions=param_distributions,
                                   n_iter=50, cv=3, random_state=42, n_jobs=-1, scoring='accuracy', verbose=2)

# Fit the RandomizedSearchCV model to the data
random_search.fit(X_train, y_train_single)

# Print the best parameters and the best score
print(f"Best Hyperparameters: {random_search.best_params_}")
print(f"Best Accuracy: {random_search.best_score_}")

# Use the best estimator to make predictions
best_rf_model_random = random_search.best_estimator_
y_pred_best_rf_random = best_rf_model_random.predict(X_test)

# Evaluate the tuned Random Forest Classifier from RandomizedSearchCV
from sklearn.metrics import accuracy_score, classification_report
print("Randomized Search Tuned Random Forest Accuracy:", accuracy_score(y_test_single, y_pred_best_rf_random))
print("Randomized Search Tuned Random Forest Classification Report:\n", classification_report(y_test_single, y_pred_best_rf_random))

"""# K-Nearest Neighbors (KNN)"""

from sklearn.neighbors import KNeighborsClassifier

# Initialize and train the KNN model
knn_model = KNeighborsClassifier(n_neighbors=5)  # 5 neighbors by default, can be tuned
knn_model.fit(X_train, y_train_single)

# Make predictions with KNN
y_pred_knn = knn_model.predict(X_test)

# Evaluate KNN
print("KNN Accuracy:", accuracy_score(y_test_single, y_pred_knn))
print("KNN Classification Report:\n", classification_report(y_test_single, y_pred_knn))

"""# Gradient Boosting (XGBoost)

takes too long
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

# Initialize the Decision Tree Classifier
decision_tree = DecisionTreeClassifier(random_state=42)

# Train the Decision Tree on the training data
decision_tree.fit(X_train, y_train_single)

# Make predictions on the test set
y_pred_tree = decision_tree.predict(X_test)

# Evaluate the Decision Tree model
print("Decision Tree Classifier Accuracy:", accuracy_score(y_test_single, y_pred_tree))
print("Decision Tree Classifier Classification Report:\n", classification_report(y_test_single, y_pred_tree))

from sklearn.model_selection import RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier
from scipy.stats import randint

# Define the hyperparameter space for tuning
param_distributions = {
    'max_depth': [None, 10, 20, 30, 40, 50],
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 20),
    'max_features': ['auto', 'sqrt', 'log2', None]
}

# Initialize the Decision Tree Classifier
decision_tree = DecisionTreeClassifier(random_state=42)

# Initialize RandomizedSearchCV
random_search_tree = RandomizedSearchCV(estimator=decision_tree, param_distributions=param_distributions,
                                        n_iter=50, cv=3, random_state=42, n_jobs=-1, scoring='accuracy', verbose=2)

# Fit RandomizedSearchCV to find the best parameters
random_search_tree.fit(X_train, y_train_single)

# Print the best parameters and the best score
print(f"Best Hyperparameters for Decision Tree: {random_search_tree.best_params_}")
print(f"Best Accuracy for Decision Tree: {random_search_tree.best_score_}")

# Use the best estimator to make predictions on the test set
best_tree_model = random_search_tree.best_estimator_
y_pred_best_tree = best_tree_model.predict(X_test)

# Evaluate the tuned Decision Tree Classifier
from sklearn.metrics import accuracy_score, classification_report
print("Tuned Decision Tree Classifier Accuracy:", accuracy_score(y_test_single, y_pred_best_tree))
print("Tuned Decision Tree Classifier Classification Report:\n", classification_report(y_test_single, y_pred_best_tree))

"""Here's a breakdown of the results:

### Random Forest Classifier (after RandomizedSearchCV):
- **Accuracy**: 99.7%
- **Classification Report**: Nearly perfect scores across all classes.
- **Best Hyperparameters** from RandomizedSearchCV:
  - `max_depth`: None
  - `max_features`: 'sqrt'
  - `min_samples_leaf`: 1
  - `min_samples_split`: 9
  - `n_estimators`: 296

The RandomizedSearchCV improved the model's accuracy marginally, from about 99.4% to 99.7%, which indicates that the tuning had a positive impact.

### Decision Tree Classifier (after RandomizedSearchCV):
- **Accuracy**: 100%
- **Classification Report**: Perfect scores across all classes.
- **Best Hyperparameters**:
  - `max_depth`: 20
  - `max_features`: None
  - `min_samples_leaf`: 4
  - `min_samples_split`: 9

The Decision Tree model achieved perfect accuracy after tuning, which is quite unusual. However, this could suggest **overfitting** since Decision Trees tend to memorize the data rather than generalize when given the flexibility to fit precisely.

### Interpretation and Next Steps:
1. **Random Forest**: With an accuracy of 99.7%, Random Forest is highly robust and generalizes well. This slight increase with tuning suggests a stable model.
2. **Decision Tree**: The perfect accuracy on the test set could be a sign of overfitting. Decision Trees are sensitive to overfitting, especially when there are no depth constraints.

### Handling Overfitting in Decision Tree
If you want to use the Decision Tree but avoid overfitting:
   - Further limit `max_depth` or increase `min_samples_leaf` and `min_samples_split` to make the model less sensitive.
   - Consider using **Pruning** techniques to reduce overfitting in Decision Trees.

Would you like to explore further tuning on the Decision Tree to control overfitting, or focus on analyzing the Random Forest model given its near-perfect performance with generalization?
"""

from sklearn.metrics import accuracy_score, classification_report

# Predictions on the training set
y_pred_train_tree = best_tree_model.predict(X_train)

# Accuracy and Classification Report on Training Data
print("Decision Tree Classifier Training Accuracy:", accuracy_score(y_train_single, y_pred_train_tree))
print("Decision Tree Classifier Training Classification Report:\n", classification_report(y_train_single, y_pred_train_tree))

# Compare this with the test data metrics you already have
print("\nDecision Tree Classifier Test Accuracy:", accuracy_score(y_test_single, y_pred_best_tree))
print("Decision Tree Classifier Test Classification Report:\n", classification_report(y_test_single, y_pred_best_tree))

from sklearn.model_selection import cross_val_score

# Cross-validation on Decision Tree
cv_scores_tree = cross_val_score(best_tree_model, X_train, y_train_single, cv=5)
print("Cross-Validation Scores (Decision Tree):", cv_scores_tree)
print("Average CV Accuracy (Decision Tree):", cv_scores_tree.mean())

# Cross-validation on Random Forest
cv_scores_rf = cross_val_score(best_rf_model_random, X_train, y_train_single, cv=5)
print("Cross-Validation Scores (Random Forest):", cv_scores_rf)
print("Average CV Accuracy (Random Forest):", cv_scores_rf.mean())

from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report

# Define base models
base_estimators = [
    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
    ('xgb', XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42)),
    ('logreg', LogisticRegression(max_iter=1000))
]

# Define the stacking model
stacking_model = StackingClassifier(estimators=base_estimators, final_estimator=LogisticRegression())

# Train the stacking model
stacking_model.fit(X_train, y_train_single)

# Make predictions
y_pred_stacking = stacking_model.predict(X_test)

# Evaluate the model
print("Stacking Ensemble Accuracy:", accuracy_score(y_test_single, y_pred_stacking))
print("Stacking Ensemble Classification Report:\n", classification_report(y_test_single, y_pred_stacking))

"""The output suggests that the stacking ensemble model performed extremely well across all classes, with near-perfect precision, recall, and F1-scores. This level of performance implies that the model is highly effective at distinguishing between the classes in your dataset."""

from sklearn.model_selection import cross_val_score

# Perform 5-fold cross-validation on the stacking model
cv_scores = cross_val_score(stacking_model, X_train, y_train_single, cv=5, scoring='accuracy')
print("Cross-Validation Scores:", cv_scores)
print("Average CV Accuracy:", cv_scores.mean())

"""If the training score is much higher than the cross-validation score and does not converge as more data is added, this is a strong sign of overfitting.
If both curves converge and stabilize at similar accuracy levels, the model is likely not overfitting.
"""

from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt
import numpy as np

# Generate learning curves
train_sizes, train_scores, test_scores = learning_curve(
    stacking_model, X_train, y_train_single, cv=5, scoring='accuracy', n_jobs=-1,
    train_sizes=np.linspace(0.1, 1.0, 10)
)

# Calculate mean and standard deviation for training and test scores
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)

# Plot learning curves
plt.figure()
plt.title("Learning Curves (Stacking Ensemble)")
plt.xlabel("Training Examples")
plt.ylabel("Accuracy Score")
plt.grid()

plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                 train_scores_mean + train_scores_std, alpha=0.1, color="r")
plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                 test_scores_mean + test_scores_std, alpha=0.1, color="g")
plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")
plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation score")

plt.legend(loc="best")
plt.show()

"""# DECISION TREE

"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report

# Define the Decision Tree and parameters to tune
tree_params = {
    'max_depth': [None, 10, 20, 30, 40],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['auto', 'sqrt', 'log2']
}

# Initialize the Decision Tree Classifier
decision_tree = DecisionTreeClassifier(random_state=42)

# Perform Grid Search
grid_search_tree = GridSearchCV(decision_tree, tree_params, cv=5, scoring='accuracy')
grid_search_tree.fit(X_train, y_train)

# Best parameters and model evaluation
print("Best Parameters:", grid_search_tree.best_params_)
best_tree_model = grid_search_tree.best_estimator_
y_pred_tree = best_tree_model.predict(X_test)

print("Decision Tree Accuracy:", accuracy_score(y_test, y_pred_tree))
print("Classification Report:\n", classification_report(y_test, y_pred_tree))


# -*- coding: utf-8 -*-
"""CO2_pollution_Datascience_finalproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QEa-SwVAKOfmzLYAtgaWALnoLg5wc_eZ

#**CO2 POLLUTION DATASETS DATA PREPARATION, GRAPHS AND DATA TRAIN, TEST AND SPLIT**

---



#**Part A: UPLOADING DATASETS TO GOOGLE DRIVE AND TO IMPORT LIBRARIES**

#**Step 1 : Uploading Datasets to Google Drive and import the libraries**
"""

from google.colab import drive
drive.mount('/content/drive')

"""# **Step 2 : Import Libraries**"""



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OneHotEncoder

"""
# **Step 3 : Selecting and recording the Datasets. Changing file formats for Google Colab**

"""

# **Path for CO2 Passenger Car Datasets

path_co2_2010 = '/content/drive/MyDrive/DataCO2Total/CO2_2010.csv'
path_co2_2011 = '/content/drive/MyDrive/DataCO2Total/CO2_2011.csv'
path_co2_2012 = '/content/drive/MyDrive/DataCO2Total/CO2_2012.csv'
path_co2_2013_HR = '/content/drive/MyDrive/DataCO2Total/CO2_2013_HR.csv'
path_co2_2013_NO = '/content/drive/MyDrive/DataCO2Total/CO2_2013_NO.csv'
path_co2_2013_V7 = '/content/drive/MyDrive/DataCO2Total/CO2_2013_V7.csv'
path_co2_2014_NO = '/content/drive/MyDrive/DataCO2Total/CO2_2014_NO.csv'
path_co2_2014_V9 = '/content/drive/MyDrive/DataCO2Total/CO2_2014_V9.csv'
path_co2_2015 = '/content/drive/MyDrive/DataCO2Total/CO2_2015.csv'
path_co2_2016 = '/content/drive/MyDrive/DataCO2Total/CO2_2016.csv'
path_co2_2017 = '/content/drive/MyDrive/DataCO2Total/CO2_2017.csv'
path_co2_2018 = '/content/drive/MyDrive/DataCO2Total/CO2_2018.csv'
path_co2_2019 = '/content/drive/MyDrive/CO2_2019.csv'


df_CO2_2010 = pd.read_csv(path_co2_2010,sep='\t', encoding='utf-8', on_bad_lines='skip', low_memory = False)
df_CO2_2011 = pd.read_csv(path_co2_2011,sep='\t', encoding='utf-8', on_bad_lines='skip',low_memory = False)
df_CO2_2012 = pd.read_csv(path_co2_2012,sep='\t', encoding='utf-8', on_bad_lines='skip',low_memory = False)
df_CO2_2013_HR = pd.read_csv(path_co2_2013_HR,sep='\t', encoding='utf-8', on_bad_lines='skip',low_memory = False)
df_CO2_2013_NO = pd.read_csv(path_co2_2013_NO,sep='\t', encoding='utf-8', on_bad_lines='skip',low_memory = False)
df_CO2_2013_V7 = pd.read_csv(path_co2_2013_V7,sep='\t', encoding='utf-8', on_bad_lines='skip',low_memory = False)
df_CO2_2014_NO = pd.read_csv(path_co2_2014_NO,sep='\t', encoding='utf-8', on_bad_lines='skip',low_memory = False)
df_CO2_2014_V9 = pd.read_csv(path_co2_2014_V9,sep='\t', encoding='utf-8', on_bad_lines='skip',low_memory = False)
df_CO2_2015 = pd.read_csv(path_co2_2015,sep='\t', encoding='utf-8', on_bad_lines='skip',low_memory = False)
df_CO2_2016 = pd.read_csv(path_co2_2016,sep='\t', encoding='utf-8', on_bad_lines='skip',low_memory = False)
#df_CO2_2017 = pd.read_csv(path_co2_2017, on_bad_lines='skip',encoding='utf-16le',delimiter=r"\s+", chunksize =1000, low_memory=False)
#df_CO2_2018 = pd.read_csv(path_co2_2018,encoding='latin', on_bad_lines='skip',delimiter=r"\s+", chunksize =1000,low_memory = False)
#df_CO2_2019 = pd.read_csv(path_co2_2019,sep=',',encoding='latin', on_bad_lines='skip', chunksize =1000,low_memory = False)


#Path for CL Datasets


path_cl_2012 = '/content/drive/MyDrive/DataCO2Project/CLFiles/cl_2012.xlsx' # This is the path where the files are located
path_cl_2013 = '/content/drive/MyDrive/DataCO2Project/CLFiles/cl_2013.xlsx' # This is the path where the files are located
path_cl_2014 = '/content/drive/MyDrive/DataCO2Project/CLFiles/cl_2014.xlsx' # This is the path where the files are located


df_cl_2012 = pd.read_excel(path_cl_2012)
df_cl_2013 = pd.read_excel(path_cl_2013)
df_cl_2014 = pd.read_excel(path_cl_2014)


df_cl_2013['Consommation extra-urbaine (l/100km)'].head()



"""# **Step 3b: Working with the 2017 , 2018 and 2019 Dataset**"""

'''
chunk_size = 1000000
batch_no = 1

for chunk in pd.read_csv('/content/drive/MyDrive/DataCO2Total/CO2_2017.csv', on_bad_lines='skip',encoding='utf-16le',delimiter=r"\s+", chunksize =chunk_size, low_memory=False):
  chunk.to_csv('df_CO2_2017' + str(batch_no) + '.csv', index= False)

  batch_no +=1
'''

'''
df = pd.read_csv('/content/df_CO2_20171.csv')

df.info()
'''



"""# **Step 4 : Making security copy files.**"""

#Coping CO2 Datasets security copies

df1 = df_CO2_2010.copy(deep = True)
df2 = df_CO2_2011.copy(deep = True)
df3 = df_CO2_2012.copy(deep = True)
df4 = df_CO2_2013_HR.copy(deep = True)
df5 = df_CO2_2013_NO.copy(deep = True)
df6 = df_CO2_2013_V7.copy(deep = True)
df7 = df_CO2_2014_NO.copy(deep = True)
df8 = df_CO2_2014_V9.copy(deep = True)
df9 = df_CO2_2015.copy(deep = True)
df10 = df_CO2_2016.copy(deep = True)
#df11 = df_CO2_2017 # while this dataset is only a part of the complete one . Chunk function we are going to workit later.
#df12 = df_CO2_2018 # while this dataset is only a part of the complete one . Chunk function we are going to workit later.
#df13 = df_CO2_2019 # while this dataset is only a part of the complete one . Chunk function we are going to workit later.






#Coping CL Datasets security copies

df_cl_1 = df_cl_2012.copy(deep = True)
df_cl_2 = df_cl_2013.copy(deep = True)
df_cl_3 = df_cl_2014.copy(deep = True)

"""#**Part B: DATASETS CO2 and CL PREPARATION I OVERALL PROCEDURES**

#**Step 5: Adding Year Column**
"""

df1['year'] = 2010
df2['year'] = 2011
df3['year'] = 2012
df4['year'] = 2013
df5['year'] = 2013
df6['year'] = 2013
df7['year'] = 2014
df8['year'] = 2014
df9['year'] = 2015
df10['year'] = 2016
#df11['year'] = 2017
#df12['year'] = 2018
#df13['year'] = 2019

df3.head()

df1['CO2_RegNum'] = df1['R']*df1['E (g/km)']
df2['CO2_RegNum'] = df2['r']*df2['e (g/km)']
df3['CO2_RegNum'] = df3['r']*df3['e (g/km)']
df4['CO2_RegNum'] = df4['r']*df4['e (g/km)']
df5['CO2_RegNum'] = df5['r']*df5['e (g/km)']
df6['CO2_RegNum'] = df6['r']*df6['e (g/km)']
df7['CO2_RegNum'] = df7['r']*df7['e (g/km)']
df8['CO2_RegNum'] = df8['r']*df8['e (g/km)']
df9['CO2_RegNum'] = df9['r']*df9['e (g/km)']
df10['CO2_RegNum'] = df10['r']*df10['e (g/km)']
#df11['CO2_RegNum'] = df11['r']*df11['e (g/km)']
#df12['CO2_RegNum'] = df12['r']*df12['e (g/km)']
#df13['CO2_RegNum'] = df13['r']*df13['e (g/km)']

#**To correct field names from Dataset 2010. The categorical column  names must be in uppercase. And we have to create to new fields that in this dataset does not exists.**

"""# **Step 6: To correct field names from Dataset 2010. The categorical column  names must be in uppercase. And we have to create to new fields that in this dataset does not exists.**"""

df1.insert(1,'TAN',1)
df1.insert(1,'ep (KW)',1)

df7.insert(1,'mp',1)


df1.columns = df1.columns.str.lower()
df2.columns = df2.columns.str.lower()
df3.columns = df3.columns.str.lower()
df4.columns = df4.columns.str.lower()
df5.columns = df5.columns.str.lower()
df6.columns = df6.columns.str.lower()
df7.columns = df7.columns.str.lower()
df8.columns = df8.columns.str.lower()
df9.columns = df9.columns.str.lower()
df10.columns = df10.columns.str.lower()

"""#**Step 7 :Normalizing names for every column name for every CO2 Dataset. First create the dictionaries for every table. There are two dictionaries because the 2015 Dataset has different column name for the field IT and and the field Man.Concatenate all the tables from CO2 and called it CO2 Union**"""

tables = [df1,df2,df3,df4,df5,df6,df7,df8,df9,df10]



CO2_union = pd.concat(tables,axis = 0)

CO2_union.isnull().sum().sort_values(ascending = False)*100/len(CO2_union)

#CO2_union.describe()

"""#**Step 8:To describe every column and to stablisch the number of classes for every categorical data**"""

CO2_union.describe(include = 'O')



"""#**Step9:Drop unnecessary columns in CO2 and extract the year in TAN row**"""

#CO2_union = CO2_union.drop(columns=['column_nameA', 'column_nameB'])

#CO2_union['year'] = CO2_union['tan'].iloc[0:4]
#CO2_union = CO2_union[~CO2_union.index.duplicated()]
#CO2_union[CO2_union['tan'].index.duplicated()]
CO2_union = CO2_union.drop(columns=['mms','id','ve','t','va','ve','z (wh/km)','it','er (g/km)','at2 (mm)','cn','mh'])

CO2_union = CO2_union.drop(columns=['tan'])

CO2_union.describe(include = 'O')

CO2_union.describe(include = [np.number])

"""#**Step 10: Change the names from the fields of CO2 Union**"""

Dictionary = {'ms':'Member_state', 'mp':'Manufacturer_pooling',
                   'mh':'Manufacturer_name_EU', 'man':'Manufacturer_name_om',
                   'mms':'Manufacturer_name_ms','tan':'Type_approval_number',
                   't':'Type','va':'Variant','ve':'Version','mk':'Make','cn':'Commercial_name',
                   'ct':'Category_vehicule','r':'Total_new_registration',
                   'e (g/km)':'CO2_emission','m (kg)':'Mass_in_running_kg',
                   'w (mm)':'Wheel_base_mm','at1 (mm)':'Axle_width_steering_axle_mm',
                   'at2 (mm)':'Axle_width_other_axle_mm','ft':'Fuel_type', 'fm':'Fuel_mode',
                   'ec (cm3)':'Engine_capacity_cm3','ep (kw)':'Engine_power_kw',
                   'z (wh/km)':'Electric_energy_consumption_wh_km','it':'Innovative_technology',
                   'er (g/km)':'Emision_reduction_inno_tech_g_km' }



CO2_union = CO2_union.rename(Dictionary, axis =1)

#CO2_union.isnull().sum().sort_values(ascending = False)*100/len(CO2_union)

CO2_union['Manufacturer_pooling'].replace({'nan':'NaN', 'FORD POOL':'Ford', 'JLT POOL':'Jlt', 'SUZUKI POOL':'Suzuki', 'MITSUBISHI POOL':'Mitsubishi',
                                           'DAIMLER AG':'Daimler', 'FORD-WERKE GMBH':'Ford', 'HONDA MOTOR EUROPE LTD':'Honda','SUZUKI':'Suzuki',
                                           'MITSUBISHI MOTORS':'Mitsubishi', 'VW GROUP PC':'Volkswagen', 'POOL RENAULT':'Renault','BMW GROUP':'Bmw',
                                           'FIAT GROUP AUTOMOBILES SPA':'Fiat','TOYOTA -DAIHATSU GROUP':'Toyota', 'GENERAL MOTORS':'Gm','TATA MOTORS LTD':'Tata',
                                           'JAGUAR CARS LTD':'Jaguar','LAND ROVER':'Land Rover','1':'1','na':'NaN','HYUNDAI':'Hyundai', 'KIA':'Kia',
                                           'FCA ITALY SPA':'Fca', 'RENAULT':'Renault','TATA MOTORS LTD':'Tata', 'JAGUAR CARS LTD':'Jaguar','LAND ROVER':'Land Rover',
                                           'TOYOTA-DAHAITSU GROUP':'Toyota','TATA MOTORS LTD, JAGUAR CARS LTD , LAND ROVER':'Several'},inplace = True)

display(CO2_union['Manufacturer_pooling'].unique())

"""# **Step 11: To standarize the categories other classes in the field Fuel_type**"""

CO2_union['Fuel_type'].replace({'Petrol':'Petrol', 'Diesel':'Diesel', 'E85':'E85', 'LPG':'Lpg', 'Electric':'Electric', 'ELECTRIC':'Electric',
       'PETROL-ELECTRIC':'Petrol_Electric', 'DIESEL':'Diesel', 'PETROL':'Petrol', 'NG-biomethane':'NG_Biomethane',
       'Petrol-Electric':'Petrol_Electric', 'Diesel-Electric':'Diesel_Electric', 'nan':'Nan', 'NG-BIOMETHANE':'NG_Biomethane',
       'Hydrogen':'Hydrogen', 'Biodiesel':'Bio_Diesel', 'Diesel-electric':'Diesel_Electric', 'petrol':'Petrol', 'diesel':'Diesel',
       'electric':'Electric', 'petrol-electric':'Petrol_Electric', 'hydrogen':'Hydrogen', 'diesel-electric':'Diesel_Electric',
       'DIESEL-ELECTRIC':'Diesel_Electric', 'Petrol-electric':'Petrol_Electric', 'Petrol-Gas':'Petrol_Gas', 'HYDROGEN':'Hydrogen',
       'BIODIESEL':'Bio_Diesel', 'HYBRID/PETROL/E':'Hybrid_Petrol_E', 'PETROL PHEV':'Petrol_Phev', 'PETROL/ELECTRIC':'Petrol_Electric',
                                'petrol ':'Petrol','PETROL ':'Petrol', 'OTHER':'Other','Diesel/Electric':'Diesel_Electric',
                                'Petrol/Electric':'Petrol_Electric','NG-Biomethane':'NG_Biomethane','NG_biomethane':'NG_Biomethane' },inplace = True) #To normalize the names of the types of Fuel

CO2_union['Fuel_type'].unique()

"""# **Step 12: Substitute all blank values for the mode in Fuel_type Variable.**"""

CO2_union['Fuel_type'] = CO2_union['Fuel_type'].fillna(CO2_union['Fuel_type'].mode()[0])
CO2_union['Fuel_type'].unique()

"""# **Step13: To standarize the categories other classes in the field Fuel_mode**"""

def categorize_fuel_mode(Fuel_type):

    if Fuel_type =='Petrol':

        return 'M'

    elif Fuel_type == 'Diesel':

        return 'M'

    elif Fuel_type == 'E85':
        return 'F'

    elif Fuel_type == 'Lpg':
        return 'M'

    elif Fuel_type == 'Electric':
        return 'E'

    elif  Fuel_type == 'Petrol_Electric':
        return 'P'

    elif  Fuel_type  == 'NG_Biomethane':
        return 'B'

    elif  Fuel_type  == 'Diesel_Electric':
        return 'P'

    elif  Fuel_type  == 'nan':
        return'NaN'

    elif Fuel_type == 'Hydrogen':
        return 'M'

    elif  Fuel_type == 'Bio_Diesel':
        return 'B'

    elif  Fuel_type == 'Petrol_Gas':
        return 'B'

    elif  Fuel_type =='Hybrid_Petrol_E':
        return 'P'

    elif  Fuel_type == 'Petrol_Phev':
        return 'P'

    elif Fuel_type == 'Other':
        return 'Other'

    else:
        return  'M'

CO2_union['Fuel_mode'] = CO2_union['Fuel_type'].apply(categorize_fuel_mode)
CO2_union['Fuel_mode'] = CO2_union['Fuel_mode'].fillna(CO2_union['Fuel_mode'].mode()[0]) # fill blank with mode

CO2_union['Fuel_mode'].value_counts()

"""##**Step 11: Change the values from the column members state with a more meaningfull values for the analysis.**"""

Data1 = ['AT', 'BE', 'BG', 'CY', 'CZ', 'DE', 'DK', 'EE', 'ES', 'FI', 'FR','GB', 'GR', 'HU', 'IE', 'IT', 'LT', 'LU', 'LV', 'MT','M1', 'NL','NO','UK', 'PL','PT', 'RO', 'SE', 'SI', 'HR']


Data2 = ['Austria','Belgium','Bulgary', 'Cyprus', 'Czech Republic', 'Deutschland', 'Denmark', 'Estonia', 'Spain','Finland','France','Great Britain',
         'Greece','Hungary','Ireland','Italy','Lituane','Luxembourg','Latvia','Malta', 'NaN','Netherland','Norway','United Kingdom', 'Poland','Portugal','Romania', 'Sweden','Slovenia','Croatia']

CO2_union['Member_state']= CO2_union['Member_state'].replace(Data1,Data2)

CO2_union.head()

"""#**Step 14 Normalizing names in column Maker**"""

CO2_union['Make'].replace({'BUICK':'Buick', 'CHAMONIX':'Chamonix','CHEVROLET':'Chevrolet','GM CHEVROLET':'Chevrolet','ASTON MARTIN':'Aston Martin','AUDI':'Audi','AUDI AG':'Audi',
                              'CITROEN':'Citroen','PEUGEOT':'Peugeot','PEUGEOT (F)':'Peugeot','LADA':'Lada','BMW':'Bmw','MINI':'Mini','BENTLEY':'Bentley','BUGATTI':'Bugatti',
                              'CATERHAM':'Caterham','CATERHAM CARS':'Caterham','DODGE':'Dodge', 'CHRYSLER':'Chrysler', 'JEEP':'Jeep', 'RENAULT':'Renault', 'DACIA':'Dacia', 'DAIHATSU':'Daihatsu',
                              'SUBARU':'Subaru', 'MERCEDES-BENZ':'Mercedes Benz', 'MERCEDES - BENZ':'Mercedes Benz', 'MERCEDES -BENZ':'Mercedes Benz','MERCDES BENZ':'Mercedes Benz',
                              'MERCEDES BENZ':'Mercedes Benz', 'MERCEDES-AMG':'Mercedes Benz', 'MERCEDES':'Mercedes Benz',
                              'MERCEDES  BENZ':'Mercedes Benz', 'SMART':'Smart', 'MERCEDES BENZ (D)':'Mercedes Benz', 'FERRARI':'Ferrari',
                              'DANGEL':'Dangel', 'FIAT':'Fiat', 'LANCIA':'Lancia', 'ALFA ROMEO':'Alfa Romeo', 'FORD':'Ford',
                              'THE LONDON TAXI COMPANY':'The London Taxi Co', 'CADILLAC':'Cadillac', 'HUMMER':'Hummer',
                              'GENERAL MOTORS -GMC':'General Motors', 'OPEL':'Opel', 'HONDA':'Honda', 'HYUNDAI MOTOR COMPANY':'Hyundai',
                              'HYUNDAI':'Hyundai', 'KIA':'Kia', 'IVECO':'Iveco', 'JAGUAR':'Jaguar', 'KIA MOTORS':'Kia', 'KTM':'Ktm',
                              'LAMBORGHINI':'Lamborghini', 'LAND ROVER':'Land Rover', 'LOTUS':'Lotus', 'SUZUKI':'Suzuki', 'MAHINDRA':'Mahindra',
                              'MASERATI':'Maserati', 'MAZDA':'Mazda', 'MITSUBISHI':'Mitsubishi', 'MITSUBISH':'Mitsubishi', 'MORGAN':'Morgan', 'NISSAN':'Nissan',
                              'INFINITI':'Infiniti', '2667':'2667', 'PORSCHE':'Porsche', 'PROTON':'Proton', 'QUATTRO':'Quattro', 'ROLLS-ROYCE':'Rolls Royce',
                              'SAAB':'Saab', 'SEAT':'Seat', 'SECMA':'Secma', 'SHUANGHUAN':'Shuanghuan', 'SKODA':'Skoda', 'SSANGYONG':'Ssangyong',
                              'TATA':'Tata', 'TOYOTA':'Toyota', 'LEXUS':'Lexus', 'VOLKSWAGEN, VW':'Volkswagen', 'VOLKSWAGEN':'Volkswagen',
                              'VOLKSWAGEN,  VW':'Volkswagen', 'VW':'Volkswagen', 'VOLKSAWGEN, VW':'Volkswagen', 'VOLKSWAGEN - VW':'Volkswagen',
                              'VOLKSWAGEN VW':'Volkswagen', 'VOLVO':'Volvo', 'VOLVO CAR CORPORATION':'Volvo', 'WIESMANN':'Wiesmann', 'nan':'Nan'}, inplace = True)

CO2_union['Make'].replace({'HYUNDAI MOTOR (KOR)':'Hyundai', 'ALFAROMEO':'Alfa Romeo', 'Porsche 2020000':'Porsche',
                               'BENTLEY CO':'Bentley', 'Mercedes-Benz':'Mercedes Benz', 'Mercedes-AMG':'Mercedes Benz', 'Maybach':'Maybach',
                              'MAYBACH':'Maybach', 'smart': 'Smart', 'ALFA':'Alfa Romeo', 'HYUNDAI MOTOR (SK)':'Hyundai', 'HYUNDAI (TR)':'Hyundai',
                              'HYUNDAI MOTOR (CZ)':'Hyundai', 'HYUNDAI MOTOR (IND)':'Hyundai', 'SANTANA':'Santana', 'Jaguar':'Jaguar',
                              'Kia':'Kia', 'KIA MOTOR (ROK)':'Kia', 'KIA MOTORS (ROK)':'Kia', 'KIA MOTORS (SK)':'Kia',
                              'KIA Motors (SK)':'Kia', 'LAMBORGHIN':'Lamborghini', 'COUPE':'Coupe', 'LANDROVER':'Landrover', 'Mazda':'Mazda',
                              'MITSUBISHI (J)':'Mitsubishi', 'MITSUBISHI (NL)':'Mitsubishi', 'VAZ':'Vaz', 'ROLLS ROYCE':'Rolls Royce',
                              'RANGE-ROVER':'Range Rover', 'CITROËN':'Citroen', 'MARTIN MOTORS':'Martin Motors', 'ŠKODA':'Skoda', 'ALPINA':'Alpina',
                              'Bentley':'Bentley', 'Dodge':'Dodge', 'Jeep':'Jeep', 'Chrysler':'Chrysler', 'Ferrari':'Ferrari',
                              'Ford-CNG-Technik':'Ford-Cng-Technik', 'Ford':'Ford', 'Cadillac':'Cadillac', 'Hummer':'Hummer', 'Chevrolet':'Chevrolet',
                              'Chevrolet Daewoo': 'Chevrolet Daewoo', 'Honda':'Honda', 'Hyundai':'Hyundai', 'Jaguar Cars Limited':'Jaguar',
                              'Lamborghini':'Lamborghini', 'Land Rover':'Land Rover', 'Porsche':'Porsche', 'Rolls-Royce':'Rolls Royce', 'Seat':'Seat',
                              'Skoda':'Skoda', 'Ssangyong':'Ssangyong', 'Ssangyong':'Ssangyong', 'Suzuki':'Suzuki', 'Lexus':'Lexus',
                              'VOLKSWAGEN,VW':'Volkswagen', 'Volkswagen-VW':'Volkswagen', 'Volkswagen,VW':'Volkswagen', 'VOLKSWAGEN,':'Volkswagen',
                              'Peugeot':'Peugeot', 'HYUNDAI MOTOR (ROK)':'Hyundai', 'Hyundai Motor (CZ)':'Hyundai', 'Saab':'Saab',
                              'COMARTH':'Comarth', 'FSO':'Fso', 'DAIMLER':'Daimler', 'K.T.M.':'K.t.m.', 'ROVER':'Rover', 'AUSTIN':'Austin',
                              'FIAT-EURA-MOBIL':'Fiat', 'FIAT-BURSTNER':'Fiat', 'ADRIA MOBIL':'Adria', 'ADRIA':'Adria',
                              'FIAT-CARTHAGO':'Fiat', 'FIAT-DETHLEFFS':'Fiat', 'FIAT-KABE':'Fiat', 'FORD-LMC':'Fiat',
                              'ARTEGA':'Artega', 'API CZ':'Api Cz', 'FORD-CNG-TECHNIK':'Ford', 'GMC':'Gmc', 'GUMPERT':'Gumpert',
                              'MERCEDES AMG':'Mercedes Benz', 'VAUXHALL':'Vauxhall', 'GENERAL MOTORS ITALIA':'Gmc', 'PGO':'Pgo',
                              'WESTFIELD':'Westfield', 'Mercedes.Benz':'Mercedes Benz', 'Mercedes Benz':'Mercedes Benz', 'ΑLFA ROMEO':'Alfa Romeo',
                              'SUBARU/IMPREZA':'Subaru Impresa','CHEVROLET GMC BUICK PONTIAC SUZUKI HOLDEN DAEWOO':'Chevrolet GMC...','CHEVROLET GMC HOLDEN DAEWOO':
                              'Chevrolet GMC...','CHEVROLET GMC BUICK PONTIAC SUZUKI DAEWOO HOLDEN':'Chevrolet GMC...','DAEWOO GM DAEWOO CHEVROLET':'Chevrolet GMC...',
                              'DAEWOO GM DAEWOO CHEVROLET CHEVROLET DAEWOO':'Chevrolet GMC...','Chevrolet GMC Buick Pontiac Suzuki Daewoo Holden':'Chevrolet GMC...'}, inplace = True)

CO2_union['Make'].replace({'CHEVROLET GMC BUICK PONTIAC SUZUKI DAEWOO':'Chevrolet GMC...','CHEVROLET,BUICK PONTIAC SUZUKI DAEWOO HOLTEN GMC':'Chevrolet GMC...',
                              'CHEVROLET GMC BUICK PONTIAC HOLDEN DAEWOO':'Chevrolet GMC...','CHEVROLET GMC BUICK PONTIAC HOLDEN DAEWOO SUZUKI':'Chevrolet GMC...',
                              'DAEWOO  BUICK  CHEVROLET PONTIAC SUZUKI HOLDEN GMC':'Chevrolet GMC...', '-':'Nan','JAGUAR CARS LTD':'Jaguar', 'JAGUAR(Jaguar Cars Limited)':'Jaguar',
                              'Automobili Lamborghini SpA':'Lamborgini', 'Lotus':'Lotus', 'Maserati':'Maserati', 'Mitsubishi':'Mitsubishi','NISSAN EUROPE S.A.S.':'Nissan',
                              'OPEL VAUXHALL':'Opel', 'Volvo':'Volvo', 'RAW':'Raw','CHRYSLER JEEP':'Chrysler', 'DAEWOO':'Daewoo', 'MG':'MG', 'PERODUA':'Perodua', 'DR MOTOR COMPANY':'Dr Motor Company',
                              'FIAT ':'Fiat', 'RFMOTO':'Rfmoto', 'PIAGGIO':'Piaggio', 'BERTONE':'Bertone', 'LANCIA ':'Lancia', 'GREAT WALL':'Great Wall','MORGAN MOTOR':'Morgan Motor', 'OMCI':'Omci', 'SHUANGHUAN AUTO':'Shuanghuan Auto', 'FORD (D)':'Ford', 'DONG FENG':'Dong Feng',
                              'ASTON-MARTIN':'Aston Martin', 'LADA-VAZ':'Lada', 'JAGUAR CARS':'Jaguar', 'UAZ':'Uaz', 'LTC':'Ltc', 'SAM':'Sam','MINIMAX':'Minimax', 'DR':'Dr', 'GWM':'Gwm',
                              'BORGWARD':'Borgward', 'LINCOLN':'Lincoln', 'MERCURY':'Mercury','NILSSON':'Nilsson', 'OLDSMOBILE':'Oldsmobile', 'PLYMOUTH':'Plymouth', 'PONTIAC':'Pontiac',
                              'WILLYS/WILLYS-OVERLAND':'Willys Overland', 'RANGE ROVER':'Range Rover', 'TVR':'Tvr', 'OTHER BRITISH':'Other British',
                              'CHEVROLET GMC':'Chevrolet GMC...', 'VOLGA':'Volga', 'ABARTH':'Abarth', 'ford':'Ford', 'HOLDEN':'Holden', 'CORVETTE':'Corvette',
                              'LONDON TAXIS INT.':'The London Taxis Co', 'CARBODIES':'Carbodies', 'RENAULT TRUCKS':'Renault Trucks', 'Make Unknown':'Nan',
                              'MERCEDES-BENZ AG':'Mercedes Benz', 'BINZ':'Binz', 'FORD - CNG-TECHNIK':'Ford','GENERAL MOTORS CORPORATIO':'GM', 'KIA MOTOR':'Kia', 'SSANG YONG':'Ssang Yong', 'Audi':'Audi',
                              'GREYT WALL':'Greyt Wall', 'ALFAROMEOO':'Alfa Romeo', 'CHEVROLETDAEWOOo':'Chevrolet GMC...','JAGUARCARSLIMITEDed':'Jaguar', 'LANDROVERr':'Land Rover', 'VOLKSWAGENVWVW':'Volkswagen',
                              'VOLKSWAGENVW VW':'Volkswagen', 'VOLKSWAGENVWW':'Volkswagen', 'Volkswagen':'Volkswagen', 'Volkswagen, VW':'Volkswagen',
                              'Caterham':'Caterham', 'CACIA':'Cacia', 'DAIHATSU MOTOR COMPANY':'Daihatsu', 'LANCIA,CHRYSLER':'Lancia','Ford-CNG- Technik':'Ford', 'IMPREZA':'Impreza', 'SUBARU/LEGACY':'Subaru Lagacy',
                              'DAEWOO,GMC,PONTIAC,CHEVROLET,HOLDEN,BUICK,SUZUKI':'Chevrolet GMC...','DAEWOO ήBUICK ήCHEVROLETήPONTIACήSUZUKIήHOLDENήGMC':'Chevrolet GMC...','CHEVROLETήGMCήBUICKήPONTIACήHOLDENήDAEWOO':'Chevrolet GMC...',
                              'DAEWOO ήGM DAEWOO ήCHEVROLETήCHEVROLET DAEWOO':'Chevrolet GMC...','CHEVROLETήGMCήBUICKήPONTIACήSUZUKIήDAEWOO':'Chevrolet GMC...','CHEVROLETήGMCήBUICKήPONTIACήSUZUKIήDAEWOOήHOLDEN':'Chevrolet GMC...',
                              'CHEVROLETήGMCήBUICKήPONTIACήSUZUKIήHOLDENήDAEWOO':'Chevrolet GMC...','CHEVROLETήGMCήHOLDENήDAEWOO':'Chevrolet GMC...','CHEVROLETήGMCήHOLDENήBUICKήPONTIAC':'Chevrolet GMC...','CHEVROLEΤήGMCήBUICKήPONTIACήHOLDEN':'Chevrolet GMC...', 'Range Rover':'Range Rover', 'Nissan':'Nissan',
                              'OPEL ή VAUXHALL':'Opel', 'Opel ή Vauxhall':'Opel', 'OPEL Η VAUXHALL':'Opel','VOLKS WAGEN , VW':'Volkswagen', 'VOLKS WAGEN':'Volkswagen', 'VOLKS WAGEN ,VW':'Volkswagen', 'MCLAREN':'Mclaren',
                              'MC LAREN':'Mc Laren', 'Quattro':'Quattro', 'Citroen':'Citroen', 'Lada':'Lada', 'Dacia':'Dacia', 'Smart':'Smart', 'Fiat':'Fiat','Lancia':'Lancia', 'Alfa Romeo':'Alfa Romeo', 'Toyota':'Toyota', 'Subaru':'Subaru', 'Opel':'Opel', 'Renault':'Renault',
                              'FIAT - INNOCENTI':'Fiat', 'LANCIA - AUTOBIANCHI':'Lancia', 'IVECO - FIAT':'Iveco','ISUZU':'Isuzu', 'SCANIA':'Scania', 'SSANGJONG':'Ssang Jong', 'BYD':'Byd', 'LANDWIND':'Landwind', 'MEGANE':'Megane',
                              'ALLIED VEHICLES LTD':'Allied Vehicles LTD', 'GEELY':'Geely', 'ACURA':'Acura', 'HUYNDAI':'Hyundai', 'HIUNDAI':'Hyundai','LAND':'Land', 'LTI VEHICLES':'Lti Vehicles', 'SPORTS TOURE':'Sports Toure', 'RELIANT':'Reliant', 'HAIMA 3':'Haima 3',
                              'MICRO COMPACT CAR SMART':'Smart', 'TRISTAR':'Tristar', 'TRIUMPH':'Triumph', 'INFINITY':'Infinity','RIMOR':'Rimor', 'KNAUS':'Knaus', 'WEINSBERG':'Weinsberg', 'MOBILVETTA':'Mobilvetta', 'GINETTA':'Ginetta', 'AC':'Ac',
                              'X-BOW':'X-Bow', 'SUKIDA':'Sukida', 'JENSEN':'Jensen', 'PENTA':'Penta', 'FISKER AUTOMOTIVE INC.':'Fisker Automitive','SUZUKI (J)':'Suzuki', 'ACHLEITNER':'Achleitner', 'AUTOMOBILI LAMBORGHINI':'Lamborghini',
                              'HYUNDAI MOTOR':'Hyundai', 'VOLKSWAGEN ,VW':'Volkswagen', 'BMW ALPINA':'Bmw', 'ASTONMARTIN':'Aston Martin','BAVARIA':'Bavaria', 'PILOTE':'Pilote', 'kia':'Kia', '79':'79', ' ':'Nan', 'VOLKSWAGEN-VW':'Volkswagen', 'Citroën':'Citroen'}, inplace = True)

CO2_union['Make'].replace({'OMAVALMISTATUD':'Omavalmistatud', '706/2007':'706/2007', 'QUATRO':'Quattro', 'A.U.D.I.':'Audi', 'B.M.W.':'Bmw','FISKER':'Fisker', 'JAGUAR CARS LIMITED':'Jaguar', 'JAGUAR LAND ROVER LIMITED':'Jaguar',
                              'RADICAL SPORTSCARS':'Radical Sportscars', 'Aston Martin':'Aston Martin', 'Mahindra':'Mahindra', 'McLaren':'Maclaren','Mercedes-Amg':'Mercedes Benz', 'OPEN':'Open', 'VOVLO':'Volvo', 'MANITOWE':'Manitowe', 'ROVER CARS':'Rover Cars',
                              'CHRYSLER ':'Chrysler', 'DODGE ':'Dodge', 'MINI ONE D COUNTRYMAN':'Mini One D Country Man','MERCEDES-BENZ/MBPL':'Mercedes Benz', 'DFM':'Dfm', 'DFSK':'Dfsk', 'ALU TRANS SYSTEM':'Alu Trans System', 'CAPRON':'Capron',
                              'FORD/FORD POLSKA':'Ford', 'FORD/GERMAZ':'Ford', 'IVECO/IPL':'Iveco', 'INFINITI/PGD':'Infiniti','FAW XIALI':'Faw Xiali', 'DAF':'Daf', 'ZOTYE':'Zotye', '`ODA':'Oda', 'M':'M', 'AB':'AB', 'SOMAC':'Sonac', 'TRIGANO':'Trigano',
                              'MCLOUIS':'Mclouis', 'EAGLE':'Eagle','CECOMP':'Cecomp','VOLKSWAGEN  COMM':'Volkswagen', 'HUMBER':'Humber', 'MARCOS':'Marcos','SAVIEM':'Saviem', 'AUTOMOBILES PEUGEOT':'Peugeot', 'STRATO':'Strato', 'IBC':'Ibc', 'CHALLENGER':'Challenger',
                              'JAGUAR LAND ROVER':'Land Rover','OPEL Η VAUXHALL ADAM':'Opel', 'DODGE (USA)':'Dodge', 'BMW I':'Bmw', 'DONKERVOORT':'Donkervoort','LAND ROVER LD':'Land Rover', 'MAHINDRA LTD.':'Mahindra', 'MITSUBISHI (THA)':'Mitsubishi', 'PORCHE':'Porsche',
                              'LADA - VAZ':'Lada', 'CRD CAR RESEARCH GMBH':'Crd Car Research', 'VOLKSWAGENVW':'Volkswagen',' OLKSWAGEN VW':'Volkswagen', ' OLKSWAGEN':'Volkswagen', 'ARIEL':'Ariel', 'NOBLE AUTOMOTIVE':'Noble',
                              'OTHER ITALIAN':'Other Italian', 'LANCIA - CHRYSLER':'Lancia', 'Jaguar Land RoverLimited':'Land Rover','Jaguar Land Rover Limited':'Land Rover', 'Jaguar Land Rover':'Land Rover', 'AUDI HUNGARIA':'Audi',
                              'AUTOMOBILES CITROEN':'Citroen', 'AVTOVAZ':'Avtovaz', 'BMW AG':'Bmw', 'BMW GMBH':'Bmw','DAIMLER AG':'Daimler', 'FIAT GROUP':'Fiat', 'FORD WERKE GMBH':'Ford','FUJI HEAVY INDUSTRIES':'Fuji', 'GENERAL MOTORS COMPANY':'Gm', 'GM KOREA':'Gm',
                              'HONDA CHINA':'Honda','0PEL Η VAUXHALL':'Opel', 'HONDA MOTOR CO':'Honda', 'HONDA TURKIYE':'Honda','HONDA UK':'Honda','HYUNDAI ASSAN ':'Hyundai', 'KIA SLOVAKIA':'Kia', 'MAGYAR SUZUKI':'Magyar','MITSUBISHI MOTORS CORPORATION':'Mitsubischi', 'MITSUBISHI MOTORS THAILAND':'Mitsubishi',
                              'SUZUKI MOTOR CORPORATION':'Suzuki', 'FORD - CNG - TECHNIK':'Ford', 'JAGUAR LAND ROVER LTD':'Jaguar', 'DODGE?':'Dodge','LANDROVER ':'Land Rover', 'DONKERVOORT(JD)':'Donkervoort', 'FIAT/PARTNER':'Fiat', 'FORD/IGLOOCAR':'Ford',
                              'CHRVROLET':'Chevrolet', 'OPEL/CARPOL':'Opel', 'TESLA MOTORS':'Tesla','RENAULT/CARPOL':'Renault','RENAULT/GRUAU':'Renault','VOLKSWAGEN/AMZ-KUTNO':'Volkswagen', 'VOLKSWAGEN/ZIMNY':'Volkswagen', 'VOLVO/CARRUS':'Volvo',
                              '\x8aKODA':'Skoda','SICAR':'Sicar', 'QOROS':'Qoros','DETHLEFFS':'Dethleffs','FORD (USA)':'Ford', 'PIRSO':'Pirso','GOLDSCHMITT':'Goldschmitt', 'CATERHAM CARS LTD':'Caterham Cars Ltd', 'LADA AUTOMOBILE GMBH':'Lada', 'IAV':'Iav',
                              'BIERMANN':'Biermann','Mini':'Mini', 'DACIA ':'Dacia','SYLVA':'Sylva','JAGUAR ':'Jaguar','CRD CAR RESEARCH & DEVELOPMENT GMBH & CO KG':'Crd Car Research...', 'SEIKEL':'Seikel', 'LAVERDA':'Laverda',
                              'IVECO-FORD':'Iveco','TAZZARI':'Tazzari','GENERAL MOTORS':'Gm', 'BENYE':'Benye','NOBLE':'Noble','Vauxhall':'Vauxhall','471ROEN':'Citroen', '2ITROEN':'Citroen', 'BMW,MINI':'Bmw', 'ΒMW,MINI':'Bmw', 'ΒMW':'Bmw', 'BMW1MINI':'Bmw',
                              'DACIA AUTOMOBILE SA':'Dacia','HAIDLMAIR':'Haidlmair','GM COACHWORK':'Gm Coachwork', 'MIA':'Mia','0ERCEDES-BENZ':'Mercedes Benz', '270':'270','LANCIA Η CHRYSLER':'Lancia','413T':'413T', 'CHEVROLETΗGMCΗBUICKΗP':'Chevrolet GMC...', 'CHEVROLETΗGMCΗHOLDENΗ':'Chevrolet GMC...',
                              'CHEVROLEΤΗGMCΗBUICKΗP':'Chevrolet GMC...', '1417ROLETΗDAEWOOΗHOLD':'Chevrolet GMC...','CHEVROLETΗDAEWOOΗHOLD':'Chevrolet GMC...', 'CHEVROLEΤ':'Chevrolet', 'CHEVROLEΤΗBUICKΗPONTI':'Chevrolet GMC...',
                              'ΗΥUNDAI':'Hyundai','CHANGHE':'Changhe' ,'FIESTA':'Fiesta','ΗYUNDAI':'Hyundai','FIAT/PZL-MIELEC':'Fiat' ,'JAGUAR (JAGUAR CARS L':'Jaguar','JAGUAR LAND ROVER LIM':'Jaguar', 'JAGUAR LAND ROVERLIMI':'Jaguar', 'ΚIA':'Kia', 'ΑΡZUKI':'Apzuki','3UZUKI':'Suzuki', '405UKI':'Suzuki', 'ΜΑRUTI SUZUKI':'Maruti', 'ΜΑΖDA':'Mazda', 'MITSUBISHI YEN':'Mitsubishi',
                              'ΝISSAN':'Nissan','2ISSAN':'Nissan','CITROEN, DS':'Citroen', '5ISSAN':'Nissan', 'OPEL, VAUXHALL':'Opel','OPEL  VAUXHALL':'Opel','ΟPEL, VAUXHALL':'Opel', ' VAUXHALL':'Opel', '0000, VAUXHALL':'Opel', '0PEL, VAUXHALL':'Opel',
                              'ΟPEL Η VAUXHALL':'Opel','GM':'Gm','MICRO VETT':'Micro Vett','SKODA AUTO S.A.':'Skoda', 'SUZUKI EUR':'Suzuki','ΤOYOTA':'Toyota','05YOTA':'Toyota', 'ΤΟΥΟΤA':'Toyota', 'VOLKSWAGEN AG':'Volkswagen', '1OLKSWAGEN':'Volkswagen', '2OLKSWAGEN,VW':'Volkswagen',
                              'WOLKSWAGEN,VW':'Volkswagen','3OLVO':'Volvo', 'Crd':'Crd', 'AA-NSS':'Aa Nss', 'ALFA-ROMEO':'Alfa Romeo', 'MAZDA 6':'Mazda','PORSCHE 2600000':'Porsche', 'DACIA-OKURA':'Dacia', 'BURSTNER':'Burstner', 'ZERO':'Zero',
                              'AUDI AG (D)':'Audi', 'DS':'Ds','PEUGOT':'Peugeot', 'OPEL / VAUXHALL':'Opel', 'DAICA':'Dacia','VEPB':'Vepb', 'MERCDEDES-BENZ':'Mercedes Benz', 'KIVI':'Kivi','RADICAL':'Radical', 'ELNAGH':'Elnagh', 'WESTFALIA':'Westfalia', 'GREAT WALL MOTOR':'Great Wall Motor',
                              'PGO AUTOMOBILES':'PGO Automobiles','GERMAN E CARS':'German Cars','RADICAL MOTORSPORT':'Radical Motorsport', 'Volkswagen VW':'Volkswagen','JAGUAR LAND ROVER LIMIT':'Jaguar', 'PAGANI':'Pagani', '05TROEN':'Citroen', 'CITROEN':'Citroen', 'DS':'Ds',
                              '00000EN':'Citroen','TAIQI':'Taiqui', 'VEICOLI':'Veicoli','46 ΤΟΥΟΤΑ':'Toyota','18002EN':'Citroen', '1EUGEOT':'Peugeot', '34 PEUGEOT':'Peugeot', 'PUGEOT':'Peugeot', '5721531':'Peugeot','3 BMW':'Bmw', '3INI':'Mini', '5011501I':'Peugeot', 'ΜINI':'Mini', 'ΜERCEDES-BENZ':'Mercedes Benz',
                              'ΜΕRCEDES-BENZ':'Mercedes Benz','BLUECAR':'Bluecar', '4091407':'Mercedes Benz', '1525':'Mercedes Benz', '4921481':'Mercedes Benz','CHEVROLETΗBUICKΗPONTI':'Chevrolet GMC...', 'HONDA EUR':'Honda', '1YUNDAI':'Hyundai',
                              'JANGUAR LAND ROVER LI':'Land Rover', 'ΝΑΙUKI':'Naiuki', 'MAGYAR':'Magyar', 'SUSUKI':'Suzuki', '15251523':'Suzuki','ΝΙSSAN':'Nissan', '3ISSAN':'Nissan', '5601560':'Nissan', 'OPEL,':'Opel', '472L, VAUXHALL':'Opel',
                              '2PEL, VAUXHALL':'Opel','SHENZHEN BYD AUTO':'Byd','050L':'050l','MICRO-VETT':'Micro Vett','VAUXHALL':'Opel', 'ΟPEL':'Opel', 'OPEL Η VAUXHALL AG':'Opel','5011505':'Opel', '2KODA':'Skoda', '1KODA':'Skoda', 'SUZUKI MOTOR':'Suzuki', '1UZUKI':'Suzuki', 'ΤΟYOTA':'Toyota',
                              'ΤΟΥOTA':'Toyota', 'ΟΥΣΙ A':'Toyota', '1OYOTA':'Toyota','BMWI':'Bmw','ΤΟΥΟΤΑ':'Toyota', 'WOLKSWAGEN':'Volkswagen', '4411434GEN':'Volkswagen','VOLKSVAGEN':'Volkswagen', 'VOGKSWAGEN':'Volkswagen', 'VOLKSWAGEN,VW  AG':'Volkswagen', '0000000000,VW':'Volkswagen',
                              'CITROEN-DS':'Citroen', 'MECEDES-BENZ':'Mercedes Benz','FORD-CNG TECHNIK':'Ford', 'VOKSWAGEN VW':'Volkswagen','KOENIGSEGG':'Koenigsegg', 'SSANGY0NG':'Ssangyong', 'SUBURU':'Subaru', 'OCKELBO':'Ockelbo', 'DODGE FIAT':'Dodge','MARUTI':'Maruti','CITROEN,DS':'Citroen','HYUNDAI ':'Hyundai','HOECKMAYR':'Hoeckmayr',
                              'DODGE  (USA)':'Dodge','THINK':'Think','MG':'Mg','MERCEDES-AMG GMBH':'Mercedes Benz','RENAULT TECH':'Renault', 'ÖVRIGA':'Ovriga','CITOREN':'Citroen','LITEX MOTORS':'Litex Motors', 'GIANTCO':'Giantco', 'FOCUS':'Focus', 'MAHINDRA (INDIEN)':'Mahindra','VW - VOLKSWAGEN':'Volkswagen', 'MPM MOTORS':'Mpm', 'HYUNDAI GENESIS':'Hyundai', 'JEPP':'Jeep',
                              'VOLKSWAGEN  VW':'Volkswagen', 'ΒMWI':'Bmw' ,'MITSUBISHI (THA':'Mitsubischi', 'TESLA':'Tesla','PIAGGIO (I)':'Piaggio', 'ULTIMA':'Ultima', 'B R M':'Brm'}, inplace =True)

CO2_union['Make'].unique()





"""#**Step 15 :Normalizing names for every column name for every CL Dataset. First create the dictionaries for every table. Concatenate all CL tables in CL_Union**"""

Dictionary_2012 = {'lib_mrq':'brand', 'lib_mod_doss':'Model_file', 'lib_mod':'Model_UTAC',
                   'dscom':'Commercial_name', 'cnit':'Code_National_Identification_Type',
                   'tvv':'Type_Variante_Version(TVV)','typ_cbr':'fuel_type',
                   'hybride':'Hybride','puiss_admin_98':'Administrative_power','puiss_max':'power_maximal (kW)','typ_boite_nb_rapp':'Gearbox',
                   'conso_urb':'Urban_consumption (l/100km)',
                   'conso_exurb':'Extra_urban_consumption(l/100km)','conso_mixte':'Consumption_mix(l/100km)',
                   'co2':'CO2','co_typ_1':'CO_type_I (g/km)',
                   'hc':'hc','nox':'nox',
                   'hcnox':'hcnox','ptcl':'Particles', 'masse_ordma_min':'Empty_mass_min(kg)',
                   'masse_ordma_max':'Empty_mass_max(kg)',
                   'champ_v9':'Champ_V9','date_maj':'Missing_data','Carosserie':'Carosserie',
                   'gamme':'range'}


Dictionary_2013 = {'Marque':'brand','Modèle dossier':'Model_file', 'Modèle UTAC':'Model_UTAC',
                   'Désignation commerciale':'Commercial_name', 'CNIT':'Code_National_Identification_Type',
                   ' Type Variante Version (TVV)':'Type_Variante_Version(TVV)','Carburant':'fuel_type',
                   'Hybride':'Hybride',' Puissance administrative':'Administrative_power','Puissance maximale (kW)':'power_maximal (kW)','Boîte de vitesse':'Gearbox',
                   'Consommation urbaine (l/100km)':'Urban_consumption (l/100km)',
                   'Consommation extra-urbaine (l/100km)':'Extra_urban_consumption(l/100km)','Consommation mixte (l/100km)':'Consumption_mix(l/100km)',
                   'CO2 (g/km)':'CO2','CO type I (g/km)':'CO_type_I (g/km)',
                   'hc':'hc','nox':'nox',
                   'hcnox':'hcnox','Particules (g/km)':'Particles', 'masse vide euro min (kg)':'Empty_mass_min(kg)',
                   'masse vide euro max (kg)':'Empty_mass_max(kg)',
                   'Champ V9':'Champ_V9','Date de mise à jour ':'Missing_data','Carosserie':'Carosserie',
                   'gamme':'range'}





Dictionary_2014 = {'lib_mrq':'brand','lib_mod_doss':'Model_file', 'lib_mod':'Model_UTAC',
                   'dscom':'Commercial_name', 'cnit':'Code_National_Identification_Type',
                   'tvv':'Type_Variante_Version(TVV)','cod_cbr':'fuel_type',
                   'hybride':'Hybride','puiss_admin_98':'Administrative_power','puiss_max':'power_maximal (kW)','typ_boite_nb_rapp':'Gearbox',
                   'conso_urb':'Urban_consumption (l/100km)',
                   'conso_exurb':'Extra_urban_consumption(l/100km)','conso_mixte':'Consumption_mix(l/100km)',
                   'co2':'CO2','co_typ_1':'CO_type_I (g/km)',
                   'hc':'hc','nox':'nox',
                   'hcnox':'hcnox','ptcl':'Particles', 'masse_ordma_min':'Empty_mass_min(kg)',
                   'masse_ordma_max':'Empty_mass_max(kg)',
                   'champ_v9':'Champ_V9','date_maj':'Missing_data','Carosserie':'Carosserie',
                   'gamme':'range'}


df_cl_1 = df_cl_1.rename(Dictionary_2012, axis = 1)
df_cl_2 = df_cl_2.rename(Dictionary_2013, axis = 1)
df_cl_3 = df_cl_3.rename(Dictionary_2014, axis = 1)

"""# **Part C: DATASETS CO2 and CL CONCATENATION I OVERALL PROCEDURES**

#**Step 16:Concatenate all the tables from CL and called it CL Union**#
"""

cl_union = pd.concat([df_cl_1,df_cl_2,df_cl_3], axis = 0) #Table that represents the total data from 2012 to 2014



cl_union.isnull().sum().sort_values(ascending = False)*100/len(cl_union)

cl_union.info()



"""# **Part D: DATASETS CO2 and CL DATA CLEANING**

# **Step 17: Cleaning the Data From Column HCnox CL Union**
"""

df = cl_union

def fill_missing_values(df, col_hc, col_nox, col_hcnox):
    # calculation of missing values in 'hcnox'
    mask_hcnox = pd.isna(df[col_hcnox]) & pd.notna(df[col_hc]) & pd.notna(df[col_nox])
    df.loc[mask_hcnox, col_hcnox] = df.loc[mask_hcnox, col_hc] + df.loc[mask_hcnox, col_nox]

   # calculation of missing values in 'hc'
    mask_hc = pd.isna(df[col_hc]) & pd.notna(df[col_nox]) & pd.notna(df[col_hcnox])
    df.loc[mask_hc, col_hc] = df.loc[mask_hc, col_hcnox] - df.loc[mask_hc, col_nox]

     # calculation of missing values in 'nox'
    mask_nox = pd.isna(df[col_nox]) & pd.notna(df[col_hc]) & pd.notna(df[col_hcnox])
    df.loc[mask_nox, col_nox] = df.loc[mask_nox, col_hcnox] - df.loc[mask_nox, col_hc]


fill_missing_values(cl_union, 'hc', 'nox', 'hcnox')


print(cl_union[['hc', 'nox', 'hcnox']])

mean_particles = cl_union['Particles'].mean()

# Display the mean of the 'particles' column
print(f"Mean of the 'Particles (g/km)' column: {mean_particles}")

"""#**Step18: Cleaning Missing values Particles CL Union**"""

# Replace missing values in the 'particles' column with the calculated mean
cl_union['Particles'].fillna(mean_particles, inplace=True)

# Display the DataFrame to ensure missing values are filled
print("DataFrame after filling missing values in the 'Particles (g/km)' column:")
print(cl_union[['Particles']])



"""# **Step18b: Cleaning Missing values CO2 Dataset and Filtering**"""

CO2_union.drop_duplicates(subset=None, keep="first", inplace=True) # Drop duplicates.

# 1) Fill the Nan with the Mode

CO2_union['Fuel_mode'] = CO2_union['Fuel_mode'].fillna(CO2_union['Fuel_mode'].mode()[0]) # fill blank with mode
# Here we filled the NaN with the Mode
CO2_union['Mass_in_running_kg'] = CO2_union['Mass_in_running_kg'].fillna(CO2_union['Mass_in_running_kg'].mode()[0]) # mode, mean, median,min,max
# Here we filled the NaN with the Mode
CO2_union['Wheel_base_mm'] = CO2_union['Wheel_base_mm'].fillna(CO2_union['Wheel_base_mm'].mode()[0]) # mode, mean, median,min,max
CO2_union['CO2_emission'] = CO2_union['CO2_emission'].fillna(CO2_union['CO2_emission'].median()) # mode, mean, median,min,max

# 2) Fill the Nan with the Mean

CO2_union['Mass_in_running_kg'] = CO2_union['Mass_in_running_kg'].fillna(CO2_union['Mass_in_running_kg'].mean()) # mode, mean, median,min,max
CO2_union['Wheel_base_mm'] = CO2_union['Wheel_base_mm'].fillna(CO2_union['Wheel_base_mm'].mean()) # mode, mean, median,min,max
CO2_union['Axle_width_steering_axle_mm'] = CO2_union['Axle_width_steering_axle_mm'].fillna(CO2_union['Axle_width_steering_axle_mm'].mean()) # mode, mean, median,min,max

CO2_union['Engine_capacity_cm3'] = CO2_union['Engine_capacity_cm3'].fillna(CO2_union['Engine_capacity_cm3'].mean()) # mode, mean, median,min,max
CO2_union['Engine_power_kw'] = CO2_union['Engine_power_kw'].fillna(CO2_union['Engine_power_kw'].mean()) # mode, mean, median,min,max

# 3) Filter CO2_union with are going to work with only the cars from the category M

CO2_union = CO2_union[CO2_union['Category_vehicule'] != 'M1G'] # Only category M
CO2_union = CO2_union[CO2_union['Category_vehicule'] != 'N1G']
CO2_union = CO2_union[CO2_union['Category_vehicule'] != 'N1']
CO2_union = CO2_union[CO2_union['Category_vehicule'] != 'nan']
CO2_union = CO2_union[CO2_union['Category_vehicule'] != 'M1']

CO2_union['Category_vehicule'] = CO2_union['Category_vehicule'].fillna(CO2_union['Category_vehicule'].mode()[0]) # mode, mean, median,min,max

fuel_type_col = 'Fuel_type'

CO2_union = CO2_union[CO2_union[fuel_type_col] != 'Electric']  # All the electric carburant are filtered
CO2_union= CO2_union[CO2_union[fuel_type_col] != 'Petrol_Electric']
CO2_union = CO2_union[CO2_union[fuel_type_col] != 'Diesel_Electric']
CO2_union = CO2_union[CO2_union[fuel_type_col] != 'Hybrid_Petrol_E']

# 4) All the vehicules below 75 gr/km and beyond 240 gr/km will be filtered from CO2_union

CO2_union = CO2_union[(CO2_union['CO2_emission'] >=80) & (CO2_union['CO2_emission']<=220)]

# 5) Engine Capacity Filtering

'Engine_capacity_cm3'

CO2_union = CO2_union[(CO2_union['Engine_capacity_cm3'] >=1700) & (CO2_union['Engine_capacity_cm3']<=2100)]

# 6) Mass in running KG Filtering

CO2_union = CO2_union[(CO2_union['Mass_in_running_kg'] >=900) & (CO2_union['Mass_in_running_kg']<=2000)]

sns.heatmap(pd.crosstab(CO2_union['Fuel_type'], CO2_union['Fuel_mode'],margins=True, margins_name= 'Totals',normalize = True, dropna = False),cmap="YlGnBu", annot=True, cbar=True);
plt.title('Fuel Type versus Fuel Mode for Autos -Normalize Data-');
plt.xlabel('Fuel Mode');
plt.ylabel('Fuel Type');

"""# **Step 19 : Exploratory Graphics from Datasets CL Union.**"""

filtered_df = cl_union[['brand', 'Model_file', 'Model_UTAC', 'Commercial_name', 'CO2']].dropna(subset=['CO2'])

# Sort the data by CO2 emissions in descending order and take the top 20
top_cars = filtered_df.sort_values(by='CO2', ascending=False).head(20)

# Combine brand and commercial name for better labels
top_cars['Car'] = top_cars['brand'] + " " + top_cars['Commercial_name']

# Create a horizontal bar plot of CO2 emissions by car model
plt.figure(figsize=(12, 8))
plt.barh(top_cars['Car'], top_cars['CO2'], color='skyblue')
plt.xlabel('CO2 Emissions (g/km)')
plt.ylabel('Car Model')
plt.title('Top 20 Cars with Highest CO2 Emissions')
plt.gca().invert_yaxis()  # Invert y-axis to have the highest CO2 emissions at the top

# Highlight the car with the highest CO2 emissions
max_co2 = top_cars['CO2'].max()
max_co2_car = top_cars[top_cars['CO2'] == max_co2]['Car'].values[0]
plt.barh(max_co2_car, max_co2, color='red')

# Show the plot
plt.tight_layout()
plt.show()

print("Column names and data types in the DataFrame:")
print(cl_union.dtypes)

co2_col = 'CO2'  # Replace with the correct column name for CO2
# Let's assume the correct column name for consumption mix is something like 'Consumption_mix_l_per_100km'
consumption_col ='Consumption_mix(l/100km)'  # Replace with the correct column name for consumption mix

# Convert relevant columns to numerical types if they are not already
cl_union[co2_col] = pd.to_numeric(cl_union[co2_col], errors='coerce')
cl_union[consumption_col] = pd.to_numeric(cl_union[consumption_col], errors='coerce')

# Select relevant variables and drop rows with missing values in any of the important columns
filtered_df = cl_union[['brand', 'Model_file', 'Commercial_name', co2_col, consumption_col]].dropna(subset=[co2_col, consumption_col])

# Combine brand and commercial name for better labels
filtered_df['Car'] = filtered_df['brand'] + " " + filtered_df['Commercial_name']

# Create a scatter plot
plt.figure(figsize=(12, 8))

# Scatter plot for CO2 vs. Consumption_mix
scatter = plt.scatter(
    filtered_df[co2_col],
    filtered_df[consumption_col],
    alpha=0.6,
    c=filtered_df[consumption_col],  # Color by Consumption_mix for additional dimension
    cmap='viridis'
)

plt.colorbar(scatter, label='Consumption Mix (l/100km)' )
plt.xlabel('CO2 Emissions (g/km)')
plt.ylabel('Consumption_mix (l/100km)' )
plt.title('Scatter Plot of CO2 Emissions and Consumption Mix')

# Highlight the car with the highest CO2 emissions
max_co2 = filtered_df[co2_col].max()
max_co2_car = filtered_df[filtered_df[co2_col] == max_co2]
plt.scatter(max_co2_car[co2_col], max_co2_car[consumption_col], color='red', edgecolor='black', label='Highest CO2 Emitter')

# Add legend
plt.legend(loc='upper right')

# Show the plot
plt.tight_layout()
plt.show()

# Correct column names based on the provided variables
columns = ['brand', 'Model_file', 'Model_UTAC', 'Commercial_name', 'Code_National_Identification_Type',
    'Type_Variante_Version(TVV)', 'fuel_type', 'Hybride', 'Administrative_power', 'power_maximal (kW)',
    'Gearbox', 'Urban_consumption (l/100km)', 'Extra_urban_consumption(l/100km)', 'Consumption_mix(l/100km)',
    'CO2', 'CO_type_I (g/km)', 'Hydrocarbon', 'nitrogen oxides', 'hydrocarbon and nitrogen oxides',
    'Particles (g/km)', 'Empty_mass_min(kg)', 'Empty_mass_max(kg)', 'Champ_V9', 'Missing_data',
    'Carrosserie', 'range']

# Ensure the columns exist in the DataFrame
columns = [col for col in columns if col in cl_union.columns]

# Select relevant variables and drop rows with missing CO2 values
filtered_df = cl_union[columns].dropna(subset=['CO2'])

# Ensure no duplicate columns exist in filtered_df
assert filtered_df.columns.duplicated().sum() == 0, "There are still duplicate columns in filtered_df"

filtered_df = filtered_df.reset_index() # Julio Mella code

# Step 1: Identify the car that emits the most CO2 emissions
max_co2 = filtered_df['CO2'].max()
max_co2_car = filtered_df[filtered_df['CO2'] == max_co2]

print("Car with the highest CO2 emissions:")
print(max_co2_car[['brand', 'Model_file', 'Commercial_name', 'CO2']])

# Step 2: Analyze correlations between CO2 emissions and other numerical variables
numerical_columns = ['CO2', 'Administrative_power', 'power_maximal (kW)', 'Urban_consumption (l/100km)',
    'Extra_urban_consumption(l/100km)', 'Consumption_mix(l/100km)', 'CO_type_I (g/km)',
    'Hydrocarbon', 'nitrogen oxides', 'hydrocarbon and nitrogen oxides', 'Particles (g/km)',
    'Empty_mass_min(kg)', 'Empty_mass_max(kg)', 'range']

# Ensure the numerical columns exist in the DataFrame
numerical_columns = [col for col in numerical_columns if col in filtered_df.columns]

# Step 3: Create a scatter plot matrix to visualize relationships between CO2 and other variables
sns.pairplot(filtered_df[numerical_columns])
plt.suptitle('Scatter Plot Matrix of CO2 and Related Variables', y=1.02)
plt.show()

# Create a heatmap to visualize the correlation matrix
correlation_matrix = filtered_df[numerical_columns].corr()

plt.figure(figsize=(14, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix with CO2 Emissions')
plt.show()

# Print the correlation matrix to identify the variables that correlate the most with CO2 emissions
print("Correlation matrix with CO2 emissions:")
print(correlation_matrix['CO2'].sort_values(ascending=False))

# Reset the index of filtered_df to ensure unique index values
filtered_df = filtered_df.reset_index(drop=True)

# Step 3: Create a scatter plot matrix to visualize relationships between CO2 and other variables
sns.pairplot(filtered_df[numerical_columns])
plt.suptitle('Scatter Plot Matrix of CO2 and Related Variables', y=1.02)
plt.show()

# Assuming cl_union is the DataFrame that already contains your combined data

# Print the first few rows to verify the data
print("First few rows of the DataFrame:")
print(cl_union.head())

# Print the data types of all columns to identify any datetime columns
print("Data types of columns:")
print(cl_union.dtypes)

# Identify and print duplicate columns
duplicate_columns = cl_union.columns[cl_union.columns.duplicated()].unique()
print("Duplicate columns:", duplicate_columns)

# Remove duplicate columns
cl_union = cl_union.loc[:, ~cl_union.columns.duplicated()]

# Verify columns are unique
assert cl_union.columns.duplicated().sum() == 0, "There are still duplicate columns"

# Print columns to verify duplicates are removed
print("Columns after removing duplicates:")
print(cl_union.columns)







"""#**Step 20: Create new variable ' Type_variante_Version(TVV)' that will be the joint for merge the CO2 and CL Datasets.**"""

#CO2_union['Type_Variante_Version(TVV)'] = CO2_union['Type'] + CO2_union['Variant'] + CO2_union['Version']
#CO2_union['Type_Variante_Version(TVV)'].tail()

"""#**Step 21: Merge CO2 and CL Datasets in one called CO2_CL_Union.**"""

#CO2_CL_Union = pd.merge(CO2_union,cl_union, how = 'left')

#CO2_CL_Union.info()

"""# **Step 22: Fuel Type versus Fuel Mode for Autos - Normalize Data -**"""

sns.heatmap(pd.crosstab(CO2_union['Fuel_type'], CO2_union['Fuel_mode'],margins=True, margins_name= 'Totals',normalize = True, dropna = False),cmap="YlGnBu", annot=True, cbar=True);
plt.title('Fuel Type versus Fuel Mode for Autos -Normalize Data-');
plt.xlabel('Fuel Mode');
plt.ylabel('Fuel Type');

"""#**Step 23: Relationship between the fuel type from CO2 and Cl**"""

#typefuelcomp = CO2_CL_Union[['Fuel_type','fuel_type','Fuel_mode']]

#typefuelcomp.groupby(['fuel_type','Fuel_type']).count()

"""# **Step 24: Filter and Graphics all the Autos that runs on electric engine**"""

# **Take out all the Autos that run on Electric motors**

FilterElectric = CO2_union[(CO2_union['Fuel_type'] == 'Electric') | (CO2_union['Fuel_type'] == 'Petrol_Electric')| (CO2_union['Fuel_type'] == 'Diesel_Electric')].index
CO2_union.drop(FilterElectric, inplace=True)
CO2_union['Fuel_type'].unique()

"""# **Step 25: Exploratory Graphics from the CO2 Datasets**"""

#**Variable CO2 Emission Analisis**

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns

fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (10,6))

sns.histplot(CO2_union['CO2_emission'], bins = 20, kde = True, ax =axes[0]);
axes[0].set_title('CO2 emmission per Auto');
axes[0].set_xlabel('CO2 gr/Km ');
axes[0].set_ylabel('Distribution');


sns.boxplot(CO2_union['CO2_emission'], ax= axes[1]);
axes[1].set_title('CO2 emmission per Auto');
axes[1].set_xlabel('Frecuency ');
axes[1].set_ylabel('Distribution');


plt.tight_layout();
plt.show();

#** Fuel Type Analysis Catplot**

plt.figure(figsize= (10,25));
sns.set_style('ticks');
sns.catplot(CO2_union['Fuel_type'],kind = 'bar',color = 'orange');
plt.xlim(0, 300000);
plt.title('Fuel Type for Autos');
plt.xlabel('Frecuency');
plt.ylabel('Fuel Type');
plt.show();

# ** Mass in Running Order M  Categorical Values

CO2_union_quantitative_columns = ['CO2_emision','Mass_in_running_kg','Wheel_base_mm','Axle_width_steering_axle_mm','Engine_capacity_cm3','Engine_power_kw']



# We analyze the results
fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (10,6))

sns.histplot(CO2_union['Mass_in_running_kg'], bins = 20, kde = True, ax =axes[0]);
axes[0].set_title('Mass in running Kg');
axes[0].set_xlabel('Mass Kg ');
axes[0].set_ylabel('Distribution');


sns.boxplot(CO2_union['Mass_in_running_kg'], ax= axes[1]);
axes[1].set_title('Mass in running Kg');
axes[1].set_xlabel('Mass Kg ');
axes[1].set_ylabel('Distribution');


plt.tight_layout();
plt.show();

# Wheel Base mm Analisis

CO2_union['Wheel_base_mm'].describe()

# We analyze the results
fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (10,6))

sns.histplot(CO2_union['Wheel_base_mm'], bins = 3, kde = True, ax =axes[0]);
axes[0].set_title('Wheel Base mm');
axes[0].set_xlabel('Wheel Base ');
axes[0].set_ylabel('Distribution');


sns.boxplot(CO2_union['Wheel_base_mm'], ax= axes[1]);
axes[1].set_title('Wheel Base mm');
axes[1].set_xlabel('Wheel Base');
axes[1].set_ylabel('Distribution');


plt.tight_layout();
plt.show();

CO2_union.groupby(['year', 'Member_state'])['CO2_emission'].agg(['sum', 'mean', 'count'])

"""'''

Environmental Analysis
While the target value for PCs declines from 130 g CO2/km in 2015 to 95 g CO2/km
by the end of 2020, the target for LCVs goes down from 175 g CO2/km in 2017 to 147 g CO2/km by 2020.


https://climate.ec.europa.eu/eu-action/transport/road-transport-reducing-co2-emissions-vehicles/co2-emission-performance-standards-cars-and-vans_en

With stricter CO2 emission targets in place since 2020, the average
CO2 emissions from all new passenger cars registered in Europe fell
by 27% between 2019 and 2022,


2025 to 2034

The targets that will apply from 2025 onwards are based on the WLTP (Worldwide harmonized Light vehicles Test Procedure) and were set out in Commission Implementing Decision (EU) 2023/1623:

Cars:  93,6 g CO2/km (2025-2029) and 49,5 g CO2/km (2030-2034)
Vans: 153,9 g CO2/km (2025-2029) and 90,6 g CO2/km (2030-2034)# Neuer Abschnitt

# **Step 26: Graphics**
"""

CO2_union['CO2_emission'].head()


df = CO2_union['CO2_emission']

CO2_union['Level_emission']= pd.cut(df, bins=[0,150,200], include_lowest=False,labels=['On Target', 'Not On Target']);

emissions = pd.Series(CO2_union['Level_emission']).value_counts();
Brand = pd.Series(CO2_union['Make']).value_counts();

plt.figure(figsize = (5,5));
sns.barplot( y = emissions.values, x = emissions.index, label = ' Emission Categories ');
plt.legend();
plt.xlabel(' Level of Emission');
plt.ylabel(' Frecuency');
plt.show();

# Correct column names based on the provided variables

fuel_type_col = 'Fuel_type'


columns = ['Member_state', 'Manufacturer_pooling','Manufacturer_name_EU','Manufacturer_name_om',
                   'Manufacturer_name_ms','Type_approval_number','Type','Variant','Version','Make','Commercial_name',
                   'Category_vehicule','Total_new_registration','CO2_emission','Mass_in_running_kg','Wheel_base_mm',
                   'Axle_width_steering_axle_mm','Axle_width_other_axle_mm','Fuel_type','Fuel_mode','Engine_capacity_cm3',
                   'Engine_power_kw']



# Ensure the columns exist in the DataFrame
columns = [col for col in columns if col in CO2_union.columns]

# Select relevant variables and drop rows with missing CO2 values
filtered_df = CO2_union[columns].dropna(subset=['CO2_emission'])

# Ensure no duplicate columns exist in filtered_df
assert filtered_df.columns.duplicated().sum() == 0, "There are still duplicate columns in filtered_df"

filtered_df = filtered_df.reset_index() # Julio Mella code

# Step 1: Identify the car that emits the most CO2 emissions
max_co2 = filtered_df['CO2_emission'].max()
max_co2_car = filtered_df[filtered_df['CO2_emission'] == max_co2]

print("Car with the highest CO2 emissions:")
print(max_co2_car[['Make', 'CO2_emission']])

# Step 2: Analyze correlations between CO2 emissions and other numerical variables
numerical_columns = ['CO2_emission','Mass_in_running_kg','Wheel_base_mm',
                   'Axle_width_steering_axle_mm','Engine_capacity_cm3',
                   'Engine_power_kw']

# Ensure the numerical columns exist in the DataFrame
numerical_columns = [col for col in numerical_columns if col in filtered_df.columns]

# Step 3: Create a scatter plot matrix to visualize relationships between CO2 and other variables
sns.pairplot(filtered_df[numerical_columns])
plt.suptitle('Scatter Plot Matrix of CO2 and Related Variables', y=1.02)
plt.show()

# Create a heatmap to visualize the correlation matrix
correlation_matrix = filtered_df[numerical_columns].corr()

plt.figure(figsize=(14, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix with CO2 Emissions')
plt.show()

# Print the correlation matrix to identify the variables that correlate the most with CO2 emissions
print("Correlation matrix with CO2 emissions:")
print(correlation_matrix['CO2_emission'].sort_values(ascending=False))

# Reset the index of filtered_df to ensure unique index values
filtered_df = filtered_df.reset_index(drop=True)

# Step 3: Create a scatter plot matrix to visualize relationships between CO2 and other variables
sns.pairplot(filtered_df[numerical_columns])
plt.suptitle('Scatter Plot Matrix of CO2 and Related Variables', y=1.02)
plt.show()

#Axle_width_steering_axle_mm

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns

fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (10,6))

sns.histplot(CO2_union['Axle_width_steering_axle_mm'], bins = 20, kde = True, ax =axes[0]);
axes[0].set_title('Axle_width_steering_axle_mm');
axes[0].set_xlabel('Axle_width_steering_axle_mm');
axes[0].set_ylabel('Distribution');


sns.boxplot(CO2_union['Axle_width_steering_axle_mm'], ax= axes[1]);
axes[1].set_title('Axle_width_steering_axle_mm');
axes[1].set_xlabel('Axle_width_steering_axle_mm');
axes[1].set_ylabel('Distribution');


plt.tight_layout();
plt.show();

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns

fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (10,6))

sns.histplot(CO2_union['Engine_power_kw'], bins = 20, kde = True, ax =axes[0]);
axes[0].set_title('Engine Power');
axes[0].set_xlabel('Engine power kw');
axes[0].set_ylabel('Distribution');


sns.boxplot(CO2_union['Engine_power_kw'], ax= axes[1]);
axes[1].set_title('Engine Power');
axes[1].set_xlabel('Engine Power kw');
axes[1].set_ylabel('Distribution');


plt.tight_layout();
plt.show();

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns

fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (10,6))

sns.histplot(CO2_union['Engine_capacity_cm3'], bins = 10, kde = True, ax =axes[0]);
axes[0].set_title('Engine capacity cm3');
axes[0].set_xlabel('Engine Capacity cm3');
axes[0].set_ylabel('Distribution');


sns.boxplot(CO2_union['Engine_capacity_cm3'], ax= axes[1]);
axes[1].set_title('Engine capacity cm3');
axes[1].set_xlabel('Engine capacity cm3');
axes[1].set_ylabel('Distribution');


plt.tight_layout();
plt.show();

#Top Polluters from the CO2 Dataset

filtered_df2 = CO2_union[['Member_state','Make','CO2_emission','Manufacturer_name_om']].dropna(subset=['CO2_emission'])

# Sort the data by CO2 emissions in descending order and take the top 20
top_cars = filtered_df2.sort_values(by='CO2_emission', ascending=False).head(20)

# Combine brand and commercial name for better labels

top_cars['Make'] = top_cars['Member_state'] + " " + top_cars['Manufacturer_name_om']


# Create a horizontal bar plot of CO2 emissions by car model
plt.figure(figsize=(12, 8))
plt.barh(top_cars['Make'], top_cars['CO2_emission'], color='skyblue')
plt.xlabel('CO2 Emissions (g/km)')
plt.ylabel('Car Model')
plt.title('Top 20 Cars with Highest CO2 Emissions')
plt.gca().invert_yaxis()  # Invert y-axis to have the highest CO2 emissions at the top

# Highlight the car with the highest CO2 emissions
max_co2 = top_cars['CO2_emission'].max()
max_co2_car = top_cars[top_cars['CO2_emission'] == max_co2]['Make'].values[0]
plt.barh(max_co2_car, max_co2, color='red')

# Show the plot
plt.tight_layout()
plt.show()

#CO2_union['CO2_emission'] = (CO2_union['CO2_emission']-CO2_union['CO2_emission'].min())/(CO2_union['CO2_emission'].max()-CO2_union['CO2_emission'].min())
#CO2_union['Engine_capacity_cm3'] = (CO2_union['Engine_capacity_cm3']-CO2_union['Engine_capacity_cm3'].min())/(CO2_union['Engine_capacity_cm3'].max()-CO2_union['Engine_capacity_cm3'].min())

scat = plt.scatter(CO2_union['CO2_emission'], CO2_union['Mass_in_running_kg'],
                   c = CO2_union['Engine_capacity_cm3'], cmap=plt.cm.rainbow,
                   s = CO2_union['Wheel_base_mm'], alpha = 0.4);

# Color scheme

plt.colorbar(scat, label = 'Mass in running Kg');
plt.rcParams['figure.dpi']= 120

plt.xlabel('CO2 emission');
plt.ylabel('Engine capacity cm3');

plt.legend(['Size : Wheel Base','Color : Mass in running KG'], loc = 'upper left')

plt.title('CO2_emission / Engine_capacity_cm3');
plt.grid(True);



"""#**Step 27:  Fill the CO2 rows with NaN**"""



CO2_union['CO2_emission'] = CO2_union['CO2_emission'].fillna(CO2_union['CO2_emission'].mode()) # mode, mean, median,min,max

"""#**Step 28: Emission Trend Analisis.**

# **Step 28: Applying pd.qcut to separate de CO2_emission data into Quartiles.**
"""



CO2_union['CO2_emission_cut'] = pd.qcut(CO2_union['CO2_emission'],3)


CO2_union.groupby(['CO2_emission_cut'], as_index=False)['CO2_emission'].mean()


# Other way to cut.
'''
test_gre = pd.cut(x = df['gre'],
                  bins = [200, 450, 550, 620, 800],
                  labels = ['bad', 'average', 'average +', 'good'])

pd.crosstab(df['admit'], test_gre)

# The more the GRE score is high, the more the candidate **seems** to have chances to be admitted.
# Indeed, the dominant admitted classes are "bon" and "moyen+"
test_gre
'''
'''
# We merge the two DataFrames using the merge method.
df = df.merge(test_gre, left_index=True, right_index=True)

# We delete the non-discretized column.
df = df.drop(columns='gre_x', axis = 1)

# We rename the discretized column.
df = df.rename({'gre_y' : 'gre'}, axis=1)

'''

# Creating the intervals, while we are going to stablish pollution levels.

CO2_union.loc[(CO2_union['CO2_emission']<=136),'CO2_emission']=1
CO2_union.loc[(CO2_union['CO2_emission']>136) & (CO2_union['CO2_emission']<=160) ,'CO2_emission']=2
CO2_union.loc[(CO2_union['CO2_emission']>160) & (CO2_union['CO2_emission']<=240) ,'CO2_emission']=3
CO2_union.loc[(CO2_union['CO2_emission']>240),'CO2_emission']=4

CO2_union['CO2_emission'].unique()

CO2_union['CO2_emission'] = CO2_union['CO2_emission'].fillna(CO2_union['CO2_emission'].median()) # mode, mean, median,min,max

CO2_union['CO2_emission'].unique()

CO2_union['Pol_Cat1'] = CO2_union.apply(lambda _: '', axis=1)
CO2_union['Pol_Cat2'] = CO2_union.apply(lambda _: '', axis=1)
CO2_union['Pol_Cat3'] = CO2_union.apply(lambda _: '', axis=1)
CO2_union['Pol_Cat4'] = CO2_union.apply(lambda _: '', axis=1)

CO2_union['Pol_Cat1'] = np.where(CO2_union['CO2_emission'] == 1, 1, CO2_union['Pol_Cat1'])
CO2_union['Pol_Cat2'] = np.where(CO2_union['CO2_emission'] == 2, 2, CO2_union['Pol_Cat2'])
CO2_union['Pol_Cat3'] = np.where(CO2_union['CO2_emission'] == 3, 3, CO2_union['Pol_Cat3'])
CO2_union['Pol_Cat4'] = np.where(CO2_union['CO2_emission'] == 4, 4, CO2_union['Pol_Cat4'])

Fields_Pol = ['Pol_Cat1','Pol_Cat2','Pol_Cat3','Pol_Cat4']

#CO2_union.groupby(['Member_state'], as_index=False)[Fields_Pol].sum()

CO2_union[Fields_Pol]

#(CO2_union.groupby(['Member_state','CO2_emission','year'], as_index=False).sum().groupby('year')['CO2_emission'].sum())


#CO2_union.groupby(['year', 'Member_state'])['CO2_emission'].agg(['sum', 'mean', 'count'])


#CO2_union.groupby(['year', 'Member_state'])['Total_new_registration'].sum()


#grouped = CO2_union.groupby(['year', 'Member_state','CO2_emission'])['CO2_emission'].agg(['sum']).reset_index()

member_polution = CO2_union.groupby(['Member_state'],as_index=False)['CO2_emission'].agg(['sum'])


member_polution.sort_values('sum',ascending=False).head(5)



auto_pollution = CO2_union.groupby(['Member_state','Make'], as_index=False)['CO2_emission'].sum()

auto_pollution.sort_values('CO2_emission',ascending=False)

pollution_levels = CO2_union.groupby(['Make','Pol_Cat1','Pol_Cat2','Pol_Cat3','Pol_Cat4'], as_index=False)['Pol_Cat4'].count()

pollution_levels.sort_values('Pol_Cat4',ascending=False)

pollution_levels_cars = CO2_union.groupby(['Make','Pol_Cat1','Pol_Cat2','Pol_Cat3','Pol_Cat4'], as_index=False)['CO2_emission'].sum()

pollution_levels_cars.sort_values('CO2_emission',ascending=False)



#CO2_union.describe(include = np.number)
CO2_union.info()

CO2_union['year'] = pd.to_datetime(CO2_union['year'])

# Count the number of cars produced each year
production_counts = CO2_union.groupby('year').size()

print("Number of cars produced each year:")
print(production_counts)

# Plot the production counts
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(production_counts.index, production_counts.values, marker='o', linestyle='-', color='blue')
plt.xlabel('Year')
plt.ylabel('Number of Cars Produced')
plt.title('Number of Cars Produced Each Year (2010-2016)')
plt.grid(True)
plt.show()

# Group by Year and Model to get average CO2 emissions per model each year
yearly_emissions_model = CO2_union.groupby(['year', 'cn'])['e (g/km)'].mean().reset_index()

# Group by Year and Carrosserie (vehicle type) to get average CO2 emissions per type each year
yearly_emissions_type = CO2_union.groupby(['year', 'mk'])['e (g/km)'].mean().reset_index()

print(yearly_emissions_model.head())
print(yearly_emissions_type.head())

import matplotlib.pyplot as plt

# Get the top 5 models based on the combined data
top_models = CO2_union['cn'].value_counts().head(5).index.tolist()
print("Top models selected:", top_models)

# Filter the DataFrame for these top models
filtered_yearly_emissions = yearly_emissions_model[yearly_emissions_model['Model_file'].isin(top_models)]

# Plot the data
plt.figure(figsize=(14, 8))

for model in top_models:
    model_data = filtered_yearly_emissions[filtered_yearly_emissions['Model_file'] == model]
    plt.plot(model_data['Year'], model_data['CO2'], marker='o', label=model)

plt.xlabel('Year')
plt.ylabel('Average CO2 Emissions (g/km)')
plt.title('CO2 Emission Trends Over Time by Vehicle Model')
plt.legend(title='Vehicle Model')
plt.grid(True)
plt.show()






