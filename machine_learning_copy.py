# -*- coding: utf-8 -*-
"""MACHINE LEARNING copy

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-3GwyEvJBjDkXzzIjRp0LcVw467_-_bV
"""

from google.colab import drive

   drive.mount('/content/drive')

import pandas as pd

cl_union_cleaned = pd.read_csv('/content/drive/MyDrive/DATA ANALYST/cl_union_cleaned_today.csv')

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

print(cl_union_cleaned.info)

# Identify unique categories for each categorical column before splitting
Carrosserie_categories = cl_union_cleaned['Carrosserie'].unique().tolist()
fuel_type_categories = cl_union_cleaned['fuel_type'].unique().tolist()
Gearbox_categories = cl_union_cleaned['Gearbox'].unique().tolist()

print("Carrosserie Categories:", Carrosserie_categories)
print("Fuel Type Categories:", fuel_type_categories)
print("Gearbox Categories:", Gearbox_categories)

"""# Step 24: Data Separation – import sklearn library"""

from sklearn.model_selection import train_test_split

"""# Step 25: Splitting Cl Dataset into train (80) and test (20) data"""

train_set, test_set = train_test_split(cl_union_cleaned, test_size=0.2, random_state=42)

# Verify the split
print(f"Training set size: {train_set.shape[0]} rows")
print(f"Test set size: {test_set.shape[0]} rows")

from sklearn.preprocessing import OneHotEncoder

# Use the identified unique categories for OneHotEncoder
encoder = OneHotEncoder(categories=[Carrosserie_categories, fuel_type_categories, Gearbox_categories], drop=None) # Remove sparse argument

# Fit the encoder on the training set and transform both train and test sets
encoded_train = encoder.fit_transform(train_set[['Carrosserie', 'fuel_type', 'Gearbox']])
encoded_test = encoder.transform(test_set[['Carrosserie', 'fuel_type', 'Gearbox']])

# Convert the encoded data back into DataFrames
encoded_train_df = pd.DataFrame(encoded_train.toarray(), columns=encoder.get_feature_names_out(['Carrosserie', 'fuel_type', 'Gearbox'])) # Call toarray() to convert to dense array
encoded_test_df = pd.DataFrame(encoded_test.toarray(), columns=encoder.get_feature_names_out(['Carrosserie', 'fuel_type', 'Gearbox'])) # Call toarray() to convert to dense array

# Reset the indices and concatenate the encoded columns with the original DataFrame
train_set = pd.concat([train_set.reset_index(drop=False), encoded_train_df], axis=1)
test_set = pd.concat([test_set.reset_index(drop=False), encoded_test_df], axis=1)

# Drop the original categorical columns if no longer needed
train_set = train_set.drop(columns=['Carrosserie', 'fuel_type', 'Gearbox'])
test_set = test_set.drop(columns=['Carrosserie', 'fuel_type', 'Gearbox'])

# Verify the resulting columns
print(train_set.columns)
print(test_set.columns)

print(cl_union_cleaned.columns)

from sklearn.preprocessing import OneHotEncoder

# List of all categorical columns to encode
categorical_columns = ['range', 'brand', 'Group', 'Country', 'CO2_class']

# Identify unique categories for each column (if needed) or let OneHotEncoder handle it automatically
encoder = OneHotEncoder(drop=None)  # Automatically detects categories from the data

# Fit the encoder on the training set and transform both train and test sets
encoded_train = encoder.fit_transform(train_set[categorical_columns])
encoded_test = encoder.transform(test_set[categorical_columns])

# Convert the encoded data back into DataFrames
encoded_train_df = pd.DataFrame(encoded_train.toarray(), columns=encoder.get_feature_names_out(categorical_columns))
encoded_test_df = pd.DataFrame(encoded_test.toarray(), columns=encoder.get_feature_names_out(categorical_columns))

# Reset the indices and concatenate the encoded columns with the original DataFrame
train_set = pd.concat([train_set.reset_index(drop=True), encoded_train_df], axis=1)
test_set = pd.concat([test_set.reset_index(drop=True), encoded_test_df], axis=1)

# Drop the original categorical columns
train_set = train_set.drop(columns=categorical_columns)
test_set = test_set.drop(columns=categorical_columns)

# Verify the resulting columns
print(train_set.columns)
print(test_set.columns)

"""# Step 26: Cleaning the Data From Column hc, nox and hcnox"""

df = cl_union_cleaned

# Define the function to fill missing values

def fill_missing_values(df, col_hc, col_nox, col_hcnox):
    # Calculation of missing values in 'hcnox'
    mask_hcnox = pd.isna(df[col_hcnox]) & pd.notna(df[col_hc]) & pd.notna(df[col_nox])
    df.loc[mask_hcnox, col_hcnox] = df.loc[mask_hcnox, col_hc] + df.loc[mask_hcnox, col_nox]

    # Calculation of missing values in 'hc'
    mask_hc = pd.isna(df[col_hc]) & pd.notna(df[col_nox]) & pd.notna(df[col_hcnox])
    df.loc[mask_hc, col_hc] = df.loc[mask_hc, col_hcnox] - df.loc[mask_hc, col_nox]

    # Calculation of missing values in 'nox'
    mask_nox = pd.isna(df[col_nox]) & pd.notna(df[col_hc]) & pd.notna(df[col_hcnox])
    df.loc[mask_nox, col_nox] = df.loc[mask_nox, col_hcnox] - df.loc[mask_nox, col_hc]

# Split the original cl_union dataset into train and test sets
train_set, test_set = train_test_split(cl_union_cleaned, test_size=0.2, random_state=42)

# Apply the function to the training set
fill_missing_values(train_set, 'hc', 'nox', 'hcnox')

# Apply the function to the test set
fill_missing_values(test_set, 'hc', 'nox', 'hcnox')

# Verify the results
print(train_set[['hc', 'nox', 'hcnox']].head())
print(test_set[['hc', 'nox', 'hcnox']].head())

# Apply the function iteratively to fill as many missing values as possible
for _ in range(5):  # Adjust the range if needed
    fill_missing_values(train_set, 'hc', 'nox', 'hcnox')

# Check for remaining missing values
print("Remaining missing values in the training set:")
print(train_set[['hc', 'nox', 'hcnox']].isnull().sum())

# Fill remaining missing values with the mean of the respective columns

train_set.fillna({'hc':train_set['hc'].mean()}, inplace=True)
train_set.fillna({'nox':train_set['nox'].mean()}, inplace=True)
train_set.fillna({'hcnox':train_set['hcnox'].mean()}, inplace=True)

#df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.
#testset
test_set.fillna({'hc':test_set['hc'].mean()}, inplace=True)
test_set.fillna({'nox':test_set['nox'].mean()}, inplace=True)
test_set.fillna({'hcnox':test_set['hcnox'].mean()}, inplace=True)

# Check the remaining missing values
print(train_set[['hc', 'nox', 'hcnox']].isnull().sum())
print(test_set[['hc', 'nox', 'hcnox']].isnull().sum())

"""# Step 27: Cleaning data from column particle through mean"""

# Calculate the mean of the 'Particles' column from the training set
mean_particles_train = train_set['Particles'].mean()

# Display the mean of the 'Particles' column from the training set
print(f"Mean of the 'Particles' column (Training set): {mean_particles_train}")

# Calculate the mean of the 'Particles' column from the test set
mean_particles_test = test_set['Particles'].mean()

# Display the mean of the 'Particles' column from the test set
print(f"Mean of the 'Particles' column (Test set): {mean_particles_test}")

#  fill missing values in 'Particles' with the mean:
train_set.fillna({'Particles': mean_particles_train}, inplace=True)
test_set.fillna({'Particles': mean_particles_train}, inplace=True)

# Verify the changes
print(train_set['Particles'].head())
print(test_set['Particles'].head())

print(train_set['Particles'].isnull().sum())
print(test_set['Particles'].isnull().sum())

"""# Step 28: Replacing missing values for numerical data with mean and for categorical data with mode"""

# Fill missing values in categorical columns
categorical_columns = ['Carrosserie', 'fuel_type', 'Gearbox', 'brand', 'Model_file']


for col in categorical_columns:
    train_set.fillna({col:train_set[col].mode()[0]}, inplace=True)
    test_set.fillna({col:test_set[col].mode()[0]}, inplace=True)

# Impute missing values on the training set
numerical_columns = ['Urban_consumption (l/100km)', 'Extra_urban_consumption(l/100km)',
                     'Consumption_mix(l/100km)', 'CO2', 'CO_type_I (g/km)', 'Year']

for col in numerical_columns:

    train_set[col] = pd.to_numeric(train_set[col], errors='coerce')
    train_set.fillna({col:train_set[col].mean()}, inplace=True)

# Apply the same strategy to the test set (be cautious of data leakage)
for col in numerical_columns:
    test_set[col] = pd.to_numeric(test_set[col], errors='coerce')
    test_set.fillna({col:train_set[col].mean()}, inplace=True) # Use training mean

# Impute categorical missing values
categorical_columns = ['fuel_type', 'Champ_V9']

for col in categorical_columns:

    train_set.fillna({col:train_set[col].mode()[0]}, inplace=True)
    test_set.fillna({col:train_set[col].mode()[0]}, inplace=True)

# Check for missing values in the training set
missing_values_train = train_set.isnull().sum()

# Check for missing values in the test set
missing_values_test = test_set.isnull().sum()

# Display columns with missing values in the training set
print("Missing values in the training set:")
print(missing_values_train[missing_values_train > 0])

# Display columns with missing values in the test set
print("Missing values in the test set:")
print(missing_values_test[missing_values_test > 0])

# Verify the resulting columns
print(train_set.columns)
print(test_set.columns)

"""# Step 29: Drop Columns deemed unnecessary after data is clean

## NOW THERE ARE NO MISSING VALUES. WE WILL HENCE DROP COLUMNS WE DO NOT NEED TO FURTHER EXPLORE OUR TARGET VARIABLE CO2

Variables we keep


NUMERICAL

1. 'Consumption_mix (l/100km)'
2. 'hcnox'  
3. 'CO2'
4. 'power_maximal (kW)'
5.  'Administrative_power'
6. 'Empty_mass_max(kg)'
7. 'Empty_mass_min(kg)'
8. 'Year'

CATEGORICAL

1. 'Carosserie'
2.  'range'
3. 'gearbox'
4. 'brand'                       
5. 'Model_file'
6. 'Group'
7. 'Country'
8. 'CO2_class '
9. 'fuel_type'
"""

#  List of columns to keep
columns_to_keep = [
    'fuel_type',
    'Carrosserie',
    'range',
    'Gearbox',
    'power_maximal (kW)',
    'Administrative_power',
    'Empty_mass_max(kg)',
    'Empty_mass_min(kg)',
    'CO2',
    'hcnox',
    'hc',
    'nox',
    'Year',
    'brand',
    'Consumption_mix(l/100km)',  # Ensure this matches the exact name in your DataFrame
    'Model_file', # Ensure this matches the exact name in your DataFrame
    'Group',
    'Country',
    'CO2_class'
]

# Step 4: Cleanse the train and test sets by selecting only the desired columns
train_set_cleaned = train_set[columns_to_keep]
test_set_cleaned = test_set[columns_to_keep]

# Step 5: Display the first few rows of the cleansed train and test sets
print("Train Set:")
print(train_set_cleaned.head())

print("\nTest Set:")
print(test_set_cleaned.head())

print(train_set_cleaned.columns)
print(test_set_cleaned.columns)

"""# Step 30: Data visualization of CO2 distribution"""

import matplotlib.pyplot as plt

# Plot distribution of CO2 emissions in the train set
plt.figure(figsize=(10, 6))
plt.hist(train_set_cleaned['CO2'], bins=50, color='blue', alpha=0.7, label='Train Set')
plt.hist(test_set_cleaned['CO2'], bins=50, color='green', alpha=0.7, label='Test Set')
plt.xlabel('CO2 Emissions (g/km)')
plt.ylabel('Frequency')
plt.title('Distribution of CO2 Emissions in Train and Test Sets')
plt.legend()
plt.show()

import matplotlib.pyplot as plt

# Step 1: Sort by CO2 emissions
sorted_train_df = train_set_cleaned[['brand', 'Model_file', 'CO2']].sort_values(by='CO2', ascending=False)

# Step 2: Drop duplicate car models based on 'Commercial_name' to ensure uniqueness
unique_top_train_cars = sorted_train_df.drop_duplicates(subset=['Model_file'])

# Step 3: Select the top 20 or as many as available
top_train_cars = unique_top_train_cars.head(20).copy()  # Explicitly make a copy to avoid SettingWithCopyWarning

# Step 4: Create a label combining brand and commercial name using .loc
top_train_cars.loc[:, 'Car'] = top_train_cars['brand'] + " " + top_train_cars['Model_file']

# Step 5: Plot the top 20 cars with the highest CO2 emissions
plt.figure(figsize=(12, 8))
plt.barh(top_train_cars['Car'], top_train_cars['CO2'], color='skyblue')
plt.xlabel('CO2 Emissions (g/km)')
plt.ylabel('Car Model')
plt.title('Top 20 Cars with Highest CO2 Emissions - Training Set')
plt.gca().invert_yaxis()

# Highlight the car with the highest CO2 emissions
max_co2_train = top_train_cars['CO2'].max()
max_co2_car_train = top_train_cars[top_train_cars['CO2'] == max_co2_train]['Car'].values[0]
plt.barh(max_co2_car_train, max_co2_train, color='red')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Step 1: Sort by CO2 emissions
sorted_test_df = test_set_cleaned[['brand', 'Model_file', 'CO2']].sort_values(by='CO2', ascending=False)

# Step 2: Drop duplicate car models based on 'Commercial_name' to ensure uniqueness
unique_top_test_cars = sorted_test_df.drop_duplicates(subset=['Model_file'])

# Step 3: Select the top 20 or as many as available
top_test_cars = unique_top_test_cars.head(20).copy()  # Explicitly make a copy to avoid SettingWithCopyWarning

# Step 4: Create a label combining brand and commercial name using .loc
top_test_cars.loc[:, 'Car'] = top_test_cars['brand'] + " " + top_test_cars['Model_file']

# Step 5: Plot the top 20 cars with the highest CO2 emissions
plt.figure(figsize=(12, 8))
plt.barh(top_test_cars['Car'], top_test_cars['CO2'], color='skyblue')
plt.xlabel('CO2 Emissions (g/km)')
plt.ylabel('Car Model')
plt.title('Top 20 Cars with Highest CO2 Emissions - Test Set')
plt.gca().invert_yaxis()

# Highlight the car with the highest CO2 emissions
max_co2_test = top_test_cars['CO2'].max()
max_co2_car_test = top_test_cars[top_test_cars['CO2'] == max_co2_test]['Car'].values[0]
plt.barh(max_co2_car_test, max_co2_test, color='red')

plt.tight_layout()
plt.show()

# Manually add missing columns if they are dropped during splitting
train_set_cleaned, test_set_cleaned = train_set_cleaned.align(test_set_cleaned, join='left', axis=1, fill_value=0)

# Check if fuel_type columns were added back properly
fuel_type_columns_train = [col for col in train_set_cleaned.columns if 'fuel_type' in col]
fuel_type_columns_test = [col for col in test_set_cleaned.columns if 'fuel_type' in col]

print("Fuel type columns in train set:", fuel_type_columns_train)

"""# Step 31 Standardizing numeric and Encoding categorical variables"""

# Redefine categorical_columns correctly
categorical_columns = ['Carrosserie', 'range', 'Gearbox', 'brand', 'Model_file', 'fuel_type', 'Country', 'Group', 'CO2_class']

"""# Step 32: Encoding and transforming data"""

# Step 1: Make a copy of the original dataset before any encoding
train_set_cleaned_backup = train_set_cleaned.copy()
test_set_cleaned_backup = test_set_cleaned.copy()

# Step 2: Apply one-hot encoding only to the selected categorical columns
categorical_columns = ['fuel_type', 'Carrosserie', 'range', 'Gearbox', 'brand', 'Model_file', 'Country', 'Group', 'CO2_class']

# Use pd.get_dummies only on the selected categorical columns
train_set_encoded = pd.get_dummies(train_set_cleaned[categorical_columns], drop_first=False)
test_set_encoded = pd.get_dummies(test_set_cleaned[categorical_columns], drop_first=False)

# Step 3: Concatenate the encoded columns back with the original datasets
train_set_cleaned = pd.concat([train_set_cleaned_backup.drop(columns=categorical_columns), train_set_encoded], axis=1)
test_set_cleaned = pd.concat([test_set_cleaned_backup.drop(columns=categorical_columns), test_set_encoded], axis=1)

# Step 4: Align train and test sets to ensure they have the same dummy variables
train_set_cleaned, test_set_cleaned = train_set_cleaned.align(test_set_cleaned, join='left', axis=1, fill_value=0)

# Verify the dataset structure after encoding
print("Training set after one-hot encoding and reattaching columns:")
print(train_set_cleaned.head())

print("\nTest set after one-hot encoding and reattaching columns:")
print(test_set_cleaned.head())

# Check for the presence of one-hot encoded fuel_type columns after encoding
fuel_type_columns_train = [col for col in train_set_cleaned.columns if 'fuel_type' in col]
fuel_type_columns_test = [col for col in test_set_cleaned.columns if 'fuel_type' in col]

print("Fuel type columns in train set after encoding:", fuel_type_columns_train)
print("Fuel type columns in test set after encoding:", fuel_type_columns_test)

print(train_set_cleaned.columns)
print(test_set_cleaned.columns)

# Convert boolean columns (True/False) to 1/0 in bulk
bool_cols_train = train_set_cleaned.select_dtypes(include=['bool']).astype(int)
bool_cols_test = test_set_cleaned.select_dtypes(include=['bool']).astype(int)

# Drop the old boolean columns from the original DataFrame
train_set_cleaned = train_set_cleaned.drop(columns=bool_cols_train.columns)
test_set_cleaned = test_set_cleaned.drop(columns=bool_cols_test.columns)

# Use pd.concat to re-attach the converted boolean columns to the DataFrame
train_set_cleaned = pd.concat([train_set_cleaned, bool_cols_train], axis=1)
test_set_cleaned = pd.concat([test_set_cleaned, bool_cols_test], axis=1)

# Verify that the columns are now 0/1 instead of True/False
print(train_set_cleaned.head())
print(test_set_cleaned.head())

# Verify that the fuel_type columns contain data (0s and 1s)
print(train_set_cleaned[fuel_type_columns_train].sum())  # Sum will tell if 0/1 values are present
print(test_set_cleaned[fuel_type_columns_test].sum())

"""** encoding is working correctly. The issue is simply that some categories are underrepresented or absent in the test set.
Low category representation (like fuel_type_GL and others) is normal after splitting and may or may not need addressing.
Optional resampling can be done to balance the categories, but it’s not strictly necessary unless we see poor model performance related to these categories.**
"""

from sklearn.preprocessing import StandardScaler

# List of numerical columns
numerical_columns = [
    'Consumption_mix(l/100km)',
    'hcnox',
    'CO2',
    'Year',
    'power_maximal (kW)',
    'Administrative_power',
    'Empty_mass_max(kg)',
    'Empty_mass_min(kg)']

# Initialize StandardScaler
scaler = StandardScaler()

# Step 1: Fit the scaler on training data and transform it
train_scaled = scaler.fit_transform(train_set_cleaned[numerical_columns])

# Step 2: Transform the test data using the fitted scaler
test_scaled = scaler.transform(test_set_cleaned[numerical_columns])

# Step 3: Convert the scaled data back into DataFrames to keep column names
train_scaled_df = pd.DataFrame(train_scaled, columns=numerical_columns, index=train_set_cleaned.index)
test_scaled_df = pd.DataFrame(test_scaled, columns=numerical_columns, index=test_set_cleaned.index)

# Step 4: Concatenate the scaled numerical columns back with the original DataFrames
train_set_cleaned = pd.concat([train_set_cleaned.drop(columns=numerical_columns), train_scaled_df], axis=1)
test_set_cleaned = pd.concat([test_set_cleaned.drop(columns=numerical_columns), test_scaled_df], axis=1)

# Step 5: Verify the results
print("Training set after scaling and preserving categorical columns:")
print(train_set_cleaned.head())

print("\nTest set after scaling and preserving categorical columns:")
print(test_set_cleaned.head())

# Verify the mean and standard deviation after scaling
print(train_set_cleaned[numerical_columns].mean(axis=0))  # Should be close to 0
print(train_set_cleaned[numerical_columns].std(axis=0))   # Should be close to 1
print(test_set_cleaned[numerical_columns].mean(axis=0))  # Should be close to 0
print(test_set_cleaned[numerical_columns].std(axis=0))   # Should be close to 1

# Check if 'CO2_class' is present in the dataset
print("Columns in train_set_cleaned:", train_set_cleaned.columns)
print("Columns in test_set_cleaned:", test_set_cleaned.columns)

"""# step 33 Label Encoder"""

# Step 1: Identify the one-hot encoded columns for CO2_class
co2_class_columns = [col for col in train_set_cleaned.columns if col.startswith('CO2_class_')]

# Step 2: Separate features and target (using one-hot encoded CO2_class columns)
X_train = train_set_cleaned.drop(columns=co2_class_columns)  # Features
y_train = train_set_cleaned[co2_class_columns]  # One-hot encoded target

X_test = test_set_cleaned.drop(columns=co2_class_columns)  # Features
y_test = test_set_cleaned[co2_class_columns]  # One-hot encoded target

# Step 3: Verify the shape of the features and target
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)

print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

# Step 4: Proceed with your model training
# (For example, if using a model like Logistic Regression, RandomForest, etc.)

"""#Step 34 Training Models

# Logistic Regression
"""

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Step 1: Ensure that the feature names in X_train and X_test are aligned
X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)

# Step 2: Convert one-hot encoded y_train and y_test back to class labels (1D array)
y_train_single = np.argmax(y_train.values, axis=1)  # Convert one-hot to single class labels
y_test_single = np.argmax(y_test.values, axis=1)    # Convert one-hot to single class labels

# Step 3: Train the Logistic Regression model
logistic_model = LogisticRegression(max_iter=1000)
logistic_model.fit(X_train, y_train_single)  # Use single class labels (1D array)

# Step 4: Make predictions on the test set
y_pred_logistic = logistic_model.predict(X_test)

# Step 5: Evaluate the model
print("Logistic Regression Accuracy:", accuracy_score(y_test_single, y_pred_logistic))
print("Logistic Regression Classification Report:\n", classification_report(y_test_single, y_pred_logistic))

"""Accuracy:

95% accuracy means that 95% of the predictions on the test set were correct.
Precision, Recall, F1-Score:

These metrics are provided for each CO2 class (labeled 0-6).
Precision: Measures how many of the positive predictions were correct.
Recall: Measures how many of the actual positives were correctly identified.
F1-Score: The harmonic mean of precision and recall, balancing the two.
Macro Average:

The macro average gives the average precision, recall, and F1-score across all classes equally.
Weighted Average:

The weighted average accounts for the number of instances in each class, giving a better sense of overall performance when the classes are imbalanced.
Interpretation:
Class 6 (which could be CO2_class_G, based on your data) has an exceptionally high precision, recall, and F1-score (nearly perfect), suggesting the model is very confident in predicting this class.
Class 0 (potentially CO2_class_A) has a lower recall (0.88), meaning that the model might be missing some instances of this class, although it still performs well overall.

# Random Forest Classifier
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Step 1: Initialize and train the Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust n_estimators for more trees
rf_classifier.fit(X_train, y_train_single)  # Train on the training set

# Step 2: Make predictions on the test set
y_pred_rf = rf_classifier.predict(X_test)

# Step 3: Evaluate the Random Forest Classifier
print("Random Forest Classifier Accuracy:", accuracy_score(y_test_single, y_pred_rf))
print("Random Forest Classifier Classification Report:\n", classification_report(y_test_single, y_pred_rf))

"""Accuracy:

100% accuracy indicates that the Random Forest Classifier is making perfect predictions.
Precision, Recall, and F1-Score:

All classes (0 through 6) have very high precision, recall, and F1-scores, with many values approaching 1.00, indicating near-perfect performance across all classes.
The macro average and weighted average are also very high at 0.99, showing consistent performance across both large and small classes.
Comparison with Logistic Regression:
Random Forest Classifier outperformed Logistic Regression across all key metrics:
Accuracy: 100% (Random Forest) vs. 95% (Logistic Regression).
Precision, Recall, F1-score: Random Forest has more consistent and higher values across all classes.
Analysis:
Random Forest is likely handling the complexity of the data better than Logistic Regression, which assumes a linear relationship between the features and the target. Random Forest’s ability to capture non-linear patterns makes it a powerful model for this type of problem.
The near-perfect performance of Random Forest suggests it is a robust model for this dataset.

# Check Feature Importance in Random Forest:
"""

import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Access the feature importances from the trained Random Forest model
feature_importances = rf_classifier.feature_importances_

# Step 2: Create a DataFrame to hold feature names and their corresponding importance
feature_importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': feature_importances
})

# Step 3: Sort the DataFrame by importance in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Step 4: Plot the feature importances as a bar chart
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.title('Feature Importance in Random Forest Classifier')
plt.gca().invert_yaxis()  # Invert y-axis to show the most important features at the top
plt.show()

"""# Dimension Reduction"""

import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Access the feature importances from the trained Random Forest model
feature_importances = rf_classifier.feature_importances_

# Step 2: Create a DataFrame to hold feature names and their corresponding importance
feature_importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': feature_importances
})

# Step 3: Sort the DataFrame by importance in descending order and keep only the top 10 features
top_n = 10  # You can adjust this number to show more or fewer features
top_features = feature_importance_df.sort_values(by='Importance', ascending=False).head(top_n)

# Step 4: Plot the top feature importances as a bar chart
plt.figure(figsize=(10, 6))
plt.barh(top_features['Feature'], top_features['Importance'], color='skyblue')
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.title(f'Top {top_n} Important Features in Random Forest Classifier')
plt.gca().invert_yaxis()  # Invert y-axis to show the most important features at the top
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Access the feature importances from the trained Random Forest model
feature_importances = rf_classifier.feature_importances_

# Step 2: Create a DataFrame to hold feature names and their corresponding importance
feature_importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': feature_importances
})

# Step 3: Sort the DataFrame by importance in descending order and keep only the top 20 features
top_n = 20  # Adjust this to show more or fewer features
top_features = feature_importance_df.sort_values(by='Importance', ascending=False).head(top_n)

# Step 4: Plot the top feature importances as a bar chart
plt.figure(figsize=(10, 6))
plt.barh(top_features['Feature'], top_features['Importance'], color='skyblue')
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.title(f'Top {top_n} Important Features in Random Forest Classifier')
plt.gca().invert_yaxis()  # Invert y-axis to show the most important features at the top
plt.show()

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from scipy.stats import randint

# Define the hyperparameters to search over
param_distributions = {
    'n_estimators': randint(50, 300),  # Randomly choose number of trees
    'max_depth': [None, 10, 20, 30, 40],  # Randomly choose depth
    'min_samples_split': randint(2, 10),  # Randomly choose minimum samples for split
    'min_samples_leaf': randint(1, 10),   # Randomly choose minimum samples at a leaf node
    'max_features': ['auto', 'sqrt', 'log2']  # Different feature selection methods
}

# Initialize the RandomForestClassifier
rf_model = RandomForestClassifier(random_state=42)

# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=rf_model, param_distributions=param_distributions,
                                   n_iter=50, cv=3, random_state=42, n_jobs=-1, scoring='accuracy', verbose=2)

# Fit the RandomizedSearchCV model to the data
random_search.fit(X_train, y_train_single)

# Print the best parameters and the best score
print(f"Best Hyperparameters: {random_search.best_params_}")
print(f"Best Accuracy: {random_search.best_score_}")

# Use the best estimator to make predictions
best_rf_model_random = random_search.best_estimator_
y_pred_best_rf_random = best_rf_model_random.predict(X_test)

# Evaluate the tuned Random Forest Classifier from RandomizedSearchCV
from sklearn.metrics import accuracy_score, classification_report
print("Randomized Search Tuned Random Forest Accuracy:", accuracy_score(y_test_single, y_pred_best_rf_random))
print("Randomized Search Tuned Random Forest Classification Report:\n", classification_report(y_test_single, y_pred_best_rf_random))

"""# K-Nearest Neighbors (KNN)"""

from sklearn.neighbors import KNeighborsClassifier

# Initialize and train the KNN model
knn_model = KNeighborsClassifier(n_neighbors=5)  # 5 neighbors by default, can be tuned
knn_model.fit(X_train, y_train_single)

# Make predictions with KNN
y_pred_knn = knn_model.predict(X_test)

# Evaluate KNN
print("KNN Accuracy:", accuracy_score(y_test_single, y_pred_knn))
print("KNN Classification Report:\n", classification_report(y_test_single, y_pred_knn))

"""# Gradient Boosting (XGBoost)

takes too long
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

# Initialize the Decision Tree Classifier
decision_tree = DecisionTreeClassifier(random_state=42)

# Train the Decision Tree on the training data
decision_tree.fit(X_train, y_train_single)

# Make predictions on the test set
y_pred_tree = decision_tree.predict(X_test)

# Evaluate the Decision Tree model
print("Decision Tree Classifier Accuracy:", accuracy_score(y_test_single, y_pred_tree))
print("Decision Tree Classifier Classification Report:\n", classification_report(y_test_single, y_pred_tree))

from sklearn.model_selection import RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier
from scipy.stats import randint

# Define the hyperparameter space for tuning
param_distributions = {
    'max_depth': [None, 10, 20, 30, 40, 50],
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 20),
    'max_features': ['auto', 'sqrt', 'log2', None]
}

# Initialize the Decision Tree Classifier
decision_tree = DecisionTreeClassifier(random_state=42)

# Initialize RandomizedSearchCV
random_search_tree = RandomizedSearchCV(estimator=decision_tree, param_distributions=param_distributions,
                                        n_iter=50, cv=3, random_state=42, n_jobs=-1, scoring='accuracy', verbose=2)

# Fit RandomizedSearchCV to find the best parameters
random_search_tree.fit(X_train, y_train_single)

# Print the best parameters and the best score
print(f"Best Hyperparameters for Decision Tree: {random_search_tree.best_params_}")
print(f"Best Accuracy for Decision Tree: {random_search_tree.best_score_}")

# Use the best estimator to make predictions on the test set
best_tree_model = random_search_tree.best_estimator_
y_pred_best_tree = best_tree_model.predict(X_test)

# Evaluate the tuned Decision Tree Classifier
from sklearn.metrics import accuracy_score, classification_report
print("Tuned Decision Tree Classifier Accuracy:", accuracy_score(y_test_single, y_pred_best_tree))
print("Tuned Decision Tree Classifier Classification Report:\n", classification_report(y_test_single, y_pred_best_tree))

"""Here's a breakdown of the results:

### Random Forest Classifier (after RandomizedSearchCV):
- **Accuracy**: 99.7%
- **Classification Report**: Nearly perfect scores across all classes.
- **Best Hyperparameters** from RandomizedSearchCV:
  - `max_depth`: None
  - `max_features`: 'sqrt'
  - `min_samples_leaf`: 1
  - `min_samples_split`: 9
  - `n_estimators`: 296

The RandomizedSearchCV improved the model's accuracy marginally, from about 99.4% to 99.7%, which indicates that the tuning had a positive impact.

### Decision Tree Classifier (after RandomizedSearchCV):
- **Accuracy**: 100%
- **Classification Report**: Perfect scores across all classes.
- **Best Hyperparameters**:
  - `max_depth`: 20
  - `max_features`: None
  - `min_samples_leaf`: 4
  - `min_samples_split`: 9

The Decision Tree model achieved perfect accuracy after tuning, which is quite unusual. However, this could suggest **overfitting** since Decision Trees tend to memorize the data rather than generalize when given the flexibility to fit precisely.

### Interpretation and Next Steps:
1. **Random Forest**: With an accuracy of 99.7%, Random Forest is highly robust and generalizes well. This slight increase with tuning suggests a stable model.
2. **Decision Tree**: The perfect accuracy on the test set could be a sign of overfitting. Decision Trees are sensitive to overfitting, especially when there are no depth constraints.

### Handling Overfitting in Decision Tree
If you want to use the Decision Tree but avoid overfitting:
   - Further limit `max_depth` or increase `min_samples_leaf` and `min_samples_split` to make the model less sensitive.
   - Consider using **Pruning** techniques to reduce overfitting in Decision Trees.

Would you like to explore further tuning on the Decision Tree to control overfitting, or focus on analyzing the Random Forest model given its near-perfect performance with generalization?
"""

from sklearn.metrics import accuracy_score, classification_report

# Predictions on the training set
y_pred_train_tree = best_tree_model.predict(X_train)

# Accuracy and Classification Report on Training Data
print("Decision Tree Classifier Training Accuracy:", accuracy_score(y_train_single, y_pred_train_tree))
print("Decision Tree Classifier Training Classification Report:\n", classification_report(y_train_single, y_pred_train_tree))

# Compare this with the test data metrics you already have
print("\nDecision Tree Classifier Test Accuracy:", accuracy_score(y_test_single, y_pred_best_tree))
print("Decision Tree Classifier Test Classification Report:\n", classification_report(y_test_single, y_pred_best_tree))

from sklearn.model_selection import cross_val_score

# Cross-validation on Decision Tree
cv_scores_tree = cross_val_score(best_tree_model, X_train, y_train_single, cv=5)
print("Cross-Validation Scores (Decision Tree):", cv_scores_tree)
print("Average CV Accuracy (Decision Tree):", cv_scores_tree.mean())

# Cross-validation on Random Forest
cv_scores_rf = cross_val_score(best_rf_model_random, X_train, y_train_single, cv=5)
print("Cross-Validation Scores (Random Forest):", cv_scores_rf)
print("Average CV Accuracy (Random Forest):", cv_scores_rf.mean())

from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report

# Define base models
base_estimators = [
    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
    ('xgb', XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42)),
    ('logreg', LogisticRegression(max_iter=1000))
]

# Define the stacking model
stacking_model = StackingClassifier(estimators=base_estimators, final_estimator=LogisticRegression())

# Train the stacking model
stacking_model.fit(X_train, y_train_single)

# Make predictions
y_pred_stacking = stacking_model.predict(X_test)

# Evaluate the model
print("Stacking Ensemble Accuracy:", accuracy_score(y_test_single, y_pred_stacking))
print("Stacking Ensemble Classification Report:\n", classification_report(y_test_single, y_pred_stacking))

"""The output suggests that the stacking ensemble model performed extremely well across all classes, with near-perfect precision, recall, and F1-scores. This level of performance implies that the model is highly effective at distinguishing between the classes in your dataset."""

from sklearn.model_selection import cross_val_score

# Perform 5-fold cross-validation on the stacking model
cv_scores = cross_val_score(stacking_model, X_train, y_train_single, cv=5, scoring='accuracy')
print("Cross-Validation Scores:", cv_scores)
print("Average CV Accuracy:", cv_scores.mean())

"""If the training score is much higher than the cross-validation score and does not converge as more data is added, this is a strong sign of overfitting.
If both curves converge and stabilize at similar accuracy levels, the model is likely not overfitting.
"""

from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt
import numpy as np

# Generate learning curves
train_sizes, train_scores, test_scores = learning_curve(
    stacking_model, X_train, y_train_single, cv=5, scoring='accuracy', n_jobs=-1,
    train_sizes=np.linspace(0.1, 1.0, 10)
)

# Calculate mean and standard deviation for training and test scores
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)

# Plot learning curves
plt.figure()
plt.title("Learning Curves (Stacking Ensemble)")
plt.xlabel("Training Examples")
plt.ylabel("Accuracy Score")
plt.grid()

plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                 train_scores_mean + train_scores_std, alpha=0.1, color="r")
plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                 test_scores_mean + test_scores_std, alpha=0.1, color="g")
plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")
plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation score")

plt.legend(loc="best")
plt.show()

"""# DECISION TREE

"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report

# Define the Decision Tree and parameters to tune
tree_params = {
    'max_depth': [None, 10, 20, 30, 40],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['auto', 'sqrt', 'log2']
}

# Initialize the Decision Tree Classifier
decision_tree = DecisionTreeClassifier(random_state=42)

# Perform Grid Search
grid_search_tree = GridSearchCV(decision_tree, tree_params, cv=5, scoring='accuracy')
grid_search_tree.fit(X_train, y_train)

# Best parameters and model evaluation
print("Best Parameters:", grid_search_tree.best_params_)
best_tree_model = grid_search_tree.best_estimator_
y_pred_tree = best_tree_model.predict(X_test)

print("Decision Tree Accuracy:", accuracy_score(y_test, y_pred_tree))
print("Classification Report:\n", classification_report(y_test, y_pred_tree))